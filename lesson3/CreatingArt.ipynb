{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](files/gans.png)\n",
    "\n",
    "Generative adversarial networks (GANs) are a class of artificial intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks contesting with each other in a zero-sum game framework. They were introduced by Ian Goodfellow et al. in 2014,[1] although the idea of adversarial training dates back to Jürgen Schmidhuber in 1992.[2]\n",
    "\n",
    "This technique can generate photographs that look authentic to human observers. For example, a synthetic photograph of a cat that fools the discriminator into accepting it as an actual photograph.[3]\n",
    "\n",
    "One network generates candidates and one evaluates them.[1] Typically, the generative network learns to map from a latent space to a particular data distribution of interest, while the discriminative network discriminates between instances from the true data distribution and candidates produced by the generator. The generative network's training objective is to increase the error rate of the discriminative network (i.e., \"fool\" the discriminator network by producing novel synthesized instances that appear to have come from the true data distribution).[1][4]\n",
    "\n",
    "In practice, a known dataset serves as the initial training data for the discriminator. Training the discriminator involves presenting it with samples from the dataset, until it reaches some level of accuracy. Typically the generator is seeded with a randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter, samples synthesized by the generator are evaluated by the discriminator. Backpropagation is applied in both networks so that the generator produces better images, while the discriminator becomes more skilled at flagging synthetic images. [5] The generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network.\n",
    "\n",
    "The idea to infer models in a competitive setting (model versus discriminator) was proposed by Li, Gauci and Gross in 2013.[6] Their method is used for behavioral inference. It is termed Turing Learning,[7] as the setting is akin to that of a Turing test.\n",
    "\n",
    "## Application\n",
    "\n",
    "![title](files/ganshype.png)\n",
    "GANs have been used to produce samples of photorealistic images for the purposes of visualizing new interior/industrial design, shoes, bags and clothing items or items for computer games' scenes. These networks were reported to be used by Facebook.[8] Recently, GANs have modeled patterns of motion in video. They have also been used to reconstruct 3D models of objects from images and to improve astronomical images.\n",
    "\n",
    "## Tutorial\n",
    "https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f\n",
    "\n",
    "\n",
    "The original, genuine data set\n",
    "I: The random noise that goes into the generator as a source of entropy\n",
    "G: The generator which tries to copy/mimic the original data set\n",
    "D: The discriminator which tries to tell apart G’s output from R\n",
    "The actual ‘training’ loop where we teach G to trick D and D to beware G.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-ade34c7bf229>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ade34c7bf229>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://www.oreilly.com/learning/generative-adversarial-networks-for-beginners?imm_mid=0f6436&cmp=em-data-na-na-newsltr_ai_20170918\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://www.oreilly.com/learning/generative-adversarial-networks-for-beginners?imm_mid=0f6436&cmp=em-data-na-na-newsltr_ai_20170918\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a GAN learn?\n",
    "![title](files/gan.png)\n",
    "A general adverserial network are two neural networks, one generator and one discriminator. The job of the generator is to create stimuli that can fool the discriminator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCGAN\n",
    "\n",
    "The DCGAN network takes as input 100 random numbers drawn from a uniform distribution (we refer to these as a code, or latent variables, in red) \n",
    "and outputs an image (in this case 64x64x3 images on the right, in green). \n",
    "As the code is changed incrementally, the generated images do too — this shows the model \n",
    "has learned features to describe how the world looks, \n",
    "rather than just memorizing some examples.\n",
    "The network (in yellow) is made up of standard convolutional neural network components, such as deconvolutional layers (reverse of convolutional layers), fully connected layers, etc.:\n",
    "![title](files/gen_models_diag_1.svg)\n",
    "\n",
    "DCGAN is initialized with random weights, so a random code plugged into the network would generate a completely random image. However, as you might imagine, the network has millions of parameters that we can tweak, and the goal is to find a setting of these parameters that makes samples generated from random codes look like the training data. Or to put it another way, we want the model distribution to match the true data distribution in the space of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tensorflow DCGAN\n",
    "https://github.com/carpedm20/DCGAN-tensorflow\n",
    "\n",
    "# Keras DCGAN\n",
    "https://github.com/jacobgil/keras-dcgan\n",
    "\n",
    "# Deep completion blog post\n",
    "http://bamos.github.io/2016/08/09/deep-completion/  \n",
    "    \n",
    "# Image completion\n",
    "https://github.com/bamos/dcgan-completion.tensorflow\n",
    "    \n",
    "# Online Demo\n",
    "https://github.com/carpedm20/DCGAN-tensorflow\n",
    "\n",
    "# Blogpost about GANS\n",
    "http://guimperarnau.com/blog/2017/03/Fantastic-GANs-and-where-to-find-them\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural style transfer\n",
    "![title](files/styletransfer.jpg)\n",
    "Neural style tranfer is done with Convolutional Neural Networks.\n",
    "\n",
    "## Image style transfer\n",
    "\n",
    "Image style transfer is defined as follow: given two images on the input, synthesize a third image that has the semantic content of the first image and the texture/style of the second. \n",
    "\n",
    "To work properly we need a way to (1) determine the content and the style of any image (content/style extractor) and then (2) merge some arbitrary content with another arbitrary style (merger).\n",
    "\n",
    "![title](files/styletransfer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Neural style transfer with Keras.\n",
    "Run the script with:\n",
    "```\n",
    "python neural_style_transfer.py path_to_your_base_image.jpg path_to_your_reference.jpg prefix_for_results\n",
    "```\n",
    "e.g.:\n",
    "```\n",
    "python neural_style_transfer.py img/tuebingen.jpg img/starry_night.jpg results/my_result\n",
    "```\n",
    "Optional parameters:\n",
    "```\n",
    "--iter, To specify the number of iterations the style transfer takes place (Default is 10)\n",
    "--content_weight, The weight given to the content loss (Default is 0.025)\n",
    "--style_weight, The weight given to the style loss (Default is 1.0)\n",
    "--tv_weight, The weight given to the total variation loss (Default is 1.0)\n",
    "```\n",
    "It is preferable to run this script on GPU, for speed.\n",
    "Example result: https://twitter.com/fchollet/status/686631033085677568\n",
    "# Details\n",
    "Style transfer consists in generating an image\n",
    "with the same \"content\" as a base image, but with the\n",
    "\"style\" of a different picture (typically artistic).\n",
    "This is achieved through the optimization of a loss function\n",
    "that has 3 components: \"style loss\", \"content loss\",\n",
    "and \"total variation loss\":\n",
    "- The total variation loss imposes local spatial continuity between\n",
    "the pixels of the combination image, giving it visual coherence.\n",
    "- The style loss is where the deep learning keeps in --that one is defined\n",
    "using a deep convolutional neural network. Precisely, it consists in a sum of\n",
    "L2 distances between the Gram matrices of the representations of\n",
    "the base image and the style reference image, extracted from\n",
    "different layers of a convnet (trained on ImageNet). The general idea\n",
    "is to capture color/texture information at different spatial\n",
    "scales (fairly large scales --defined by the depth of the layer considered).\n",
    " - The content loss is a L2 distance between the features of the base\n",
    "image (extracted from a deep layer) and the features of the combination image,\n",
    "keeping the generated image close enough to the original one.\n",
    "# References\n",
    "    - [A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you need to run this code as a script. it is located in the current directory as\n",
    "# neural_style_transfer.py \n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from scipy.misc import imsave\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from keras.applications import vgg19\n",
    "from keras import backend as K\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Neural style transfer with Keras.')\n",
    "parser.add_argument('base_image_path', metavar='base', type=str,\n",
    "                    help='Path to the image to transform.')\n",
    "parser.add_argument('style_reference_image_path', metavar='ref', type=str,\n",
    "                    help='Path to the style reference image.')\n",
    "parser.add_argument('result_prefix', metavar='res_prefix', type=str,\n",
    "                    help='Prefix for the saved results.')\n",
    "parser.add_argument('--iter', type=int, default=10, required=False,\n",
    "                    help='Number of iterations to run.')\n",
    "parser.add_argument('--content_weight', type=float, default=0.025, required=False,\n",
    "                    help='Content weight.')\n",
    "parser.add_argument('--style_weight', type=float, default=1.0, required=False,\n",
    "                    help='Style weight.')\n",
    "parser.add_argument('--tv_weight', type=float, default=1.0, required=False,\n",
    "                    help='Total Variation weight.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "base_image_path = args.base_image_path\n",
    "style_reference_image_path = args.style_reference_image_path\n",
    "result_prefix = args.result_prefix\n",
    "iterations = args.iter\n",
    "\n",
    "# these are the weights of the different loss components\n",
    "total_variation_weight = args.tv_weight\n",
    "style_weight = args.style_weight\n",
    "content_weight = args.content_weight\n",
    "\n",
    "# dimensions of the generated picture.\n",
    "width, height = load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)\n",
    "\n",
    "# util function to open, resize and format pictures into appropriate tensors\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, img_nrows, img_ncols))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# get tensor representations of our images\n",
    "base_image = K.variable(preprocess_image(base_image_path))\n",
    "style_reference_image = K.variable(preprocess_image(style_reference_image_path))\n",
    "\n",
    "# this will contain our generated image\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    combination_image = K.placeholder((1, 3, img_nrows, img_ncols))\n",
    "else:\n",
    "    combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\n",
    "\n",
    "# combine the 3 images into a single Keras tensor\n",
    "input_tensor = K.concatenate([base_image,\n",
    "                              style_reference_image,\n",
    "                              combination_image], axis=0)\n",
    "\n",
    "# build the VGG16 network with our 3 images as input\n",
    "# the model will be loaded with pre-trained ImageNet weights\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,\n",
    "                    weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "# compute the neural style loss\n",
    "# first we need to define 4 util functions\n",
    "\n",
    "# the gram matrix of an image tensor (feature-wise outer product)\n",
    "\n",
    "\n",
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        features = K.batch_flatten(x)\n",
    "    else:\n",
    "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "# the \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
    "\n",
    "# an auxiliary loss function\n",
    "# designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))\n",
    "\n",
    "# the 3rd loss function, total variation loss,\n",
    "# designed to keep the generated image locally coherent\n",
    "\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n",
    "        b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n",
    "    else:\n",
    "        a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
    "        b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))\n",
    "\n",
    "# combine these loss functions into a single scalar\n",
    "loss = K.variable(0.)\n",
    "layer_features = outputs_dict['block5_conv2']\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss += content_weight * content_loss(base_image_features,\n",
    "                                      combination_features)\n",
    "\n",
    "feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                  'block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(feature_layers)) * sl\n",
    "loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "\n",
    "# get the gradients of the generated image wrt the loss\n",
    "grads = K.gradients(loss, combination_image)\n",
    "\n",
    "outputs = [loss]\n",
    "if isinstance(grads, (list, tuple)):\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)\n",
    "\n",
    "f_outputs = K.function([combination_image], outputs)\n",
    "\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((1, 3, img_nrows, img_ncols))\n",
    "    else:\n",
    "        x = x.reshape((1, img_nrows, img_ncols, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "# this Evaluator class makes it possible\n",
    "# to compute loss and gradients in one pass\n",
    "# while retrieving them via two separate functions,\n",
    "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# requires separate functions for loss and gradients,\n",
    "# but computing them separately would be inefficient.\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()\n",
    "\n",
    "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# so as to minimize the neural style loss\n",
    "x = preprocess_image(base_image_path)\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "                                     fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    # save current generated image\n",
    "    img = deprocess_image(x.copy())\n",
    "    fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "    imsave(fname, img)\n",
    "    end_time = time.time()\n",
    "    print('Image saved as', fname)\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "Running the code with the following input\n",
    "python neural_style_transfer.py img/birger.jpg img/starry_night.jpg results/my_result\n",
    "gives the following results\n",
    "## Input\n",
    "![title](files/birger.jpg)\n",
    "## Filter\n",
    "![title](files/starrynight.jpg)\n",
    "## Result\n",
    "![title](files/styletransfer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Art + Art = Art\n",
    "I tried it out with some of my paintings. Here are the results\n",
    "![title](files/art1.jpg)\n",
    "![title](files/art2.jpg)\n",
    "![title](files/art3.jpg)\n",
    "![title](files/art4.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Neural style transfer\n",
    "\n",
    "Neural Style Transfer is an algorithm for combining the content of one image with the style of another image using convolutional neural networks. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Play around with style transfer\n",
    "There are many different implementations of style transfer.\n",
    "\n",
    "## Deep-photo-style transfer\n",
    "A technique for style transfer that keeps the first photo intact\n",
    "https://github.com/luanfujun/deep-photo-styletransfer?utm_content=buffer39dd6&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer\n",
    "\n",
    "## Neural style transfer in your browser\n",
    "Using the deeplearn.js framwork, neural style transfer is implemented in your browser.\n",
    "https://reiinakano.github.io/fast-style-transfer-deeplearnjs/\n",
    "\n",
    "## Neural style transfer using Floydhub\n",
    "Floydhub is a service that lets you host your machine learning models in the cloud and train them on cloud servers. With Neural Style transfer from Floydhub you can train a neural style transfer that works \n",
    "https://docs.floydhub.com/examples/style_transfer/\n",
    "\n",
    "Here's an example that maps the artistic style of The Starry Night onto a night-time photograph of the Stanford campus:\n",
    "\n",
    "![title](files/starry_night_google.jpg)\n",
    "![title](files/hoovertowernight.jpg)\n",
    "![title](files/starry_stanford_bigger.png)\n",
    "\n",
    "## Torch Implementation of Neural Style Transfer\n",
    "https://github.com/jcjohnson/neural-style\n",
    "\n",
    "## Tensorflow Implementation of Neural Style Transfer\n",
    "https://github.com/titu1994/Neural-Style-Transfer\n",
    "\n",
    "## Keras implementation of Neural Style Transfer\n",
    "https://github.com/titu1994/Neural-Style-Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a mobile phone application that can handle style transfer\n",
    "Last time we built a mobile phone application that could predict if an image was a cat or a dog.\n",
    "https://github.com/bcarlyle/Momentum-AI-machine-learning-course/blob/master/lesson2/Machine%20learning%20on%20mobile.ipynb\n",
    "\n",
    "With everything we learned last time, perhaps we could do something similar and train a model that can perform style transfer on an image you take on your phone?\n",
    "\n",
    "Use the source code from last time to look for good ways of implementing this.\n",
    "There isn't really a right or wrong way of doing things.\n",
    "\n",
    "Perhaps you would like to look into using Floydhubs tutorial on how to build a style transfer API?\n",
    "https://docs.floydhub.com/examples/style_transfer/\n",
    "\n",
    "Or maybe you could port deeplearning.js to React Native with the help of the source code from neural style transfer in the browser?\n",
    "https://reiinakano.github.io/fast-style-transfer-deeplearnjs/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a flask application working\n",
    "Here are instructions for getting last weeks Flask API working for creating an image classifier that runs on mobile\n",
    "\n",
    "For getting your API working\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/eleijonmarck/image-preprocessing.git\n",
    "virtualenv env -p python3\n",
    "source env/bin/activate\n",
    "pip install -r image-preprocessing/cat_dog/requirements.txt\n",
    "export FLASK_APP=image-preprocessing/cat_dog/application.py\n",
    "flask run --host=0.0.0.0\n",
    "nohup flask run --host=0.0.0.0 & `\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with React Native\n",
    "Here is the code from last week React Native application.\n",
    "You can simply paste it into snack.expo.io and run it on your phone using the Expo application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import React from 'react';\n",
    "import {\n",
    "  ActivityIndicator,\n",
    "  Button,\n",
    "  Clipboard,\n",
    "  Image,\n",
    "  Share,\n",
    "  StatusBar,\n",
    "  StyleSheet,\n",
    "  Text,\n",
    "  View,\n",
    "  ScrollView\n",
    "} from 'react-native';\n",
    "import { ImagePicker } from 'expo';\n",
    "\n",
    "export default class App extends React.Component {\n",
    "  state = {\n",
    "    image: null,\n",
    "    uploading: false,\n",
    "    imageclass: false\n",
    "  };\n",
    "\n",
    "  getFbData(){\n",
    "  return fetch('https://facebook.github.io/react-native/movies.json')\n",
    "    .then((response) => response.json())\n",
    "    .then((movies) => {\n",
    "      this.setState({movies})\n",
    "      //alert(movies.movies[0].title);\n",
    "      alert(\"50% dog\")\n",
    "    })\n",
    "    .catch((error) => {\n",
    "      console.error(error);\n",
    "    });\n",
    "}\n",
    "\n",
    "componentDidMount(){\n",
    "  //this.getFbData();\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  render() {\n",
    "\n",
    "    return (\n",
    "      <ScrollView style={{ flex: 1, alignItems: 'center', justifyContent: 'center' }}>\n",
    "\n",
    "        <Text\n",
    "          style={{\n",
    "            fontSize: 20,\n",
    "            marginBottom: 20,\n",
    "            textAlign: 'center',\n",
    "            marginHorizontal: 15,\n",
    "          }}>\n",
    "          Cat vs Dog\n",
    "        </Text>\n",
    "    \n",
    "        <Image\n",
    "          source={{ uri: 'http://d23dyxeqlo5psv.cloudfront.net/cat.gif' }}\n",
    "          style={{ height: 140, width: 200 }}\n",
    "        />\n",
    "    \n",
    "    \n",
    "       \n",
    "        <Image\n",
    "          source={{ uri: 'https://media.giphy.com/media/l0HlIOM2MTxoB6tI4/giphy.gif' }}\n",
    "          style={{ height: 140, width: 200 }}\n",
    "        />\n",
    "      \n",
    "      \n",
    "\n",
    "        <Button\n",
    "          onPress={this._pickImage}\n",
    "          title=\"Pick an image from camera rolllll\"\n",
    "        />\n",
    "\n",
    "        <Button onPress={this._takePhoto} title=\"Take a photo\" />\n",
    "\n",
    "        {this._maybeRenderImage()}\n",
    "        {this._maybeRenderResultOverlay()}\n",
    "        {this._maybeRenderUploadingOverlay()} \n",
    "        <StatusBar barStyle=\"default\" />\n",
    "      </ScrollView>\n",
    "    );\n",
    "  }\n",
    "\n",
    "  _maybeRenderResultOverlay = () => {\n",
    "    if (this.state.imageclass) {\n",
    "      return (\n",
    "        <View\n",
    "          style={[\n",
    "            StyleSheet.absoluteFill,\n",
    "            {\n",
    "              backgroundColor: 'pink',\n",
    "              alignItems: 'center',\n",
    "              justifyContent: 'center',\n",
    "            },\n",
    "          ]}>\n",
    "          <Text\n",
    "          style={{\n",
    "            fontSize: 20,\n",
    "            marginBottom: 20,\n",
    "            textAlign: 'center',\n",
    "            marginHorizontal: 15,\n",
    "          }}>\n",
    "        {this.state.imageclass}\n",
    "        </Text>\n",
    "\n",
    "        </View>\n",
    "      );\n",
    "    }\n",
    "  };\n",
    "  _maybeRenderUploadingOverlay = () => {\n",
    "    if (this.state.uploading) {\n",
    "      return (\n",
    "        <View\n",
    "          style={[\n",
    "            StyleSheet.absoluteFill,\n",
    "            {\n",
    "              backgroundColor: 'rgba(0,0,0,0.4)',\n",
    "              alignItems: 'center',\n",
    "              justifyContent: 'center',\n",
    "            },\n",
    "          ]}>\n",
    "          <ActivityIndicator color=\"#fff\" animating size=\"large\" />\n",
    "        </View>\n",
    "      );\n",
    "    }\n",
    "  };\n",
    "\n",
    "  _maybeRenderImage = () => {\n",
    "    let { image } = this.state;\n",
    "    if (!image) {\n",
    "      return;\n",
    "    }\n",
    "\n",
    "    return (\n",
    "      <View\n",
    "        style={{\n",
    "          marginTop: 30,\n",
    "          width: 250,\n",
    "          borderRadius: 3,\n",
    "          elevation: 2,\n",
    "          shadowColor: 'rgba(0,0,0,1)',\n",
    "          shadowOpacity: 0.2,\n",
    "          shadowOffset: { width: 4, height: 4 },\n",
    "          shadowRadius: 5,\n",
    "        }}>\n",
    "        <View\n",
    "          style={{\n",
    "            borderTopRightRadius: 3,\n",
    "            borderTopLeftRadius: 3,\n",
    "            overflow: 'hidden',\n",
    "          }}>\n",
    "            \n",
    "          \n",
    "          <Image source={{ uri: image }} style={{ width: 250, height: 250 }} /> \n",
    "        </View>\n",
    "\n",
    "        <Text\n",
    "          onPress={this._copyToClipboard}\n",
    "          onLongPress={this._share}\n",
    "          style={{ paddingVertical: 10, paddingHorizontal: 10 }}>\n",
    "          {image}\n",
    "        </Text>\n",
    "      </View>\n",
    "    );\n",
    "  };\n",
    "\n",
    "  _share = () => {\n",
    "    Share.share({\n",
    "      message: this.state.image,\n",
    "      title: 'Check out this photo',\n",
    "      url: this.state.image,\n",
    "    });\n",
    "  };\n",
    "\n",
    "  _copyToClipboard = () => {\n",
    "    Clipboard.setString(this.state.image);\n",
    "    alert('Copied image URL to clipboard');\n",
    "  };\n",
    "\n",
    "  _takePhoto = async () => {\n",
    "    let pickerResult = await ImagePicker.launchCameraAsync({\n",
    "      allowsEditing: true,\n",
    "      aspect: [4, 3],\n",
    "    });\n",
    "\n",
    "    this._handleImagePicked(pickerResult);\n",
    "  };\n",
    "\n",
    "  _pickImage = async () => {\n",
    "    let pickerResult = await ImagePicker.launchImageLibraryAsync({\n",
    "      allowsEditing: true,\n",
    "      aspect: [4, 3],\n",
    "    });\n",
    "\n",
    "    this._handleImagePicked(pickerResult);\n",
    "  };\n",
    "\n",
    "  _handleImagePicked = async pickerResult => {\n",
    "    let uploadResponse, uploadResult;\n",
    "\n",
    "    try {\n",
    "      this.setState({ uploading: true });\n",
    "\n",
    "      if (!pickerResult.cancelled) {\n",
    "        uploadResponse = await uploadImageAsync(pickerResult.uri);\n",
    "        uploadResult = await uploadResponse.json();\n",
    "        this.setState({ image: uploadResult.location });\n",
    "        if(uploadResult.classification.dog > uploadResult.classification.cat) {\n",
    "          this.setState({ imageclass: 'Dog' });\n",
    "        } else {\n",
    "          this.setState({ imageclass: 'Cat' });\n",
    "        }\n",
    "        \n",
    "      }\n",
    "    } catch (e) {\n",
    "      console.log({ uploadResponse });\n",
    "      console.log({ uploadResult });\n",
    "      console.log({ e });\n",
    "      alert('Upload failed, sorry :(');\n",
    "    } finally {\n",
    "      this.setState({ uploading: false });\n",
    "    }\n",
    "  };\n",
    "}\n",
    "\n",
    "async function uploadImageAsync(uri) {\n",
    "  let apiUrl = 'http://82.196.10.39:5000/predict';\n",
    "\n",
    "  // Send image to our API\n",
    "  \n",
    "  // Change text based on API response\n",
    "  \n",
    "  \n",
    "  \n",
    "  // Note:\n",
    "  // Uncomment this if you want to experiment with local server\n",
    "  //\n",
    "  // if (Constants.isDevice) {\n",
    "  //   apiUrl = `https://your-ngrok-subdomain.ngrok.io/upload`;\n",
    "  // } else {\n",
    "  //   apiUrl = `http://localhost:3000/upload`\n",
    "  // }\n",
    "\n",
    "\n",
    "  //let uriParts = uri.split('.');\n",
    "  let fileType = uri[uri.length - 1];\n",
    "\n",
    "  let formData = new FormData();\n",
    "  formData.append('photo', {\n",
    "    uri,\n",
    "    name: `photo.${fileType}`,\n",
    "    type: `image/${fileType}`,\n",
    "  });\n",
    "\n",
    "  let options = {\n",
    "    method: 'POST',\n",
    "    body: formData,\n",
    "    headers: {\n",
    "      Accept: 'application/json',\n",
    "      'Content-Type': 'multipart/form-data',\n",
    "    },\n",
    "  };\n",
    "\n",
    "  return fetch(apiUrl, options);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
