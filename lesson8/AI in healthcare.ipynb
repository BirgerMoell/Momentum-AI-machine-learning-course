{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI in healthcare\n",
    "\n",
    "![title](https://www.researchgate.net/profile/Kylie_Rochford/publication/260761456/figure/fig2/AS:273959681785862@1442328486230/Figure-2-The-default-mode-network-DMN-Strong-overlap-in-the-DMN-representation-based.png)\n",
    "This work will give some examples of datasets related to healthcare. First you will work with brain imaging data and then you will get to try out making a convolutional neural network.\n",
    "\n",
    "Even if the convolutional neural network currently train on cats vs dogs the classification algorithms works in a similar way if you want to make classifications on health data. \n",
    "\n",
    "To get started, first download the necessary depenencies.\n",
    "\n",
    "## Python 3\n",
    "http://python.org\n",
    "\n",
    "## Tensorflow\n",
    "http://tensorflow.org\n",
    "\n",
    "## Keras\n",
    "http://keras.io\n",
    "\n",
    "## Jupyter notebooks\n",
    "http://jupyter.org\n",
    "\n",
    "## Anaconda\n",
    "https://anaconda.org/\n",
    "\n",
    "Then clone this repository and open up the AI in Healthcare file in Jupyter notebook to get started\n",
    "https://github.com/bcarlyle/Momentum-AI-machine-learning-course/blob/master/lesson8/AI%20in%20healthcare.ipynb\n",
    "\n",
    "Artificial intelligence (AI) in healthcare uses algorithms and software to approximate human cognition in the analysis of complex medical data. The primary aim of health-related AI applications is to analyze relationships between prevention or treatment techniques and patient outcomes.[1] AI programs have been developed and applied to practices such as diagnosis processes, treatment protocol development, drug development, personalized medicine and patient monitoring and care, among others. Medical institutions such as The Mayo Clinic, Memorial Sloan Kettering Cancer Center[2] and National Health Service,[3] multinational technology companies such as IBM[4] and Google[3] and startups such as Welltok and Ayasdi,[5] have created solutions currently used in the industry. Healthcare remains the top area of investment in AI as measured by venture capital deal flow.[5]\n",
    "\n",
    "History[edit]\n",
    "Research in the 1960s and 1970s produced the first problem-solving program, or expert system, known as Dendral.[6] While it was designed for applications in organic chemistry, it provided the basis for the subsequent system MYCIN,[7] considered one of the most significant early uses of artificial intelligence in medicine,.[7][8] MYCIN and other systems such as INTERNIST-1 and CASNET did not achieve routine use by practitioners however.[9]\n",
    "\n",
    "The 1980s and 1990s brought the proliferation of the microcomputer and new levels of network connectivity, as well as the recognition by researchers and developers that AI systems in healthcare must be designed to accommodate the absence of perfect data and build on the expertise of physician users.[10] New approaches involving fuzzy set theory,[11] Bayesian networks[12] and artificial neural networks,[13][14] were created to reflect the evolved needs of intelligent computing systems in healthcare.\n",
    "\n",
    "Medical and technological advancements occurring over this half-century period that have simultaneously enabled the growth healthcare-related applications of AI include:\n",
    "\n",
    "Improvements in computing power resulting in faster data collection and data processing[15]\n",
    "Increased volume and availability of health-related data from personal and healthcare-related devices[16]\n",
    "Growth of genomic sequencing databases[17]\n",
    "Widespread implementation of electronic health record systems[18]\n",
    "Improvements in natural language processing and computer vision, enabling machines to replicate human perceptual processes,[19][20]\n",
    "Examples[edit]\n",
    "IBM[edit]\n",
    "IBM's Watson Oncology is in development at Memorial Sloan Kettering Cancer Center and Cleveland Clinic.[21] IBM is also working with CVS Health on AI applications in chronic disease treatment and with Johnson & Johnson on analysis of scientific papers to find new connections for drug development.[22]\n",
    "\n",
    "Microsoft[edit]\n",
    "Microsoft's Hanover project, in partnership with Oregon Health & Science University's Knight Cancer Institute, analyzes medical research to predict the most effective cancer drug treatment options for patients.[23] Other projects include medical image analysis of tumor progression and the development of programmable cells.[24]\n",
    "\n",
    "Google[edit]\n",
    "Google's DeepMind platform is being used by the UK National Health Service to detect certain health risks through data collected via a mobile app.[25] A second project with the NHS involves analysis of medical images collected from NHS patients to develop computer vision algorithms to detect cancerous tissues.[26]\n",
    "\n",
    "Intel[edit]\n",
    "Intel's venture capital arm Intel Capital recently invested in startup Lumiata which uses AI to identify at-risk patients and develop care options.[27]\n",
    "\n",
    "Startups[edit]\n",
    "Predictive Medical Technologies uses intensive care unit data to identify patients likely to suffer cardiac incidents.[21] Modernizing Medicine uses knowledge gathered from healthcare professionals as well as patient outcome data to recommend treatments.[28] Nimblr.ai uses an A.I. Chatbot to connect scheduling Electronic health record systems and automate the confirmation and scheduling of patients.[29]\n",
    "\n",
    "Regulation[edit]\n",
    "In May 2016, the White House announced its plan to host a series of workshops and formation of the National Science and Technology Council (NSTC) Subcommittee on Machine Learning and Artificial Intelligence.[30] In October 2016, the group published The National Artificial Intelligence Research and Development Strategic Plan, outlining its proposed priorities for Federally-funded AI research and development (within government and academia). The report notes a strategic R&D plan for the subfield of health information technology is in development stages.[31]\n",
    "\n",
    "Investments from the US government in healthcare initiatives that will rely on AI[30] include its $1B proposed budget for the Cancer Moonshot[32] and $215M proposed investment in the Precision Medicine Initiative.[33]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is AI?\n",
    "\n",
    "https://en.wikipedia.org/wiki/Artificial_intelligence\n",
    "\n",
    "AI or Artificial Intelligence is the idea that intelligence can be defined so precisely that we can make an make a machine perform all the required steps to become intelligent.\n",
    "\n",
    "One of the fields long term goals is general intelligence, making machine that can think like humans.\n",
    "\n",
    "## Why is AI suddenly so popular?\n",
    "AI has suddenly become incredibly popular. It is mainly based on two factors, the increased computational powers of modern computers and the availability of huge datasets.\n",
    "\n",
    "AI like people gain information from the world that they use to make predictions on how to act. The more information, the better the prediction. \n",
    "\n",
    "## AI and neuroscience\n",
    "AI and neuroscience are two fields that are becoming more and more intertwined. The most effective forms of AI are based on theories for how the brain works. Artificial neural networks are modeled on the biological neural networks of the brain. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a Jupyter notebook. \n",
    "\n",
    "It's an interactive way to write Python code. Most of machine learning is done using Python because Python is great for data analysis and the biggest machine learning libraries (Tensorflow and Keras) are made in Python.\n",
    "\n",
    "Here you can learn how to get started with Jupyter Notebooks.\n",
    "http://jupyter.org/\n",
    "\n",
    "In Machine Learning there really isn't a big question of what language to use. The answer is obviously Python. So just start learning Python.\n",
    "https://www.python.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is machine learning?\n",
    "![Datasteps](files/datasteps.jpg)\n",
    "\n",
    "Machine learning is a part of AI. It is the subfield of computer science that, according to Arthur Samuel in 1959, gives \"computers the ability to learn without being explicitly programmed.\"\n",
    "https://en.wikipedia.org/wiki/Machine_learning\n",
    "\n",
    "Machine Learning is a way to make computers make decisions based on data. There are two types of machine learning, supervised learning and unsupervised learning.\n",
    "\n",
    "## Supervised Learning\n",
    "Our work will focus on supervised learning because it is the field that is making the most progress right now and is the most mature. So it's the easiest to get up and running in production. \n",
    "\n",
    "For all supervised learning , the first rule of machine learning is this.\n",
    "All your output is determined on your input.\n",
    "\n",
    "## Garbaged in -> Garbaged out\n",
    "\n",
    "So the biggest problem in machine learning isn't always how to train your machine learning models, it's how to get good data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do I need to do in order to get started with machine learning?\n",
    "![Machine Learning Map](files/machinelearningmap.png)\n",
    "\n",
    "## AI is a fast moving discipline\n",
    "Machine learning and AI is a huge discipline that is always growing. However it's easy to get started and it's moving really fast so you can just grab on and stay on the bleeding edge of technology.\n",
    "\n",
    "## Tensorflow\n",
    "Google open sourcing Tensorflow https://www.tensorflow.org/ made it much easier to get started in machine learning. What would have taken months to do ten years ago can now be accomplished in days.\n",
    "\n",
    "## Keras\n",
    "Even better, Keras came along as an abstraction on top of Tensorflow for deep learning, one of the most advanced parts of deep learning. \n",
    "\n",
    "## Types of machine learning\n",
    "\n",
    "There are many different types of machine learning. The most advances in machine learning have been done using artificial neural networks, so if you want to start out doing just one thing, start out with artificial neural networks.\n",
    "\n",
    "Here is a list of different types of machine learning.\n",
    "\n",
    "Decision tree learning\n",
    "Association rule learning\n",
    "Artificial neural networks\n",
    "Deep learning\n",
    "Inductive logic programming\n",
    "Support vector machines\n",
    "Clustering\n",
    "Bayesian networks\n",
    "Reinforcement learning\n",
    "Representation learning\n",
    "Similarity and metric learning\n",
    "Sparse dictionary learning\n",
    "Genetic algorithms\n",
    "Rule-based machine learning\n",
    "Learning classifier systems\n",
    "\n",
    "\n",
    "## What you need to know about data and machine learning\n",
    "Machine learning is a way to make advanced statistical models using math. It’s a way to make a computer guess. For machine learning to work you need good data, the more data the better. So before you go further it might be good to take a moment and think about data. The reason why machine learning models are becoming so good is that we have so much data. However machine learning models can’t perform magic (even thought it might seem like it).\n",
    "\n",
    "## Garbage in = Garbage out\n",
    "\n",
    "Take a moment and think about what problem you are trying to solve? What data do you need for solving your problem?\n",
    "\n",
    "## Classic machine learning datasets\n",
    "\n",
    "## MNIST\n",
    "MNIST is a dataset of handwritten digits. It is the hello world of machine learning. The goal with MNIST is to classify (guess) which handwritten digit is displayed by use of a machine learning model.\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "## IRIS\n",
    "IRIS is another equally famous machine learning dataset. Iris is a dataset for classifying different types of flowers.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/iris\n",
    "\n",
    "\n",
    "## Open Datasets from Kaggle\n",
    "\n",
    "Here are places you can find ready made datasets for machine learning\n",
    "http://kaggle.com/\n",
    "\n",
    "Here is a list of all the datasets available at Kaggle with a link on where to download them. \n",
    "\n",
    "Dataset\n",
    "Link\n",
    "IMDB\n",
    "\n",
    "https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset\n",
    "Soccer scores dataset\n",
    "https://www.kaggle.com/hugomathien/soccer\n",
    "Credit card fraud detection\n",
    "\n",
    "\n",
    "https://www.kaggle.com/dalpozz/creditcardfraud\n",
    "Human resources analytics\n",
    "\n",
    "\n",
    "https://www.kaggle.com/ludobenistant/hr-analytics\n",
    "Food facts\n",
    "\n",
    "\n",
    "https://www.kaggle.com/openfoodfacts/world-food-facts\n",
    "Climate change\n",
    "\n",
    "\n",
    "https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "Daily news for stock market predictions\n",
    "https://www.kaggle.com/aaron7sun/stocknews\n",
    "New York Stock Exchange\n",
    "https://www.kaggle.com/dgawlik/nyse\n",
    "US Stocks Fundamentals\n",
    "\n",
    "\n",
    "https://www.kaggle.com/usfundamentals/us-stocks-fundamentals\n",
    "Bitcoin historical data\n",
    "https://www.kaggle.com/mczielinski/bitcoin-historical-data\n",
    "Adult census income, predict earning based on census\n",
    "https://www.kaggle.com/uciml/adult-census-income\n",
    "House sales in King County\n",
    "https://www.kaggle.com/harlfoxem/housesalesprediction\n",
    "EEG brain waves\n",
    "https://www.kaggle.com/wanghaohan/eeg-brain-wave-for-confusion\n",
    "Synchronized brain wave dataset\n",
    "https://www.kaggle.com/berkeley-biosense/synchronized-brainwave-dataset\n",
    "CT medical image analysis \n",
    "https://www.kaggle.com/kmader/siim-medical-image-analysis-tutorial\n",
    "Structural MRI Dataset\n",
    "https://www.kaggle.com/ilknuricke/neurohackinginrimages\n",
    "Circadian rhythm in the brain\n",
    "https://www.kaggle.com/kmader/circadian-rhythm-in-the-brain\n",
    "Complete genome dataset\n",
    "https://www.kaggle.com/zusmani/mygenome\n",
    "Question answers dataset 100 000 + questions\n",
    "https://www.kaggle.com/stanfordu/stanford-question-answering-dataset\n",
    "What is a note\n",
    "https://www.kaggle.com/juliancienfuegos/what-is-a-note\n",
    "Health nutrition and population across the world\n",
    "https://www.kaggle.com/theworldbank/health-nutrition-and-population-statistics\n",
    "\n",
    "\n",
    "## Scrape data for machine learning\n",
    "Scraping is another way to get data. Scraping means to request data from websites and store it offline for later analysis.\n",
    "\n",
    "Here you can learn more about scraping.\n",
    "\n",
    "https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an artificial neural network?\n",
    "![Machine Learning Map](files/neuralnetwork.png)\n",
    "\n",
    "The part of machine learning that has been making the most progress in the last years are artificial neural networks, a form of machine learning that is loosely modeled on how the neurons in our brain works. Artificual neural network has been proved really effective at making progress in many different domains.\n",
    "\n",
    "For getting started with artificial neural networks, Keras is an excellent library built on top of the Tensorflow API.\n",
    "https://keras.io/\n",
    "\n",
    "\n",
    "## Here is a good video explaining artificial neural networks\n",
    "https://www.youtube.com/watch?v=ILsA4nyG7I0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The math of machine learning\n",
    "\n",
    "![Image of Activation function](files/activation.jpeg)\n",
    "The truth is that math is a big part of machine learning but you don't need to know any math to get started with machine learning. It's an excellent idea to pick up the math along the way and to expand the field of machine learning you definitely need to learn math.\n",
    "\n",
    "If you are curious, here are some resources for looking at the math behind deep learning.\n",
    "\n",
    "## The different math sources\n",
    "AI draws math from three different sources, linear algebra, calculus and statistics. \n",
    "\n",
    "### Linear algebra\n",
    "http://machinelearningmastery.com/linear-algebra-machine-learning/\n",
    "\n",
    "### Calculus\n",
    "https://www.umiacs.umd.edu/~hal/courses/2013S_ML/math4ml.pdf\n",
    "\n",
    "### Statistics\n",
    "http://machinelearningmastery.com/crash-course-statistics-machine-learning/\n",
    "\n",
    "## Neural network from scratch\n",
    "Building a simple neural network from scratch. Walkthrough and experiments. \n",
    "http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets train our first neural network!\n",
    "To train a neural network we need two things.\n",
    "1. A dataset with a classifier to train on\n",
    "2. A mathematical neural network\n",
    "\n",
    "## Task default dataset from my brain\n",
    "To make it fun, lets use a dataset that takes input data from my brain measured with laser sensors to try to determine what brain region is active, the default mode network or the task positive network.\n",
    "\n",
    "The data takes measurement from two probes placed on my forehead and returns eight values of oxygenated and deoxygenated blood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Oxy1</th>\n",
       "      <th>DeOxy1</th>\n",
       "      <th>Oxy2</th>\n",
       "      <th>DeOxy2</th>\n",
       "      <th>Oxy3</th>\n",
       "      <th>DeOxy3</th>\n",
       "      <th>Oxy4</th>\n",
       "      <th>DeOxy4</th>\n",
       "      <th>Oxy5</th>\n",
       "      <th>DeOxy5</th>\n",
       "      <th>Oxy6</th>\n",
       "      <th>DeOxy6</th>\n",
       "      <th>Oxy7</th>\n",
       "      <th>DeOxy7</th>\n",
       "      <th>Oxy8</th>\n",
       "      <th>DeOxy8</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-2.590000e-305</td>\n",
       "      <td>3.087000e-305</td>\n",
       "      <td>-4.040000e-306</td>\n",
       "      <td>4.816000e-306</td>\n",
       "      <td>-4.747000e-306</td>\n",
       "      <td>5.658000e-306</td>\n",
       "      <td>-3.440000e-306</td>\n",
       "      <td>4.100000e-306</td>\n",
       "      <td>5.880000e-305</td>\n",
       "      <td>-7.009000e-305</td>\n",
       "      <td>4.894000e-306</td>\n",
       "      <td>-5.833000e-306</td>\n",
       "      <td>3.826000e-306</td>\n",
       "      <td>-4.561000e-306</td>\n",
       "      <td>1.449000e-306</td>\n",
       "      <td>-1.727000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-2.315000e-305</td>\n",
       "      <td>2.759000e-305</td>\n",
       "      <td>-3.601000e-306</td>\n",
       "      <td>4.292000e-306</td>\n",
       "      <td>-4.902000e-306</td>\n",
       "      <td>5.843000e-306</td>\n",
       "      <td>-3.444000e-306</td>\n",
       "      <td>4.106000e-306</td>\n",
       "      <td>6.224000e-305</td>\n",
       "      <td>-7.418000e-305</td>\n",
       "      <td>4.948000e-306</td>\n",
       "      <td>-5.898000e-306</td>\n",
       "      <td>3.715000e-306</td>\n",
       "      <td>-4.428000e-306</td>\n",
       "      <td>1.947000e-306</td>\n",
       "      <td>-2.321000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-2.571000e-305</td>\n",
       "      <td>3.065000e-305</td>\n",
       "      <td>-3.766000e-306</td>\n",
       "      <td>4.489000e-306</td>\n",
       "      <td>-4.511000e-306</td>\n",
       "      <td>5.376000e-306</td>\n",
       "      <td>-3.847000e-306</td>\n",
       "      <td>4.586000e-306</td>\n",
       "      <td>5.882000e-305</td>\n",
       "      <td>-7.011000e-305</td>\n",
       "      <td>4.274000e-306</td>\n",
       "      <td>-5.095000e-306</td>\n",
       "      <td>4.150000e-306</td>\n",
       "      <td>-4.947000e-306</td>\n",
       "      <td>1.831000e-306</td>\n",
       "      <td>-2.182000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-3.335000e-305</td>\n",
       "      <td>3.975000e-305</td>\n",
       "      <td>-4.188000e-306</td>\n",
       "      <td>4.992000e-306</td>\n",
       "      <td>-4.373000e-306</td>\n",
       "      <td>5.213000e-306</td>\n",
       "      <td>-3.726000e-306</td>\n",
       "      <td>4.442000e-306</td>\n",
       "      <td>5.547000e-305</td>\n",
       "      <td>-6.612000e-305</td>\n",
       "      <td>4.184000e-306</td>\n",
       "      <td>-4.987000e-306</td>\n",
       "      <td>3.722000e-306</td>\n",
       "      <td>-4.436000e-306</td>\n",
       "      <td>1.948000e-306</td>\n",
       "      <td>-2.322000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-2.897000e-305</td>\n",
       "      <td>3.453000e-305</td>\n",
       "      <td>-4.086000e-306</td>\n",
       "      <td>4.870000e-306</td>\n",
       "      <td>-3.768000e-306</td>\n",
       "      <td>4.491000e-306</td>\n",
       "      <td>-3.724000e-306</td>\n",
       "      <td>4.439000e-306</td>\n",
       "      <td>5.415000e-305</td>\n",
       "      <td>-6.454000e-305</td>\n",
       "      <td>3.878000e-306</td>\n",
       "      <td>-4.622000e-306</td>\n",
       "      <td>3.749000e-306</td>\n",
       "      <td>-4.469000e-306</td>\n",
       "      <td>1.614000e-306</td>\n",
       "      <td>-1.923000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>-3.190000e-305</td>\n",
       "      <td>3.802000e-305</td>\n",
       "      <td>-4.075000e-306</td>\n",
       "      <td>4.857000e-306</td>\n",
       "      <td>-4.281000e-306</td>\n",
       "      <td>5.103000e-306</td>\n",
       "      <td>-3.555000e-306</td>\n",
       "      <td>4.237000e-306</td>\n",
       "      <td>5.650000e-305</td>\n",
       "      <td>-6.734000e-305</td>\n",
       "      <td>4.038000e-306</td>\n",
       "      <td>-4.813000e-306</td>\n",
       "      <td>3.536000e-306</td>\n",
       "      <td>-4.214000e-306</td>\n",
       "      <td>1.665000e-306</td>\n",
       "      <td>-1.985000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>-2.973000e-305</td>\n",
       "      <td>3.544000e-305</td>\n",
       "      <td>-3.587000e-306</td>\n",
       "      <td>4.275000e-306</td>\n",
       "      <td>-4.187000e-306</td>\n",
       "      <td>4.991000e-306</td>\n",
       "      <td>-3.385000e-306</td>\n",
       "      <td>4.035000e-306</td>\n",
       "      <td>5.781000e-305</td>\n",
       "      <td>-6.891000e-305</td>\n",
       "      <td>4.578000e-306</td>\n",
       "      <td>-5.457000e-306</td>\n",
       "      <td>3.904000e-306</td>\n",
       "      <td>-4.653000e-306</td>\n",
       "      <td>1.846000e-306</td>\n",
       "      <td>-2.201000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-2.889000e-305</td>\n",
       "      <td>3.443000e-305</td>\n",
       "      <td>-4.629000e-306</td>\n",
       "      <td>5.518000e-306</td>\n",
       "      <td>-4.397000e-306</td>\n",
       "      <td>5.241000e-306</td>\n",
       "      <td>-3.511000e-306</td>\n",
       "      <td>4.185000e-306</td>\n",
       "      <td>5.751000e-305</td>\n",
       "      <td>-6.855000e-305</td>\n",
       "      <td>4.372000e-306</td>\n",
       "      <td>-5.211000e-306</td>\n",
       "      <td>3.611000e-306</td>\n",
       "      <td>-4.305000e-306</td>\n",
       "      <td>1.959000e-306</td>\n",
       "      <td>-2.336000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>-2.816000e-305</td>\n",
       "      <td>3.357000e-305</td>\n",
       "      <td>-3.981000e-306</td>\n",
       "      <td>4.745000e-306</td>\n",
       "      <td>-4.038000e-306</td>\n",
       "      <td>4.814000e-306</td>\n",
       "      <td>-3.389000e-306</td>\n",
       "      <td>4.039000e-306</td>\n",
       "      <td>6.264000e-305</td>\n",
       "      <td>-7.466000e-305</td>\n",
       "      <td>4.382000e-306</td>\n",
       "      <td>-5.224000e-306</td>\n",
       "      <td>3.372000e-306</td>\n",
       "      <td>-4.019000e-306</td>\n",
       "      <td>1.476000e-306</td>\n",
       "      <td>-1.760000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-3.034000e-305</td>\n",
       "      <td>3.616000e-305</td>\n",
       "      <td>-3.769000e-306</td>\n",
       "      <td>4.493000e-306</td>\n",
       "      <td>-4.417000e-306</td>\n",
       "      <td>5.264000e-306</td>\n",
       "      <td>-3.315000e-306</td>\n",
       "      <td>3.952000e-306</td>\n",
       "      <td>6.293000e-305</td>\n",
       "      <td>-7.501000e-305</td>\n",
       "      <td>4.328000e-306</td>\n",
       "      <td>-5.159000e-306</td>\n",
       "      <td>3.911000e-306</td>\n",
       "      <td>-4.662000e-306</td>\n",
       "      <td>1.501000e-306</td>\n",
       "      <td>-1.790000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>-2.485000e-305</td>\n",
       "      <td>2.962000e-305</td>\n",
       "      <td>-4.116000e-306</td>\n",
       "      <td>4.906000e-306</td>\n",
       "      <td>-4.311000e-306</td>\n",
       "      <td>5.138000e-306</td>\n",
       "      <td>-3.056000e-306</td>\n",
       "      <td>3.643000e-306</td>\n",
       "      <td>5.568000e-305</td>\n",
       "      <td>-6.636000e-305</td>\n",
       "      <td>4.165000e-306</td>\n",
       "      <td>-4.965000e-306</td>\n",
       "      <td>3.611000e-306</td>\n",
       "      <td>-4.304000e-306</td>\n",
       "      <td>1.627000e-306</td>\n",
       "      <td>-1.940000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>-2.536000e-305</td>\n",
       "      <td>3.023000e-305</td>\n",
       "      <td>-4.095000e-306</td>\n",
       "      <td>4.881000e-306</td>\n",
       "      <td>-4.130000e-306</td>\n",
       "      <td>4.923000e-306</td>\n",
       "      <td>-3.339000e-306</td>\n",
       "      <td>3.980000e-306</td>\n",
       "      <td>5.815000e-305</td>\n",
       "      <td>-6.931000e-305</td>\n",
       "      <td>5.019000e-306</td>\n",
       "      <td>-5.982000e-306</td>\n",
       "      <td>3.215000e-306</td>\n",
       "      <td>-3.832000e-306</td>\n",
       "      <td>1.698000e-306</td>\n",
       "      <td>-2.023000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>-2.029000e-305</td>\n",
       "      <td>2.419000e-305</td>\n",
       "      <td>-3.427000e-306</td>\n",
       "      <td>4.085000e-306</td>\n",
       "      <td>-3.911000e-306</td>\n",
       "      <td>4.662000e-306</td>\n",
       "      <td>-3.039000e-306</td>\n",
       "      <td>3.622000e-306</td>\n",
       "      <td>5.078000e-305</td>\n",
       "      <td>-6.053000e-305</td>\n",
       "      <td>4.064000e-306</td>\n",
       "      <td>-4.844000e-306</td>\n",
       "      <td>3.347000e-306</td>\n",
       "      <td>-3.990000e-306</td>\n",
       "      <td>1.565000e-306</td>\n",
       "      <td>-1.866000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>-2.618000e-305</td>\n",
       "      <td>3.121000e-305</td>\n",
       "      <td>-3.072000e-306</td>\n",
       "      <td>3.662000e-306</td>\n",
       "      <td>-3.469000e-306</td>\n",
       "      <td>4.136000e-306</td>\n",
       "      <td>-3.090000e-306</td>\n",
       "      <td>3.683000e-306</td>\n",
       "      <td>4.812000e-305</td>\n",
       "      <td>-5.735000e-305</td>\n",
       "      <td>3.648000e-306</td>\n",
       "      <td>-4.349000e-306</td>\n",
       "      <td>3.161000e-306</td>\n",
       "      <td>-3.768000e-306</td>\n",
       "      <td>1.328000e-306</td>\n",
       "      <td>-1.583000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>-2.634000e-305</td>\n",
       "      <td>3.140000e-305</td>\n",
       "      <td>-3.259000e-306</td>\n",
       "      <td>3.885000e-306</td>\n",
       "      <td>-3.590000e-306</td>\n",
       "      <td>4.279000e-306</td>\n",
       "      <td>-2.856000e-306</td>\n",
       "      <td>3.404000e-306</td>\n",
       "      <td>4.955000e-305</td>\n",
       "      <td>-5.906000e-305</td>\n",
       "      <td>3.360000e-306</td>\n",
       "      <td>-4.005000e-306</td>\n",
       "      <td>2.755000e-306</td>\n",
       "      <td>-3.283000e-306</td>\n",
       "      <td>1.411000e-306</td>\n",
       "      <td>-1.682000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>-2.816000e-305</td>\n",
       "      <td>3.357000e-305</td>\n",
       "      <td>-2.989000e-306</td>\n",
       "      <td>3.563000e-306</td>\n",
       "      <td>-3.591000e-306</td>\n",
       "      <td>4.280000e-306</td>\n",
       "      <td>-2.647000e-306</td>\n",
       "      <td>3.155000e-306</td>\n",
       "      <td>4.466000e-305</td>\n",
       "      <td>-5.323000e-305</td>\n",
       "      <td>3.574000e-306</td>\n",
       "      <td>-4.261000e-306</td>\n",
       "      <td>3.172000e-306</td>\n",
       "      <td>-3.781000e-306</td>\n",
       "      <td>1.334000e-306</td>\n",
       "      <td>-1.590000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>-2.703000e-305</td>\n",
       "      <td>3.222000e-305</td>\n",
       "      <td>-3.461000e-306</td>\n",
       "      <td>4.126000e-306</td>\n",
       "      <td>-3.694000e-306</td>\n",
       "      <td>4.403000e-306</td>\n",
       "      <td>-2.949000e-306</td>\n",
       "      <td>3.515000e-306</td>\n",
       "      <td>5.087000e-305</td>\n",
       "      <td>-6.064000e-305</td>\n",
       "      <td>3.492000e-306</td>\n",
       "      <td>-4.162000e-306</td>\n",
       "      <td>2.795000e-306</td>\n",
       "      <td>-3.331000e-306</td>\n",
       "      <td>1.578000e-306</td>\n",
       "      <td>-1.881000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>-2.982000e-305</td>\n",
       "      <td>3.554000e-305</td>\n",
       "      <td>-3.309000e-306</td>\n",
       "      <td>3.944000e-306</td>\n",
       "      <td>-3.950000e-306</td>\n",
       "      <td>4.709000e-306</td>\n",
       "      <td>-3.024000e-306</td>\n",
       "      <td>3.605000e-306</td>\n",
       "      <td>5.476000e-305</td>\n",
       "      <td>-6.527000e-305</td>\n",
       "      <td>3.473000e-306</td>\n",
       "      <td>-4.140000e-306</td>\n",
       "      <td>3.110000e-306</td>\n",
       "      <td>-3.707000e-306</td>\n",
       "      <td>1.446000e-306</td>\n",
       "      <td>-1.724000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>-2.726000e-305</td>\n",
       "      <td>3.250000e-305</td>\n",
       "      <td>-3.910000e-306</td>\n",
       "      <td>4.661000e-306</td>\n",
       "      <td>-3.993000e-306</td>\n",
       "      <td>4.759000e-306</td>\n",
       "      <td>-3.171000e-306</td>\n",
       "      <td>3.780000e-306</td>\n",
       "      <td>5.569000e-305</td>\n",
       "      <td>-6.638000e-305</td>\n",
       "      <td>3.989000e-306</td>\n",
       "      <td>-4.755000e-306</td>\n",
       "      <td>3.163000e-306</td>\n",
       "      <td>-3.770000e-306</td>\n",
       "      <td>1.583000e-306</td>\n",
       "      <td>-1.887000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>-3.112000e-305</td>\n",
       "      <td>3.709000e-305</td>\n",
       "      <td>-4.601000e-306</td>\n",
       "      <td>5.485000e-306</td>\n",
       "      <td>-4.158000e-306</td>\n",
       "      <td>4.956000e-306</td>\n",
       "      <td>-3.275000e-306</td>\n",
       "      <td>3.904000e-306</td>\n",
       "      <td>5.714000e-305</td>\n",
       "      <td>-6.811000e-305</td>\n",
       "      <td>4.346000e-306</td>\n",
       "      <td>-5.180000e-306</td>\n",
       "      <td>3.027000e-306</td>\n",
       "      <td>-3.609000e-306</td>\n",
       "      <td>1.652000e-306</td>\n",
       "      <td>-1.969000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>-2.860000e-305</td>\n",
       "      <td>3.409000e-305</td>\n",
       "      <td>-4.241000e-306</td>\n",
       "      <td>5.056000e-306</td>\n",
       "      <td>-4.624000e-306</td>\n",
       "      <td>5.511000e-306</td>\n",
       "      <td>-3.714000e-306</td>\n",
       "      <td>4.427000e-306</td>\n",
       "      <td>5.988000e-305</td>\n",
       "      <td>-7.138000e-305</td>\n",
       "      <td>4.666000e-306</td>\n",
       "      <td>-5.561000e-306</td>\n",
       "      <td>3.811000e-306</td>\n",
       "      <td>-4.542000e-306</td>\n",
       "      <td>1.465000e-306</td>\n",
       "      <td>-1.747000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>-2.848000e-305</td>\n",
       "      <td>3.395000e-305</td>\n",
       "      <td>-4.471000e-306</td>\n",
       "      <td>5.329000e-306</td>\n",
       "      <td>-4.251000e-306</td>\n",
       "      <td>5.067000e-306</td>\n",
       "      <td>-3.472000e-306</td>\n",
       "      <td>4.139000e-306</td>\n",
       "      <td>6.074000e-305</td>\n",
       "      <td>-7.240000e-305</td>\n",
       "      <td>4.696000e-306</td>\n",
       "      <td>-5.598000e-306</td>\n",
       "      <td>3.842000e-306</td>\n",
       "      <td>-4.579000e-306</td>\n",
       "      <td>1.466000e-306</td>\n",
       "      <td>-1.748000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>-2.363000e-305</td>\n",
       "      <td>2.816000e-305</td>\n",
       "      <td>-4.586000e-306</td>\n",
       "      <td>5.467000e-306</td>\n",
       "      <td>-4.740000e-306</td>\n",
       "      <td>5.649000e-306</td>\n",
       "      <td>-4.016000e-306</td>\n",
       "      <td>4.787000e-306</td>\n",
       "      <td>6.225000e-305</td>\n",
       "      <td>-7.420000e-305</td>\n",
       "      <td>4.427000e-306</td>\n",
       "      <td>-5.277000e-306</td>\n",
       "      <td>4.125000e-306</td>\n",
       "      <td>-4.917000e-306</td>\n",
       "      <td>1.620000e-306</td>\n",
       "      <td>-1.930000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>-2.683000e-305</td>\n",
       "      <td>3.198000e-305</td>\n",
       "      <td>-4.445000e-306</td>\n",
       "      <td>5.299000e-306</td>\n",
       "      <td>-4.759000e-306</td>\n",
       "      <td>5.672000e-306</td>\n",
       "      <td>-3.374000e-306</td>\n",
       "      <td>4.022000e-306</td>\n",
       "      <td>6.005000e-305</td>\n",
       "      <td>-7.157000e-305</td>\n",
       "      <td>4.751000e-306</td>\n",
       "      <td>-5.663000e-306</td>\n",
       "      <td>3.982000e-306</td>\n",
       "      <td>-4.747000e-306</td>\n",
       "      <td>1.701000e-306</td>\n",
       "      <td>-2.028000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>-3.186000e-305</td>\n",
       "      <td>3.797000e-305</td>\n",
       "      <td>-4.319000e-306</td>\n",
       "      <td>5.148000e-306</td>\n",
       "      <td>-4.940000e-306</td>\n",
       "      <td>5.888000e-306</td>\n",
       "      <td>-3.776000e-306</td>\n",
       "      <td>4.500000e-306</td>\n",
       "      <td>6.025000e-305</td>\n",
       "      <td>-7.182000e-305</td>\n",
       "      <td>4.495000e-306</td>\n",
       "      <td>-5.358000e-306</td>\n",
       "      <td>3.947000e-306</td>\n",
       "      <td>-4.705000e-306</td>\n",
       "      <td>1.912000e-306</td>\n",
       "      <td>-2.280000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>-3.066000e-305</td>\n",
       "      <td>3.654000e-305</td>\n",
       "      <td>-4.567000e-306</td>\n",
       "      <td>5.443000e-306</td>\n",
       "      <td>-4.923000e-306</td>\n",
       "      <td>5.868000e-306</td>\n",
       "      <td>-3.803000e-306</td>\n",
       "      <td>4.533000e-306</td>\n",
       "      <td>5.997000e-305</td>\n",
       "      <td>-7.148000e-305</td>\n",
       "      <td>4.569000e-306</td>\n",
       "      <td>-5.446000e-306</td>\n",
       "      <td>4.022000e-306</td>\n",
       "      <td>-4.794000e-306</td>\n",
       "      <td>1.917000e-306</td>\n",
       "      <td>-2.285000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>-2.799000e-305</td>\n",
       "      <td>3.337000e-305</td>\n",
       "      <td>-4.486000e-306</td>\n",
       "      <td>5.347000e-306</td>\n",
       "      <td>-4.563000e-306</td>\n",
       "      <td>5.439000e-306</td>\n",
       "      <td>-3.906000e-306</td>\n",
       "      <td>4.655000e-306</td>\n",
       "      <td>5.621000e-305</td>\n",
       "      <td>-6.700000e-305</td>\n",
       "      <td>4.132000e-306</td>\n",
       "      <td>-4.925000e-306</td>\n",
       "      <td>4.069000e-306</td>\n",
       "      <td>-4.851000e-306</td>\n",
       "      <td>2.159000e-306</td>\n",
       "      <td>-2.573000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>-2.965000e-305</td>\n",
       "      <td>3.535000e-305</td>\n",
       "      <td>-4.150000e-306</td>\n",
       "      <td>4.946000e-306</td>\n",
       "      <td>-4.621000e-306</td>\n",
       "      <td>5.508000e-306</td>\n",
       "      <td>-3.704000e-306</td>\n",
       "      <td>4.415000e-306</td>\n",
       "      <td>5.584000e-305</td>\n",
       "      <td>-6.656000e-305</td>\n",
       "      <td>4.415000e-306</td>\n",
       "      <td>-5.262000e-306</td>\n",
       "      <td>3.971000e-306</td>\n",
       "      <td>-4.734000e-306</td>\n",
       "      <td>2.002000e-306</td>\n",
       "      <td>-2.387000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>-3.161000e-305</td>\n",
       "      <td>3.768000e-305</td>\n",
       "      <td>-4.631000e-306</td>\n",
       "      <td>5.520000e-306</td>\n",
       "      <td>-3.914000e-306</td>\n",
       "      <td>4.666000e-306</td>\n",
       "      <td>-3.508000e-306</td>\n",
       "      <td>4.181000e-306</td>\n",
       "      <td>5.776000e-305</td>\n",
       "      <td>-6.885000e-305</td>\n",
       "      <td>4.713000e-306</td>\n",
       "      <td>-5.617000e-306</td>\n",
       "      <td>3.479000e-306</td>\n",
       "      <td>-4.147000e-306</td>\n",
       "      <td>1.687000e-306</td>\n",
       "      <td>-2.011000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>-3.083000e-305</td>\n",
       "      <td>3.675000e-305</td>\n",
       "      <td>-4.347000e-306</td>\n",
       "      <td>5.181000e-306</td>\n",
       "      <td>-4.322000e-306</td>\n",
       "      <td>5.151000e-306</td>\n",
       "      <td>-3.225000e-306</td>\n",
       "      <td>3.844000e-306</td>\n",
       "      <td>6.146000e-305</td>\n",
       "      <td>-7.325000e-305</td>\n",
       "      <td>4.402000e-306</td>\n",
       "      <td>-5.247000e-306</td>\n",
       "      <td>3.757000e-306</td>\n",
       "      <td>-4.479000e-306</td>\n",
       "      <td>1.783000e-306</td>\n",
       "      <td>-2.126000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>-3.171000e-305</td>\n",
       "      <td>3.780000e-305</td>\n",
       "      <td>-4.629000e-306</td>\n",
       "      <td>5.517000e-306</td>\n",
       "      <td>-4.569000e-306</td>\n",
       "      <td>5.446000e-306</td>\n",
       "      <td>-3.133000e-306</td>\n",
       "      <td>3.735000e-306</td>\n",
       "      <td>6.133000e-305</td>\n",
       "      <td>-7.310000e-305</td>\n",
       "      <td>4.457000e-306</td>\n",
       "      <td>-5.313000e-306</td>\n",
       "      <td>3.664000e-306</td>\n",
       "      <td>-4.367000e-306</td>\n",
       "      <td>1.752000e-306</td>\n",
       "      <td>-2.088000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>-3.095000e-305</td>\n",
       "      <td>3.689000e-305</td>\n",
       "      <td>-4.811000e-306</td>\n",
       "      <td>5.734000e-306</td>\n",
       "      <td>-4.901000e-306</td>\n",
       "      <td>5.841000e-306</td>\n",
       "      <td>-3.494000e-306</td>\n",
       "      <td>4.164000e-306</td>\n",
       "      <td>6.355000e-305</td>\n",
       "      <td>-7.575000e-305</td>\n",
       "      <td>4.698000e-306</td>\n",
       "      <td>-5.600000e-306</td>\n",
       "      <td>3.558000e-306</td>\n",
       "      <td>-4.241000e-306</td>\n",
       "      <td>1.763000e-306</td>\n",
       "      <td>-2.102000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>-2.961000e-305</td>\n",
       "      <td>3.529000e-305</td>\n",
       "      <td>-4.543000e-306</td>\n",
       "      <td>5.415000e-306</td>\n",
       "      <td>-4.773000e-306</td>\n",
       "      <td>5.689000e-306</td>\n",
       "      <td>-3.524000e-306</td>\n",
       "      <td>4.200000e-306</td>\n",
       "      <td>6.070000e-305</td>\n",
       "      <td>-7.236000e-305</td>\n",
       "      <td>4.727000e-306</td>\n",
       "      <td>-5.635000e-306</td>\n",
       "      <td>3.760000e-306</td>\n",
       "      <td>-4.482000e-306</td>\n",
       "      <td>1.755000e-306</td>\n",
       "      <td>-2.092000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>-3.063000e-305</td>\n",
       "      <td>3.651000e-305</td>\n",
       "      <td>-5.038000e-306</td>\n",
       "      <td>6.005000e-306</td>\n",
       "      <td>-4.805000e-306</td>\n",
       "      <td>5.728000e-306</td>\n",
       "      <td>-3.722000e-306</td>\n",
       "      <td>4.436000e-306</td>\n",
       "      <td>6.279000e-305</td>\n",
       "      <td>-7.485000e-305</td>\n",
       "      <td>4.975000e-306</td>\n",
       "      <td>-5.930000e-306</td>\n",
       "      <td>4.260000e-306</td>\n",
       "      <td>-5.077000e-306</td>\n",
       "      <td>1.704000e-306</td>\n",
       "      <td>-2.031000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>-2.776000e-305</td>\n",
       "      <td>3.309000e-305</td>\n",
       "      <td>-4.270000e-306</td>\n",
       "      <td>5.090000e-306</td>\n",
       "      <td>-5.076000e-306</td>\n",
       "      <td>6.051000e-306</td>\n",
       "      <td>-3.976000e-306</td>\n",
       "      <td>4.739000e-306</td>\n",
       "      <td>6.068000e-305</td>\n",
       "      <td>-7.233000e-305</td>\n",
       "      <td>4.781000e-306</td>\n",
       "      <td>-5.699000e-306</td>\n",
       "      <td>4.027000e-306</td>\n",
       "      <td>-4.800000e-306</td>\n",
       "      <td>1.643000e-306</td>\n",
       "      <td>-1.959000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>-2.650000e-305</td>\n",
       "      <td>3.158000e-305</td>\n",
       "      <td>-4.355000e-306</td>\n",
       "      <td>5.191000e-306</td>\n",
       "      <td>-4.887000e-306</td>\n",
       "      <td>5.825000e-306</td>\n",
       "      <td>-3.999000e-306</td>\n",
       "      <td>4.767000e-306</td>\n",
       "      <td>6.175000e-305</td>\n",
       "      <td>-7.361000e-305</td>\n",
       "      <td>4.969000e-306</td>\n",
       "      <td>-5.923000e-306</td>\n",
       "      <td>4.108000e-306</td>\n",
       "      <td>-4.897000e-306</td>\n",
       "      <td>1.913000e-306</td>\n",
       "      <td>-2.280000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>-3.403000e-305</td>\n",
       "      <td>4.057000e-305</td>\n",
       "      <td>-4.584000e-306</td>\n",
       "      <td>5.463000e-306</td>\n",
       "      <td>-4.400000e-306</td>\n",
       "      <td>5.245000e-306</td>\n",
       "      <td>-4.059000e-306</td>\n",
       "      <td>4.839000e-306</td>\n",
       "      <td>5.777000e-305</td>\n",
       "      <td>-6.886000e-305</td>\n",
       "      <td>4.764000e-306</td>\n",
       "      <td>-5.679000e-306</td>\n",
       "      <td>3.889000e-306</td>\n",
       "      <td>-4.636000e-306</td>\n",
       "      <td>1.905000e-306</td>\n",
       "      <td>-2.271000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>-2.971000e-305</td>\n",
       "      <td>3.541000e-305</td>\n",
       "      <td>-4.676000e-306</td>\n",
       "      <td>5.574000e-306</td>\n",
       "      <td>-5.092000e-306</td>\n",
       "      <td>6.069000e-306</td>\n",
       "      <td>-3.769000e-306</td>\n",
       "      <td>4.493000e-306</td>\n",
       "      <td>5.772000e-305</td>\n",
       "      <td>-6.880000e-305</td>\n",
       "      <td>4.684000e-306</td>\n",
       "      <td>-5.583000e-306</td>\n",
       "      <td>4.214000e-306</td>\n",
       "      <td>-5.024000e-306</td>\n",
       "      <td>1.664000e-306</td>\n",
       "      <td>-1.983000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>-2.854000e-305</td>\n",
       "      <td>3.402000e-305</td>\n",
       "      <td>-3.925000e-306</td>\n",
       "      <td>4.678000e-306</td>\n",
       "      <td>-4.863000e-306</td>\n",
       "      <td>5.797000e-306</td>\n",
       "      <td>-3.810000e-306</td>\n",
       "      <td>4.541000e-306</td>\n",
       "      <td>5.842000e-305</td>\n",
       "      <td>-6.964000e-305</td>\n",
       "      <td>4.682000e-306</td>\n",
       "      <td>-5.581000e-306</td>\n",
       "      <td>4.249000e-306</td>\n",
       "      <td>-5.064000e-306</td>\n",
       "      <td>2.110000e-306</td>\n",
       "      <td>-2.515000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>-3.204000e-305</td>\n",
       "      <td>3.819000e-305</td>\n",
       "      <td>-4.274000e-306</td>\n",
       "      <td>5.095000e-306</td>\n",
       "      <td>-4.603000e-306</td>\n",
       "      <td>5.486000e-306</td>\n",
       "      <td>-4.054000e-306</td>\n",
       "      <td>4.832000e-306</td>\n",
       "      <td>5.936000e-305</td>\n",
       "      <td>-7.076000e-305</td>\n",
       "      <td>4.407000e-306</td>\n",
       "      <td>-5.253000e-306</td>\n",
       "      <td>4.306000e-306</td>\n",
       "      <td>-5.133000e-306</td>\n",
       "      <td>1.855000e-306</td>\n",
       "      <td>-2.212000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>-3.013000e-305</td>\n",
       "      <td>3.591000e-305</td>\n",
       "      <td>-4.289000e-306</td>\n",
       "      <td>5.112000e-306</td>\n",
       "      <td>-4.674000e-306</td>\n",
       "      <td>5.571000e-306</td>\n",
       "      <td>-3.823000e-306</td>\n",
       "      <td>4.557000e-306</td>\n",
       "      <td>5.945000e-305</td>\n",
       "      <td>-7.087000e-305</td>\n",
       "      <td>4.532000e-306</td>\n",
       "      <td>-5.402000e-306</td>\n",
       "      <td>4.317000e-306</td>\n",
       "      <td>-5.146000e-306</td>\n",
       "      <td>2.134000e-306</td>\n",
       "      <td>-2.544000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>-3.249000e-305</td>\n",
       "      <td>3.873000e-305</td>\n",
       "      <td>-4.464000e-306</td>\n",
       "      <td>5.321000e-306</td>\n",
       "      <td>-4.338000e-306</td>\n",
       "      <td>5.171000e-306</td>\n",
       "      <td>-3.742000e-306</td>\n",
       "      <td>4.461000e-306</td>\n",
       "      <td>5.874000e-305</td>\n",
       "      <td>-7.002000e-305</td>\n",
       "      <td>4.850000e-306</td>\n",
       "      <td>-5.781000e-306</td>\n",
       "      <td>3.653000e-306</td>\n",
       "      <td>-4.354000e-306</td>\n",
       "      <td>2.031000e-306</td>\n",
       "      <td>-2.421000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>-2.804000e-305</td>\n",
       "      <td>3.342000e-305</td>\n",
       "      <td>-4.474000e-306</td>\n",
       "      <td>5.333000e-306</td>\n",
       "      <td>-4.854000e-306</td>\n",
       "      <td>5.786000e-306</td>\n",
       "      <td>-3.321000e-306</td>\n",
       "      <td>3.959000e-306</td>\n",
       "      <td>5.945000e-305</td>\n",
       "      <td>-7.086000e-305</td>\n",
       "      <td>4.585000e-306</td>\n",
       "      <td>-5.465000e-306</td>\n",
       "      <td>3.665000e-306</td>\n",
       "      <td>-4.369000e-306</td>\n",
       "      <td>2.034000e-306</td>\n",
       "      <td>-2.425000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>-3.084000e-305</td>\n",
       "      <td>3.676000e-305</td>\n",
       "      <td>-4.154000e-306</td>\n",
       "      <td>4.951000e-306</td>\n",
       "      <td>-4.320000e-306</td>\n",
       "      <td>5.150000e-306</td>\n",
       "      <td>-3.665000e-306</td>\n",
       "      <td>4.369000e-306</td>\n",
       "      <td>6.276000e-305</td>\n",
       "      <td>-7.481000e-305</td>\n",
       "      <td>4.600000e-306</td>\n",
       "      <td>-5.484000e-306</td>\n",
       "      <td>3.230000e-306</td>\n",
       "      <td>-3.850000e-306</td>\n",
       "      <td>1.751000e-306</td>\n",
       "      <td>-2.087000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>-2.871000e-305</td>\n",
       "      <td>3.422000e-305</td>\n",
       "      <td>-5.076000e-306</td>\n",
       "      <td>6.051000e-306</td>\n",
       "      <td>-4.500000e-306</td>\n",
       "      <td>5.364000e-306</td>\n",
       "      <td>-3.675000e-306</td>\n",
       "      <td>4.380000e-306</td>\n",
       "      <td>6.199000e-305</td>\n",
       "      <td>-7.389000e-305</td>\n",
       "      <td>4.596000e-306</td>\n",
       "      <td>-5.478000e-306</td>\n",
       "      <td>3.954000e-306</td>\n",
       "      <td>-4.714000e-306</td>\n",
       "      <td>1.734000e-306</td>\n",
       "      <td>-2.067000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>-3.258000e-305</td>\n",
       "      <td>3.883000e-305</td>\n",
       "      <td>-4.731000e-306</td>\n",
       "      <td>5.639000e-306</td>\n",
       "      <td>-5.029000e-306</td>\n",
       "      <td>5.995000e-306</td>\n",
       "      <td>-3.550000e-306</td>\n",
       "      <td>4.231000e-306</td>\n",
       "      <td>5.910000e-305</td>\n",
       "      <td>-7.045000e-305</td>\n",
       "      <td>4.979000e-306</td>\n",
       "      <td>-5.935000e-306</td>\n",
       "      <td>4.045000e-306</td>\n",
       "      <td>-4.821000e-306</td>\n",
       "      <td>1.846000e-306</td>\n",
       "      <td>-2.201000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>-2.945000e-305</td>\n",
       "      <td>3.510000e-305</td>\n",
       "      <td>-4.856000e-306</td>\n",
       "      <td>5.789000e-306</td>\n",
       "      <td>-5.003000e-306</td>\n",
       "      <td>5.963000e-306</td>\n",
       "      <td>-3.719000e-306</td>\n",
       "      <td>4.433000e-306</td>\n",
       "      <td>6.263000e-305</td>\n",
       "      <td>-7.465000e-305</td>\n",
       "      <td>5.479000e-306</td>\n",
       "      <td>-6.530000e-306</td>\n",
       "      <td>4.131000e-306</td>\n",
       "      <td>-4.924000e-306</td>\n",
       "      <td>1.883000e-306</td>\n",
       "      <td>-2.245000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>-2.937000e-305</td>\n",
       "      <td>3.500000e-305</td>\n",
       "      <td>-4.626000e-306</td>\n",
       "      <td>5.514000e-306</td>\n",
       "      <td>-4.848000e-306</td>\n",
       "      <td>5.779000e-306</td>\n",
       "      <td>-4.040000e-306</td>\n",
       "      <td>4.816000e-306</td>\n",
       "      <td>6.043000e-305</td>\n",
       "      <td>-7.203000e-305</td>\n",
       "      <td>5.117000e-306</td>\n",
       "      <td>-6.100000e-306</td>\n",
       "      <td>4.223000e-306</td>\n",
       "      <td>-5.034000e-306</td>\n",
       "      <td>1.893000e-306</td>\n",
       "      <td>-2.257000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>-2.886000e-305</td>\n",
       "      <td>3.440000e-305</td>\n",
       "      <td>-4.518000e-306</td>\n",
       "      <td>5.385000e-306</td>\n",
       "      <td>-5.050000e-306</td>\n",
       "      <td>6.020000e-306</td>\n",
       "      <td>-4.113000e-306</td>\n",
       "      <td>4.903000e-306</td>\n",
       "      <td>6.082000e-305</td>\n",
       "      <td>-7.249000e-305</td>\n",
       "      <td>5.299000e-306</td>\n",
       "      <td>-6.316000e-306</td>\n",
       "      <td>3.941000e-306</td>\n",
       "      <td>-4.698000e-306</td>\n",
       "      <td>1.943000e-306</td>\n",
       "      <td>-2.316000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>-2.917000e-305</td>\n",
       "      <td>3.477000e-305</td>\n",
       "      <td>-3.800000e-306</td>\n",
       "      <td>4.530000e-306</td>\n",
       "      <td>-4.590000e-306</td>\n",
       "      <td>5.471000e-306</td>\n",
       "      <td>-3.858000e-306</td>\n",
       "      <td>4.599000e-306</td>\n",
       "      <td>5.936000e-305</td>\n",
       "      <td>-7.075000e-305</td>\n",
       "      <td>4.810000e-306</td>\n",
       "      <td>-5.734000e-306</td>\n",
       "      <td>4.568000e-306</td>\n",
       "      <td>-5.445000e-306</td>\n",
       "      <td>1.869000e-306</td>\n",
       "      <td>-2.228000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Timestamp           Oxy1         DeOxy1           Oxy2         DeOxy2  \\\n",
       "0           1 -2.590000e-305  3.087000e-305 -4.040000e-306  4.816000e-306   \n",
       "1           2 -2.315000e-305  2.759000e-305 -3.601000e-306  4.292000e-306   \n",
       "2           3 -2.571000e-305  3.065000e-305 -3.766000e-306  4.489000e-306   \n",
       "3           4 -3.335000e-305  3.975000e-305 -4.188000e-306  4.992000e-306   \n",
       "4           5 -2.897000e-305  3.453000e-305 -4.086000e-306  4.870000e-306   \n",
       "5           6 -3.190000e-305  3.802000e-305 -4.075000e-306  4.857000e-306   \n",
       "6           7 -2.973000e-305  3.544000e-305 -3.587000e-306  4.275000e-306   \n",
       "7           8 -2.889000e-305  3.443000e-305 -4.629000e-306  5.518000e-306   \n",
       "8           9 -2.816000e-305  3.357000e-305 -3.981000e-306  4.745000e-306   \n",
       "9          10 -3.034000e-305  3.616000e-305 -3.769000e-306  4.493000e-306   \n",
       "10         11 -2.485000e-305  2.962000e-305 -4.116000e-306  4.906000e-306   \n",
       "11         12 -2.536000e-305  3.023000e-305 -4.095000e-306  4.881000e-306   \n",
       "12         13 -2.029000e-305  2.419000e-305 -3.427000e-306  4.085000e-306   \n",
       "13         14 -2.618000e-305  3.121000e-305 -3.072000e-306  3.662000e-306   \n",
       "14         15 -2.634000e-305  3.140000e-305 -3.259000e-306  3.885000e-306   \n",
       "15         16 -2.816000e-305  3.357000e-305 -2.989000e-306  3.563000e-306   \n",
       "16         17 -2.703000e-305  3.222000e-305 -3.461000e-306  4.126000e-306   \n",
       "17         18 -2.982000e-305  3.554000e-305 -3.309000e-306  3.944000e-306   \n",
       "18         19 -2.726000e-305  3.250000e-305 -3.910000e-306  4.661000e-306   \n",
       "19         20 -3.112000e-305  3.709000e-305 -4.601000e-306  5.485000e-306   \n",
       "20         21 -2.860000e-305  3.409000e-305 -4.241000e-306  5.056000e-306   \n",
       "21         22 -2.848000e-305  3.395000e-305 -4.471000e-306  5.329000e-306   \n",
       "22         23 -2.363000e-305  2.816000e-305 -4.586000e-306  5.467000e-306   \n",
       "23         24 -2.683000e-305  3.198000e-305 -4.445000e-306  5.299000e-306   \n",
       "24         25 -3.186000e-305  3.797000e-305 -4.319000e-306  5.148000e-306   \n",
       "25         26 -3.066000e-305  3.654000e-305 -4.567000e-306  5.443000e-306   \n",
       "26         27 -2.799000e-305  3.337000e-305 -4.486000e-306  5.347000e-306   \n",
       "27         28 -2.965000e-305  3.535000e-305 -4.150000e-306  4.946000e-306   \n",
       "28         29 -3.161000e-305  3.768000e-305 -4.631000e-306  5.520000e-306   \n",
       "29         30 -3.083000e-305  3.675000e-305 -4.347000e-306  5.181000e-306   \n",
       "30         31 -3.171000e-305  3.780000e-305 -4.629000e-306  5.517000e-306   \n",
       "31         32 -3.095000e-305  3.689000e-305 -4.811000e-306  5.734000e-306   \n",
       "32         33 -2.961000e-305  3.529000e-305 -4.543000e-306  5.415000e-306   \n",
       "33         34 -3.063000e-305  3.651000e-305 -5.038000e-306  6.005000e-306   \n",
       "34         35 -2.776000e-305  3.309000e-305 -4.270000e-306  5.090000e-306   \n",
       "35         36 -2.650000e-305  3.158000e-305 -4.355000e-306  5.191000e-306   \n",
       "36         37 -3.403000e-305  4.057000e-305 -4.584000e-306  5.463000e-306   \n",
       "37         38 -2.971000e-305  3.541000e-305 -4.676000e-306  5.574000e-306   \n",
       "38         39 -2.854000e-305  3.402000e-305 -3.925000e-306  4.678000e-306   \n",
       "39         40 -3.204000e-305  3.819000e-305 -4.274000e-306  5.095000e-306   \n",
       "40         41 -3.013000e-305  3.591000e-305 -4.289000e-306  5.112000e-306   \n",
       "41         42 -3.249000e-305  3.873000e-305 -4.464000e-306  5.321000e-306   \n",
       "42         43 -2.804000e-305  3.342000e-305 -4.474000e-306  5.333000e-306   \n",
       "43         44 -3.084000e-305  3.676000e-305 -4.154000e-306  4.951000e-306   \n",
       "44         45 -2.871000e-305  3.422000e-305 -5.076000e-306  6.051000e-306   \n",
       "45         46 -3.258000e-305  3.883000e-305 -4.731000e-306  5.639000e-306   \n",
       "46         47 -2.945000e-305  3.510000e-305 -4.856000e-306  5.789000e-306   \n",
       "47         48 -2.937000e-305  3.500000e-305 -4.626000e-306  5.514000e-306   \n",
       "48         49 -2.886000e-305  3.440000e-305 -4.518000e-306  5.385000e-306   \n",
       "49         50 -2.917000e-305  3.477000e-305 -3.800000e-306  4.530000e-306   \n",
       "\n",
       "             Oxy3         DeOxy3           Oxy4         DeOxy4           Oxy5  \\\n",
       "0  -4.747000e-306  5.658000e-306 -3.440000e-306  4.100000e-306  5.880000e-305   \n",
       "1  -4.902000e-306  5.843000e-306 -3.444000e-306  4.106000e-306  6.224000e-305   \n",
       "2  -4.511000e-306  5.376000e-306 -3.847000e-306  4.586000e-306  5.882000e-305   \n",
       "3  -4.373000e-306  5.213000e-306 -3.726000e-306  4.442000e-306  5.547000e-305   \n",
       "4  -3.768000e-306  4.491000e-306 -3.724000e-306  4.439000e-306  5.415000e-305   \n",
       "5  -4.281000e-306  5.103000e-306 -3.555000e-306  4.237000e-306  5.650000e-305   \n",
       "6  -4.187000e-306  4.991000e-306 -3.385000e-306  4.035000e-306  5.781000e-305   \n",
       "7  -4.397000e-306  5.241000e-306 -3.511000e-306  4.185000e-306  5.751000e-305   \n",
       "8  -4.038000e-306  4.814000e-306 -3.389000e-306  4.039000e-306  6.264000e-305   \n",
       "9  -4.417000e-306  5.264000e-306 -3.315000e-306  3.952000e-306  6.293000e-305   \n",
       "10 -4.311000e-306  5.138000e-306 -3.056000e-306  3.643000e-306  5.568000e-305   \n",
       "11 -4.130000e-306  4.923000e-306 -3.339000e-306  3.980000e-306  5.815000e-305   \n",
       "12 -3.911000e-306  4.662000e-306 -3.039000e-306  3.622000e-306  5.078000e-305   \n",
       "13 -3.469000e-306  4.136000e-306 -3.090000e-306  3.683000e-306  4.812000e-305   \n",
       "14 -3.590000e-306  4.279000e-306 -2.856000e-306  3.404000e-306  4.955000e-305   \n",
       "15 -3.591000e-306  4.280000e-306 -2.647000e-306  3.155000e-306  4.466000e-305   \n",
       "16 -3.694000e-306  4.403000e-306 -2.949000e-306  3.515000e-306  5.087000e-305   \n",
       "17 -3.950000e-306  4.709000e-306 -3.024000e-306  3.605000e-306  5.476000e-305   \n",
       "18 -3.993000e-306  4.759000e-306 -3.171000e-306  3.780000e-306  5.569000e-305   \n",
       "19 -4.158000e-306  4.956000e-306 -3.275000e-306  3.904000e-306  5.714000e-305   \n",
       "20 -4.624000e-306  5.511000e-306 -3.714000e-306  4.427000e-306  5.988000e-305   \n",
       "21 -4.251000e-306  5.067000e-306 -3.472000e-306  4.139000e-306  6.074000e-305   \n",
       "22 -4.740000e-306  5.649000e-306 -4.016000e-306  4.787000e-306  6.225000e-305   \n",
       "23 -4.759000e-306  5.672000e-306 -3.374000e-306  4.022000e-306  6.005000e-305   \n",
       "24 -4.940000e-306  5.888000e-306 -3.776000e-306  4.500000e-306  6.025000e-305   \n",
       "25 -4.923000e-306  5.868000e-306 -3.803000e-306  4.533000e-306  5.997000e-305   \n",
       "26 -4.563000e-306  5.439000e-306 -3.906000e-306  4.655000e-306  5.621000e-305   \n",
       "27 -4.621000e-306  5.508000e-306 -3.704000e-306  4.415000e-306  5.584000e-305   \n",
       "28 -3.914000e-306  4.666000e-306 -3.508000e-306  4.181000e-306  5.776000e-305   \n",
       "29 -4.322000e-306  5.151000e-306 -3.225000e-306  3.844000e-306  6.146000e-305   \n",
       "30 -4.569000e-306  5.446000e-306 -3.133000e-306  3.735000e-306  6.133000e-305   \n",
       "31 -4.901000e-306  5.841000e-306 -3.494000e-306  4.164000e-306  6.355000e-305   \n",
       "32 -4.773000e-306  5.689000e-306 -3.524000e-306  4.200000e-306  6.070000e-305   \n",
       "33 -4.805000e-306  5.728000e-306 -3.722000e-306  4.436000e-306  6.279000e-305   \n",
       "34 -5.076000e-306  6.051000e-306 -3.976000e-306  4.739000e-306  6.068000e-305   \n",
       "35 -4.887000e-306  5.825000e-306 -3.999000e-306  4.767000e-306  6.175000e-305   \n",
       "36 -4.400000e-306  5.245000e-306 -4.059000e-306  4.839000e-306  5.777000e-305   \n",
       "37 -5.092000e-306  6.069000e-306 -3.769000e-306  4.493000e-306  5.772000e-305   \n",
       "38 -4.863000e-306  5.797000e-306 -3.810000e-306  4.541000e-306  5.842000e-305   \n",
       "39 -4.603000e-306  5.486000e-306 -4.054000e-306  4.832000e-306  5.936000e-305   \n",
       "40 -4.674000e-306  5.571000e-306 -3.823000e-306  4.557000e-306  5.945000e-305   \n",
       "41 -4.338000e-306  5.171000e-306 -3.742000e-306  4.461000e-306  5.874000e-305   \n",
       "42 -4.854000e-306  5.786000e-306 -3.321000e-306  3.959000e-306  5.945000e-305   \n",
       "43 -4.320000e-306  5.150000e-306 -3.665000e-306  4.369000e-306  6.276000e-305   \n",
       "44 -4.500000e-306  5.364000e-306 -3.675000e-306  4.380000e-306  6.199000e-305   \n",
       "45 -5.029000e-306  5.995000e-306 -3.550000e-306  4.231000e-306  5.910000e-305   \n",
       "46 -5.003000e-306  5.963000e-306 -3.719000e-306  4.433000e-306  6.263000e-305   \n",
       "47 -4.848000e-306  5.779000e-306 -4.040000e-306  4.816000e-306  6.043000e-305   \n",
       "48 -5.050000e-306  6.020000e-306 -4.113000e-306  4.903000e-306  6.082000e-305   \n",
       "49 -4.590000e-306  5.471000e-306 -3.858000e-306  4.599000e-306  5.936000e-305   \n",
       "\n",
       "           DeOxy5           Oxy6         DeOxy6           Oxy7         DeOxy7  \\\n",
       "0  -7.009000e-305  4.894000e-306 -5.833000e-306  3.826000e-306 -4.561000e-306   \n",
       "1  -7.418000e-305  4.948000e-306 -5.898000e-306  3.715000e-306 -4.428000e-306   \n",
       "2  -7.011000e-305  4.274000e-306 -5.095000e-306  4.150000e-306 -4.947000e-306   \n",
       "3  -6.612000e-305  4.184000e-306 -4.987000e-306  3.722000e-306 -4.436000e-306   \n",
       "4  -6.454000e-305  3.878000e-306 -4.622000e-306  3.749000e-306 -4.469000e-306   \n",
       "5  -6.734000e-305  4.038000e-306 -4.813000e-306  3.536000e-306 -4.214000e-306   \n",
       "6  -6.891000e-305  4.578000e-306 -5.457000e-306  3.904000e-306 -4.653000e-306   \n",
       "7  -6.855000e-305  4.372000e-306 -5.211000e-306  3.611000e-306 -4.305000e-306   \n",
       "8  -7.466000e-305  4.382000e-306 -5.224000e-306  3.372000e-306 -4.019000e-306   \n",
       "9  -7.501000e-305  4.328000e-306 -5.159000e-306  3.911000e-306 -4.662000e-306   \n",
       "10 -6.636000e-305  4.165000e-306 -4.965000e-306  3.611000e-306 -4.304000e-306   \n",
       "11 -6.931000e-305  5.019000e-306 -5.982000e-306  3.215000e-306 -3.832000e-306   \n",
       "12 -6.053000e-305  4.064000e-306 -4.844000e-306  3.347000e-306 -3.990000e-306   \n",
       "13 -5.735000e-305  3.648000e-306 -4.349000e-306  3.161000e-306 -3.768000e-306   \n",
       "14 -5.906000e-305  3.360000e-306 -4.005000e-306  2.755000e-306 -3.283000e-306   \n",
       "15 -5.323000e-305  3.574000e-306 -4.261000e-306  3.172000e-306 -3.781000e-306   \n",
       "16 -6.064000e-305  3.492000e-306 -4.162000e-306  2.795000e-306 -3.331000e-306   \n",
       "17 -6.527000e-305  3.473000e-306 -4.140000e-306  3.110000e-306 -3.707000e-306   \n",
       "18 -6.638000e-305  3.989000e-306 -4.755000e-306  3.163000e-306 -3.770000e-306   \n",
       "19 -6.811000e-305  4.346000e-306 -5.180000e-306  3.027000e-306 -3.609000e-306   \n",
       "20 -7.138000e-305  4.666000e-306 -5.561000e-306  3.811000e-306 -4.542000e-306   \n",
       "21 -7.240000e-305  4.696000e-306 -5.598000e-306  3.842000e-306 -4.579000e-306   \n",
       "22 -7.420000e-305  4.427000e-306 -5.277000e-306  4.125000e-306 -4.917000e-306   \n",
       "23 -7.157000e-305  4.751000e-306 -5.663000e-306  3.982000e-306 -4.747000e-306   \n",
       "24 -7.182000e-305  4.495000e-306 -5.358000e-306  3.947000e-306 -4.705000e-306   \n",
       "25 -7.148000e-305  4.569000e-306 -5.446000e-306  4.022000e-306 -4.794000e-306   \n",
       "26 -6.700000e-305  4.132000e-306 -4.925000e-306  4.069000e-306 -4.851000e-306   \n",
       "27 -6.656000e-305  4.415000e-306 -5.262000e-306  3.971000e-306 -4.734000e-306   \n",
       "28 -6.885000e-305  4.713000e-306 -5.617000e-306  3.479000e-306 -4.147000e-306   \n",
       "29 -7.325000e-305  4.402000e-306 -5.247000e-306  3.757000e-306 -4.479000e-306   \n",
       "30 -7.310000e-305  4.457000e-306 -5.313000e-306  3.664000e-306 -4.367000e-306   \n",
       "31 -7.575000e-305  4.698000e-306 -5.600000e-306  3.558000e-306 -4.241000e-306   \n",
       "32 -7.236000e-305  4.727000e-306 -5.635000e-306  3.760000e-306 -4.482000e-306   \n",
       "33 -7.485000e-305  4.975000e-306 -5.930000e-306  4.260000e-306 -5.077000e-306   \n",
       "34 -7.233000e-305  4.781000e-306 -5.699000e-306  4.027000e-306 -4.800000e-306   \n",
       "35 -7.361000e-305  4.969000e-306 -5.923000e-306  4.108000e-306 -4.897000e-306   \n",
       "36 -6.886000e-305  4.764000e-306 -5.679000e-306  3.889000e-306 -4.636000e-306   \n",
       "37 -6.880000e-305  4.684000e-306 -5.583000e-306  4.214000e-306 -5.024000e-306   \n",
       "38 -6.964000e-305  4.682000e-306 -5.581000e-306  4.249000e-306 -5.064000e-306   \n",
       "39 -7.076000e-305  4.407000e-306 -5.253000e-306  4.306000e-306 -5.133000e-306   \n",
       "40 -7.087000e-305  4.532000e-306 -5.402000e-306  4.317000e-306 -5.146000e-306   \n",
       "41 -7.002000e-305  4.850000e-306 -5.781000e-306  3.653000e-306 -4.354000e-306   \n",
       "42 -7.086000e-305  4.585000e-306 -5.465000e-306  3.665000e-306 -4.369000e-306   \n",
       "43 -7.481000e-305  4.600000e-306 -5.484000e-306  3.230000e-306 -3.850000e-306   \n",
       "44 -7.389000e-305  4.596000e-306 -5.478000e-306  3.954000e-306 -4.714000e-306   \n",
       "45 -7.045000e-305  4.979000e-306 -5.935000e-306  4.045000e-306 -4.821000e-306   \n",
       "46 -7.465000e-305  5.479000e-306 -6.530000e-306  4.131000e-306 -4.924000e-306   \n",
       "47 -7.203000e-305  5.117000e-306 -6.100000e-306  4.223000e-306 -5.034000e-306   \n",
       "48 -7.249000e-305  5.299000e-306 -6.316000e-306  3.941000e-306 -4.698000e-306   \n",
       "49 -7.075000e-305  4.810000e-306 -5.734000e-306  4.568000e-306 -5.445000e-306   \n",
       "\n",
       "             Oxy8         DeOxy8  State  \n",
       "0   1.449000e-306 -1.727000e-306    1.0  \n",
       "1   1.947000e-306 -2.321000e-306    1.0  \n",
       "2   1.831000e-306 -2.182000e-306    1.0  \n",
       "3   1.948000e-306 -2.322000e-306    1.0  \n",
       "4   1.614000e-306 -1.923000e-306    1.0  \n",
       "5   1.665000e-306 -1.985000e-306    1.0  \n",
       "6   1.846000e-306 -2.201000e-306    1.0  \n",
       "7   1.959000e-306 -2.336000e-306    1.0  \n",
       "8   1.476000e-306 -1.760000e-306    1.0  \n",
       "9   1.501000e-306 -1.790000e-306    1.0  \n",
       "10  1.627000e-306 -1.940000e-306    1.0  \n",
       "11  1.698000e-306 -2.023000e-306    1.0  \n",
       "12  1.565000e-306 -1.866000e-306    1.0  \n",
       "13  1.328000e-306 -1.583000e-306    1.0  \n",
       "14  1.411000e-306 -1.682000e-306    1.0  \n",
       "15  1.334000e-306 -1.590000e-306    1.0  \n",
       "16  1.578000e-306 -1.881000e-306    1.0  \n",
       "17  1.446000e-306 -1.724000e-306    1.0  \n",
       "18  1.583000e-306 -1.887000e-306    1.0  \n",
       "19  1.652000e-306 -1.969000e-306    1.0  \n",
       "20  1.465000e-306 -1.747000e-306    1.0  \n",
       "21  1.466000e-306 -1.748000e-306    1.0  \n",
       "22  1.620000e-306 -1.930000e-306    1.0  \n",
       "23  1.701000e-306 -2.028000e-306    1.0  \n",
       "24  1.912000e-306 -2.280000e-306    1.0  \n",
       "25  1.917000e-306 -2.285000e-306    1.0  \n",
       "26  2.159000e-306 -2.573000e-306    1.0  \n",
       "27  2.002000e-306 -2.387000e-306    1.0  \n",
       "28  1.687000e-306 -2.011000e-306    1.0  \n",
       "29  1.783000e-306 -2.126000e-306    1.0  \n",
       "30  1.752000e-306 -2.088000e-306    1.0  \n",
       "31  1.763000e-306 -2.102000e-306    1.0  \n",
       "32  1.755000e-306 -2.092000e-306    1.0  \n",
       "33  1.704000e-306 -2.031000e-306    1.0  \n",
       "34  1.643000e-306 -1.959000e-306    1.0  \n",
       "35  1.913000e-306 -2.280000e-306    1.0  \n",
       "36  1.905000e-306 -2.271000e-306    1.0  \n",
       "37  1.664000e-306 -1.983000e-306    1.0  \n",
       "38  2.110000e-306 -2.515000e-306    1.0  \n",
       "39  1.855000e-306 -2.212000e-306    1.0  \n",
       "40  2.134000e-306 -2.544000e-306    1.0  \n",
       "41  2.031000e-306 -2.421000e-306    1.0  \n",
       "42  2.034000e-306 -2.425000e-306    1.0  \n",
       "43  1.751000e-306 -2.087000e-306    1.0  \n",
       "44  1.734000e-306 -2.067000e-306    1.0  \n",
       "45  1.846000e-306 -2.201000e-306    1.0  \n",
       "46  1.883000e-306 -2.245000e-306    1.0  \n",
       "47  1.893000e-306 -2.257000e-306    1.0  \n",
       "48  1.943000e-306 -2.316000e-306    1.0  \n",
       "49  1.869000e-306 -2.228000e-306    1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas is a library for storing csv files in a dataframe, a data format that makes it easy to work with csv files\n",
    "import pandas as pd\n",
    "\n",
    "# read in the csv file as the variable dataset\n",
    "dataset = pd.read_csv(\"../datasets/taskdefault.csv\")\n",
    "# add columns to the dataset\n",
    "dataset.columns = ['Timestamp','Oxy1','DeOxy1','Oxy2','DeOxy2','Oxy3','DeOxy3', 'Oxy4','DeOxy4','Oxy5','DeOxy5','Oxy6','DeOxy6','Oxy7','DeOxy7','Oxy8','DeOxy8', 'State']\n",
    "\n",
    "# shows the first 50 rows in the dataset\n",
    "dataset.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shows the columns in the dataset\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making our neural network\n",
    "\n",
    "# Adding dependencies\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import keras\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# Remove missing values\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Shuffle the data\n",
    "dataset = dataset.sample(frac=1)\n",
    "\n",
    "# for use with two channels\n",
    "X = np.column_stack((dataset[\"Oxy1\"], dataset[\"Oxy2\"], dataset[\"Oxy3\"], dataset[\"Oxy4\"], dataset[\"Oxy5\"], dataset[\"Oxy6\"], dataset[\"Oxy7\"], dataset[\"Oxy8\"], dataset[\"DeOxy1\"], dataset[\"DeOxy2\"], dataset[\"DeOxy3\"], dataset[\"DeOxy4\"], dataset[\"DeOxy5\"], dataset[\"DeOxy6\"], dataset[\"DeOxy7\"], dataset[\"DeOxy8\"] ))\n",
    "Y = np.array([[1,0] if i == 0 else [0,1] for i in dataset.State])\n",
    "\n",
    "# Batch size - the number of data points added at each time, affects training time\n",
    "# Epochs - the number of training/test sessions\n",
    "# create model\n",
    "model = Sequential()\n",
    "# batchnormalization, makes the value fit between 0-1\n",
    "# Input layer with 16 inputs, because we X has 16 values\n",
    "model.add(BatchNormalization(input_shape=(16, )))\n",
    "# Dropout - the number of neurons removed at each layers, who are readded when testing\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "\n",
    "# output layer guesses default or task positive network\n",
    "model.add(Dense(2, init=\"normal\", activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "history = model.fit(X, Y, validation_split=0.5, nb_epoch=5, batch_size=5, verbose=1)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what just happened?\n",
    "We trained a neural network that took half of the input 1228 samples to train and half of the sample 1228 samples to guess what state my mind was in.\n",
    "\n",
    "It doesn't seem to be working so well. This an excellent chance for us to figure out why that is!\n",
    "\n",
    "## The unsexy part of machine learning\n",
    "The sexy part of machine learning might be building machine learning models, sadly that is a tiny part of your job as a data scientist. The big job is figuring out, why doesn't your model that should be working work?\n",
    "\n",
    "## What could be wrong?\n",
    "When we are working with AI it is important to learn how to troubleshoot. Everything that can go wrong will go wrong so we just have to learn to deal with that.\n",
    "\n",
    "The most common problems in machine learning.\n",
    "## There is something wrong with your data\n",
    "One of the most common problems in machine learning is that your data isn't any good.\n",
    "\n",
    "Remember? \n",
    "Garbage in = Garbage out\n",
    "\n",
    "Depending on your dataset there could be many things that could be wrong. So lets take a moment to think what could be wrong with our data?\n",
    "\n",
    "## Can you think of all the things that might be wrong with our data?\n",
    "\n",
    "There was too much light in the room that made the sensor have an overflow error\n",
    "The placement of the brain sensor was wrong so it couldn't detect activity in the brain properly\n",
    "There was something wrong with the sensor that detected the brain signal, so it couldn't detect it properly.\n",
    "There was something wrong with the program that gave us the data from the brain sensor that corrupted the data.\n",
    "A mistake was made while converting the data into a csv file.\n",
    "We wrote something wrong when we prepropessed the data (adding labels, removing missing values, shuffling the data)\n",
    "\n",
    "## Can you think of all the things that be wrong with our machine learning model?\n",
    "\n",
    "We didn't assing our X and Y variables properly.\n",
    "We are using a machine learning model that isn't suitable for handling our data format.\n",
    "Our activation function (currently RELU) might be inefficient.\n",
    "Our loss function (currently mean_squared_error) might be inefficient. \n",
    "We need to tweak the number of layers in our neural network (currently a 16 neuron input layer, 4 deep layers of 100 neurons each, with  and a 2 neuron output layer).\n",
    "We need to add our remove dropout to deal with overfitting (currently done twice with 20% of the network)\n",
    "We need to tweak our output parameters (validation_split=0.5, nb_epoch=5, batch_size=5), validation_split is how much of our data is training data and how much is test data, nb_epochs is how long the network will train and batch_size is how much data will be processed before updating weights. \n",
    "\n",
    "## Figure out if you have a problem with your data or your model\n",
    "\n",
    "The reason why it's so important to work with data preprocessing is because it's really good to know that it's actually your neural network that is the problem before you spend all your time trying to fix it.\n",
    "\n",
    "I'll save you the suspense for this dataset. Default mode network activation has never been able to be detected using FNIRS (the technology we are using for the dataset), so even with the worlds greatest neural network, some problems might still be impossible to solve. The reason as always has to do with the input data.\n",
    "\n",
    "Artificial neural network are (like our brain!) universal function approximators. This means that they can theoretically solve any mathematical problem. \n",
    "\n",
    "When you are working in deep learning, your neural network are usually fine once you get the working, but getting the right data will always remain a big problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need more data\n",
    "Okay, so that didn't work so well. Lets try instead with a larger dataset. \n",
    "This dataset uses data taken from the 52 broadmann areas of the brain for 51 users.\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Gray726-Brodman.png/480px-Gray726-Brodman.png \"Logo Title Text 1\")\n",
    "\n",
    "The goal is to try to detect activation in the default mode network or task positive networks on the brain with the help of FNIRS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## The default mode network\n",
    "The default mode network is a brain area that is active during rest.\n",
    "\n",
    "It is the neurological basis for the self\n",
    "\n",
    "Autobiographical information: Memories of collection of events and facts about one's self\n",
    "Self-reference: Referring to traits and descriptions of one's self\n",
    "Emotion of one's self: Reflecting about one's own emotional state\n",
    "Thinking about others:\n",
    "\n",
    "Theory of Mind: Thinking about the thoughts of others and what they might or might not know\n",
    "Emotions of other: Understanding the emotions of other people and empathizing with their feelings\n",
    "Moral reasoning: Determining just and unjust result of an action\n",
    "Social evaluations: Good-bad attitude judgments about social concepts\n",
    "Social categories: Reflecting on important social characteristics and status of a group\n",
    "Remembering the past and thinking about the future:\n",
    "\n",
    "Remembering the past: Recalling events that happened in the past\n",
    "Imagining the future: Envisioning events that might happen in the future\n",
    "Episodic memory: Detailed memory related to specific events in time\n",
    "Story comprehension: Understanding and remembering a narrative\n",
    "The default mode network is active during passive rest and mind-wandering. Mind-wandering usually involves thinking about others, thinking about one's self, remembering the past, and envisioning the future. Electrocorticography studies (which involve placing electrodes on the surface of epileptic patient's brains) have shown the default mode network becomes activated within an order of a fraction of a second after participants finish a task.\n",
    "![alt text](files/defaultmodenetwork2.jpg \"Logo Title Text 1\")\n",
    "## The task positive network\n",
    "The task positive network is active when you are doing a task and focusing your attention.\n",
    "The task-positive network (TPN) is a network of areas in the human brain that typically responds with activation increases to attention-demanding tasks in functional imaging studies. The task-positive network encompasses regions of the dorsal attention system, but in addition includes dorsolateral and ventrolateral prefrontal regions, the insular cortex, and the SMA/pre-SMA. Notably, the nodes of this network are also correlated during rest (i.e., in the absence of any task). The task-positive network is anti-correlated with the default mode network. \n",
    "\n",
    "During rest the TPN has been claimed to subserve intermittent \"external awareness\", defined as the conscious perception through different sensory modalities of one's surrounding environment[further explanation needed].\n",
    "![alt text](files/taskpositivenetwork.png \"Logo Title Text 1\")\n",
    "## Anticorrelations\n",
    "The task positive network and the default mode network are anti-correlated brain regions and you spend your time in either brain region.\n",
    "\n",
    "## Classifying brain region activation\n",
    "The goal of this assignment is to see if we can make a classifier that can understand what brain region is active. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward neural network\n",
    "<img src=\"http://ufldl.stanford.edu/tutorial/images/Network331.png\">\n",
    "## Calculated on an individual users file\n",
    "### Changing the value in pd.read_csv changes the user that the model trains on\n",
    "### This model uses a feedforward neural network on the data from the user\n",
    "### The data is randomized and 50% of the dataset is user for training and 50% for testing\n",
    "### X = input from the 52 broadmann areas at each timestep\n",
    "### Y = REST or ADDITION from the Mark column of the dataset\n",
    "### The model tries learns and tries to predict if each timeserie of data from the 52 broadmann areas is REST (Default mode network) or Addition (Task positive network)\n",
    "\n",
    "<img src=\"https://i.imgur.com/Thqsh33.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-42db93a02080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.floydhub.com/viewer/data/res7UHjG5WSPgPStBT84xW/ID-OXY-20.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# split into input (X) and output 󰀀 variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    373\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[1;32m    374\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         compression=kwds.get('compression', None))\n\u001b[0m\u001b[1;32m    376\u001b[0m     kwds['compression'] = (inferred_compression if compression == 'infer'\n\u001b[1;32m    377\u001b[0m                            else compression)\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Content-Encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 642\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "# Visualize training history\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import keras\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "dataset = pd.read_csv(\"https://www.floydhub.com/viewer/data/res7UHjG5WSPgPStBT84xW/ID-OXY-20.csv\")\n",
    "# split into input (X) and output 󰀀 variables\n",
    "\n",
    "# Remove missing values\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# make a label dataset\n",
    "dataset[\"Label\"] = dataset[\"Mark\"]\n",
    "\n",
    "# change rest values to \n",
    "# default mode\n",
    "dataset[\"Mark\"][dataset[\"Mark\"] == \"REST\"] = 0\n",
    "# task positive network\n",
    "dataset[\"Mark\"][dataset[\"Mark\"] == \"ADDITION\"] = 1\n",
    "dataset[\"Mark\"][dataset[\"Mark\"] == \"PASSTHOUGHT\"] = 2\n",
    "dataset[\"Mark\"][dataset[\"Mark\"] == \"JUNK\"] = 3\n",
    "\n",
    "# remove the JUNK data\n",
    "dataset = dataset[dataset.Mark != 2]\n",
    "dataset = dataset[dataset.Mark != 3]\n",
    "\n",
    "# shuffle the data\n",
    "dataset = dataset.sample(frac=1)\n",
    "\n",
    "# 52 broadmann areas data\n",
    "X = np.array(dataset.ix[:, :'CH52'])\n",
    "# default mode network or task positive network\n",
    "Y = np.array([[1,0] if i == 0 else [0,1] for i in dataset.Mark])\n",
    "\n",
    "# Dropout - the number of neurons removed at each layers, who are readded when testing\n",
    "# Batch size - the number of data points added at each time, affects training time\n",
    "# Epochs - the number of training/test sessions\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "# makes the values between 0 and 1\n",
    "model.add(BatchNormalization(input_shape=(52,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "\n",
    "model.add(Dense(2, init=\"normal\", activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "history = model.fit(X, Y, validation_split=0.5, nb_epoch=50, batch_size=50, verbose=1)\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Image recognition\n",
    "One area that is suitable for machine learning is image recognition of medical imaging data. This is because image recognition is a mature problem domain and a large amount of image data is used in healthcare.\n",
    "\n",
    "To get started with image recognition we will now show you how to get started with image recognition using deep neural networks. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2002 images belonging to 2 classes.\n",
      "Found 801 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "  9/125 [=>............................] - ETA: 67s - loss: 1.6744 - acc: 0.4028 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fdcd4cb6dc53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     validation_steps=nb_validation_samples // batch_size)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first_try.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2042\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/birgermoell/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''This script goes along the blog post\n",
    "\"Building powerful image classification models using very little data\"\n",
    "from blog.keras.io.\n",
    "It uses data that can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "In our setup, we:\n",
    "- created a data/ folder\n",
    "- created train/ and validation/ subfolders inside data/\n",
    "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
    "- put the cat pictures index 0-999 in data/train/cats\n",
    "- put the cat pictures index 1000-1400 in data/validation/cats\n",
    "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
    "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
    "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
    "In summary, this is our directory structure:\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```\n",
    "'''\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "# flowdhub folder\n",
    "flowdhub = 'https://www.floydhub.com/viewer/data/xLHRt9d9UnipGEEr4FkWdF/'\n",
    "flowhubtrain = 'https://www.floydhub.com/viewer/data/xLHRt9d9UnipGEEr4FkWdF/train'\n",
    "flowdhubvalidation = 'https://www.floydhub.com/viewer/data/xLHRt9d9UnipGEEr4FkWdF/validation'\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')\n",
    "saved_model = model.to_json()\n",
    "with open('first_try.json','w') as f:\n",
    "    f.write(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the model weights\n",
    "model.save_weights('first_try.h5')\n",
    "saved_model = model.to_json()\n",
    "with open('first_try.json','w') as f:\n",
    "    f.write(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the saved model\n",
    "from keras.models import model_from_json\n",
    "# Load trained model\n",
    "# load json and create model\n",
    "json_file = open('first_try.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "global loaded_model\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"first_try.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshaping our trained image\n",
    "import scipy.misc\n",
    "\n",
    "testimage = scipy.misc.imresize(scipy.misc.imread('data/validation/cats/cat.1211.jpg'), (150, 150))\n",
    "testimage = testimage.reshape((1,) + testimage.shape)\n",
    "\n",
    "# checking the shape of our new image\n",
    "testimage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = loaded_model.predict(testimage)\n",
    "print(prediction)\n",
    "\n",
    "if (prediction[0] == 1):\n",
    "        print(\"Cat\")\n",
    "        one = True\n",
    "else:\n",
    "    print(\"Dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a restful API to serve the trained Keras Model and store it on Heroku\n",
    "## Building a AI web app\n",
    "Machine learning is it's own paradigm that is complicated to learn. However even if you learn everything about machine learning you need to figure out a way to get your machine learning model up and working online.\n",
    "There are a few different ways of doing that.\n",
    "The easiests is using a small python library, while other frameworks can be helpful for web applications\n",
    "\n",
    "# Flask mini framework\n",
    "Flask is a micro framework for creating web server. With it you can create a web application that serves up your trained machine learning models in an API.\n",
    "# Trained keras model\n",
    "To be able to serve a model we need to train a model that can be returned with an API call. We have already trained our model. So now our job is to load our trained model.\n",
    "Then we will build an API that returns neural networks best guess of what the image is depicting, a cat or a dog.\n",
    "## Building an app in Flask\n",
    "\n",
    "The example code below is NOT a functioning Flask Application. It is however a rest API that is close to working. Here you can learn more about flask http://flask.pocoo.org/\n",
    "It is your job to find a way to make it work. You are of course allowed to try other methods of making it work. Tensorflow serving is one method. https://www.tensorflow.org/serving/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "#!flask/bin/python\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "from flask import request\n",
    "from flask import jsonify\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# loading the model\n",
    "\n",
    "import json\n",
    "import scipy.misc\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"Hello, World!\"\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    json_file = open('first_try.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"first_try.h5\")\n",
    "    print('loaded the model')\n",
    "    print(request.files)\n",
    "    file = request.files['file']\n",
    "    print(file.filename)\n",
    "    #file.filename=\"our.jpg\"\n",
    "    #print(file.filename)\n",
    "    #print(file.filename)\n",
    "    print(\"testing\")\n",
    "    file.save(file.filename)\n",
    "    testimage = scipy.misc.imresize(scipy.misc.imread(file),(150,150))\n",
    "    print(testimage.shape)\n",
    "    #testimage = testimage.reshape((3, 150, 150, 1))\n",
    "    testimage = testimage.reshape((1,) + testimage.shape)\n",
    "    prediction = loaded_model.predict(testimage).astype(float)\n",
    "    print(prediction)\n",
    "    return jsonify({ 'classification': { 'dog': prediction[0][0], 'cat' : 1-prediction[0][0]} })\n",
    "    #return json.dump({ 'classification': { 'cat': prediction[0][0], 'dog' : 1-prediction[0][0]} })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
