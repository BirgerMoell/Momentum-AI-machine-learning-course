{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARAAAAECCAYAAAA2FIiFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b63a748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#exercise\n",
    "\n",
    "input = np.array([[0.52, 0.47, 0.6, 0.5, 0.56]])\n",
    "\n",
    "# plt.matshow(input)\n",
    "# plt.show()\n",
    "\n",
    "plt.matshow(np.arange(0,1, 16).reshape([-1,1] ))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Do you like gradient descent. Look at this notebook.\n",
    "Or put some stuff from that notebook in our notebook.\n",
    "\n",
    "## Objective functions (Loss function)\n",
    "\n",
    "### Derivatives\n",
    "\n",
    "### Chain rule\n",
    "\n",
    "4 + w1 = 5\n",
    "\n",
    "loss equals 1\n",
    "\n",
    "4 + w2 = 3\n",
    "\n",
    "loss equals -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Forward prop\n",
    "\n",
    "## Activation function\n",
    "\n",
    "### Sigmoid, Relu, Softmax\n",
    "\n",
    "### Non-linear relationships\n",
    "\n",
    "\n",
    "## Weight multiplication\n",
    "\n",
    "w*x\n",
    "\n",
    "\n",
    "## Initialisation\n",
    "\n",
    "kx + m = y\n",
    "w*x + b = y\n",
    "\n",
    "w1*x1\n",
    "\n",
    "w1*x1 + b\n",
    "\n",
    "## Universal function approximator\n",
    "\n",
    "### LSTM = Turing Machine\n",
    "\n",
    "\n",
    "## Normalization\n",
    "\n",
    "Normalizisation of input values to avoid feature scaling\n",
    "\n",
    "Normalization of output valies to have fun\n",
    "\n",
    "\n",
    "# Have fun\n",
    "\n",
    "## Feynman lectures\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your Deep Neural Network: Step by Step\n",
    "\n",
    "Welcome to your week 4 assignment (part 1 of 2)! You have previously trained a 2-layer Neural Network (with a single hidden layer). This week, you will build a deep neural network, with as many layers as you want!\n",
    "\n",
    "- In this notebook, you will implement all the functions required to build a deep neural network.\n",
    "- In the next assignment, you will use these functions to build a deep neural network for image classification.\n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "- Use non-linear units like ReLU to improve your model\n",
    "- Build a deeper neural network (with more than 1 hidden layer)\n",
    "- Implement an easy-to-use neural network class\n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- dnn_utils provides some necessary functions for this notebook.\n",
    "- testCases provides some test cases to assess the correctness of your functions\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'testCases_v2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cd2cb3cc70a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtestCases_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdnn_utils_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'testCases_v2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v2 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Gradient Descent\n",
    "\n",
    "A simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all $m$ examples on each step, it is also called Batch Gradient Descent. \n",
    "\n",
    "**Warm-up exercise**: Implement the gradient descent update rule. The  gradient descent rule is, for $l = 1, ..., L$: \n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}$$\n",
    "\n",
    "where L is the number of layers and $\\alpha$ is the learning rate. All parameters should be stored in the `parameters` dictionary. Note that the iterator `l` starts at 0 in the `for` loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift `l` to `l+1` when coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters_with_gd\n",
    "\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using one step of gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters to be updated:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients to update each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        parameters[\"W\" + str(l+1)] = None\n",
    "        parameters[\"b\" + str(l+1)] = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural network as a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4caf9434c108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcost1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "cost1 = NN.costFunction(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dJdW1, dJdW2 = NN.costFunctionPrime(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dJdW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle #saving and loading our serialized model \n",
    "import numpy as np #matrix math\n",
    "from app.model.preprocessor import Preprocessor as img_prep #image preprocessing\n",
    "\n",
    "#class for loading our saved model and classifying new images\n",
    "class LiteOCR:\n",
    "    \n",
    "\tdef __init__(self, fn=\"alpha_weights.pkl\", pool_size=2):\n",
    "        #load the weights from the pickle file and the meta data\n",
    "\t\t[weights, meta] = pickle.load(open(fn, 'rb'), encoding='latin1') #currently, this class MUST be initialized from a pickle file\n",
    "\t\t#list to store labels\n",
    "        self.vocab = meta[\"vocab\"]\n",
    "        \n",
    "        #how many rows and columns in an image\n",
    "\t\tself.img_rows = meta[\"img_side\"] ; self.img_cols = meta[\"img_side\"]\n",
    "        \n",
    "        #load our CNN\n",
    "\t\tself.CNN = LiteCNN()\n",
    "        #with our saved weights\n",
    "\t\tself.CNN.load_weights(weights)\n",
    "        #define the pooling layers size\n",
    "\t\tself.CNN.pool_size=int(pool_size)\n",
    "    \n",
    "    #classify new image\n",
    "\tdef predict(self, image):\n",
    "\t\tprint(image.shape)\n",
    "        #vectorize the image into the right shape for our network\n",
    "\t\tX = np.reshape(image, (1, 1, self.img_rows, self.img_cols))\n",
    "\t\tX = X.astype(\"float32\")\n",
    "        \n",
    "        #make the prediction\n",
    "\t\tpredicted_i = self.CNN.predict(X)\n",
    "        #return the predicted label\n",
    "\t\treturn self.vocab[predicted_i]\n",
    "\n",
    "class LiteCNN:\n",
    "\tdef __init__(self):\n",
    "        # a place to store the layers\n",
    "\t\tself.layers = [] \n",
    "        # size of pooling area for max pooling\n",
    "\t\tself.pool_size = None \n",
    "\n",
    "\tdef load_weights(self, weights):\n",
    "\t\tassert not self.layers, \"Weights can only be loaded once!\"\n",
    "        #add the saved matrix values to the convolutional network\n",
    "\t\tfor k in range(len(weights.keys())):\n",
    "\t\t\tself.layers.append(weights['layer_{}'.format(k)])\n",
    "\n",
    "\tdef predict(self, X):        \n",
    "        #here is where the network magic happens at a high level\n",
    "        h = self.cnn_layer(X, layer_i=0, border_mode=\"full\"); X= h\n",
    "        h = self.relu_layer(X); X = h;\n",
    "        h = self.cnn_layer(X, layer_i=2, border_mode=\"valid\"); X = h\n",
    "        h = self.relu_layer(X); X = h;\n",
    "        h = self.maxpooling_layer(X); X = h\n",
    "        h = self.dropout_layer(X, .25); X = h\n",
    "        h = self.flatten_layer(X, layer_i=7); X = h;\n",
    "        h = self.dense_layer(X, fully, layer_i=10); x = H\n",
    "        h = self.softmax_layer2D(X); x = h\n",
    "        max_i = self.classify(X)\n",
    "        return max_i[0]\n",
    "    \n",
    "    #given our feature map we've learned from convolving around the image\n",
    "    #lets make it more dense by performing pooling, specifically max pooling\n",
    "    #we'll select the max values from the image matrix and use that as our new feature map\n",
    "\tdef maxpooling_layer(self, convolved_features):\n",
    "        #given our learned features and images\n",
    "\t\tnb_features = convolved_features.shape[0]\n",
    "\t\tnb_images = convolved_features.shape[1]\n",
    "\t\tconv_dim = convolved_features.shape[2]\n",
    "\t\tres_dim = int(conv_dim / self.pool_size)       #assumed square shape\n",
    "\n",
    "        #initialize our more dense feature list as empty\n",
    "\t\tpooled_features = np.zeros((nb_features, nb_images, res_dim, res_dim))\n",
    "        #for each image\n",
    "\t\tfor image_i in range(nb_images):\n",
    "            #and each feature map\n",
    "\t\t\tfor feature_i in range(nb_features):\n",
    "                #begin by the row\n",
    "\t\t\t\tfor pool_row in range(res_dim):\n",
    "                    #define start and end points\n",
    "\t\t\t\t\trow_start = pool_row * self.pool_size\n",
    "\t\t\t\t\trow_end   = row_start + self.pool_size\n",
    "\n",
    "                    #for each column (so its a 2D iteration)\n",
    "\t\t\t\t\tfor pool_col in range(res_dim):\n",
    "                        #define start and end points\n",
    "\t\t\t\t\t\tcol_start = pool_col * self.pool_size\n",
    "\t\t\t\t\t\tcol_end   = col_start + self.pool_size\n",
    "                        \n",
    "                        #define a patch given our defined starting ending points\n",
    "\t\t\t\t\t\tpatch = convolved_features[feature_i, image_i, row_start : row_end,col_start : col_end]\n",
    "                        #then take the max value from that patch\n",
    "                        #store it. this is our new learned feature/filter\n",
    "\t\t\t\t\t\tpooled_features[feature_i, image_i, pool_row, pool_col] = np.max(patch)\n",
    "\t\treturn pooled_features\n",
    "\n",
    "    #convolution is the most important of the matrix operations here\n",
    "    #well define our input, lauyer number, and a border mode (explained below)\n",
    "\tdef cnn_layer(self, X, layer_i=0, border_mode = \"full\"):\n",
    "        #we'll store our feature maps and bias value in these 2 vars\n",
    "\t\tfeatures = self.layers[layer_i][\"param_0\"]\n",
    "\t\tbias = self.layers[layer_i][\"param_1\"]\n",
    "        #how big is our filter/patch?\n",
    "\t\tpatch_dim = features[0].shape[-1]\n",
    "        #how many features do we have?\n",
    "\t\tnb_features = features.shape[0]\n",
    "        #How big is our image?\n",
    "\t\timage_dim = X.shape[2] #assume image square\n",
    "        #R G B values\n",
    "\t\timage_channels = X.shape[1]\n",
    "        #how many images do we have?\n",
    "\t\tnb_images = X.shape[0]\n",
    "        \n",
    "        #With border mode \"full\" you get an output that is the \"full\" size as the input. \n",
    "        #That means that the filter has to go outside the bounds of the input by \"filter size / 2\" - \n",
    "        #the area outside of the input is normally padded with zeros.\n",
    "\t\tif border_mode == \"full\":\n",
    "\t\t\tconv_dim = image_dim + patch_dim - 1\n",
    "        #With border mode \"valid\" you get an output that is smaller than the input because \n",
    "        #the convolution is only computed where the input and the filter fully overlap.\n",
    "\t\telif border_mode == \"valid\":\n",
    "\t\t\tconv_dim = image_dim - patch_dim + 1\n",
    "        \n",
    "        #we'll initialize our feature matrix\n",
    "\t\tconvolved_features = np.zeros((nb_images, nb_features, conv_dim, conv_dim));\n",
    "        #then we'll iterate through each image that we have\n",
    "\t\tfor image_i in range(nb_images):\n",
    "            #for each feature \n",
    "\t\t\tfor feature_i in range(nb_features):\n",
    "                #lets initialize a convolved image as empty\n",
    "\t\t\t\tconvolved_image = np.zeros((conv_dim, conv_dim))\n",
    "                #then for each channel (r g b )\n",
    "\t\t\t\tfor channel in range(image_channels):\n",
    "                    #lets extract a feature from our feature map\n",
    "\t\t\t\t\tfeature = features[feature_i, channel, :, :]\n",
    "                    #then define a channel specific part of our image\n",
    "\t\t\t\t\timage   = X[image_i, channel, :, :]\n",
    "                    #perform convolution on our image, using a given feature filter\n",
    "\t\t\t\t\tconvolved_image += self.convolve2d(image, feature, border_mode);\n",
    "\n",
    "                #add a bias to our convoved image\n",
    "\t\t\t\tconvolved_image = convolved_image + bias[feature_i]\n",
    "                #add it to our list of convolved features (learnings)\n",
    "\t\t\t\tconvolved_features[image_i, feature_i, :, :] = convolved_image\n",
    "\t\treturn convolved_features\n",
    "\n",
    "    #In a dense layer, every node in the layer is connected to every node in the preceding layer.\n",
    "\tdef dense_layer(self, X, layer_i=0):\n",
    "        #so we'll initialize our weight and bias for this layer\n",
    "\t\tW = self.layers[layer_i][\"param_0\"]\n",
    "\t\tb = self.layers[layer_i][\"param_1\"]\n",
    "        #and multiply it by our input (dot product)\n",
    "\t\toutput = np.dot(X, W) + b\n",
    "\t\treturn output\n",
    "\n",
    "\t@staticmethod\n",
    "    \n",
    "    #so what does the convolution operation look like?, given an image and a feature map (filter)\n",
    "\tdef convolve2d(image, feature, border_mode=\"full\"):\n",
    "        #we'll define the tensor dimensions of the image and the feature\n",
    "\t\timage_dim = np.array(image.shape)\n",
    "\t\tfeature_dim = np.array(feature.shape)\n",
    "        #as well as a target dimension\n",
    "\t\ttarget_dim = image_dim + feature_dim - 1\n",
    "        #then we'll perform a fast fourier transform on both the input and the filter\n",
    "        #performing a convolution can be written as a for loop but for many convolutions\n",
    "        #this approach is too comp. expensive/slow. it can be performed orders of magnitude\n",
    "        #faster using a fast fourier transform. \n",
    "\t\tfft_result = np.fft.fft2(image, target_dim) * np.fft.fft2(feature, target_dim)\n",
    "        #and set the result to our target \n",
    "\t\ttarget = np.fft.ifft2(fft_result).real\n",
    "\n",
    "\t\tif border_mode == \"valid\":\n",
    "\t\t\t# To compute a valid shape, either np.all(x_shape >= y_shape) or\n",
    "\t\t\t# np.all(y_shape >= x_shape).\n",
    "            #decide a target dimension to convolve around\n",
    "\t\t\tvalid_dim = image_dim - feature_dim + 1\n",
    "\t\t\tif np.any(valid_dim < 1):\n",
    "\t\t\t\tvalid_dim = feature_dim - image_dim + 1\n",
    "\t\t\tstart_i = (target_dim - valid_dim) // 2\n",
    "\t\t\tend_i = start_i + valid_dim\n",
    "\t\t\ttarget = target[start_i[0]:end_i[0], start_i[1]:end_i[1]]\n",
    "\t\treturn target\n",
    "\n",
    "\tdef relu_layer(x):\n",
    "        #turn all negative values in a matrix into zeros\n",
    "\t\tz = np.zeros_like(x)\n",
    "\t\treturn np.where(x>z,x,z)\n",
    "\n",
    "\tdef softmax_layer2D(w):\n",
    "        #this function will calculate the probabilities of each\n",
    "        #target class over all possible target classes. \n",
    "\t\tmaxes = np.amax(w, axis=1)\n",
    "\t\tmaxes = maxes.reshape(maxes.shape[0], 1)\n",
    "\t\te = np.exp(w - maxes)\n",
    "\t\tdist = e / np.sum(e, axis=1, keepdims=True)\n",
    "\t\treturn dist\n",
    "\n",
    "    #affect the probability a node will be turned off by multiplying it\n",
    "    #by a p values (.25 we define)\n",
    "\tdef dropout_layer(X, p):\n",
    "\t\tretain_prob = 1. - p\n",
    "\t\tX *= retain_prob\n",
    "\t\treturn X\n",
    "\n",
    "    #get the largest probabililty value from the list\n",
    "\tdef classify(X):\n",
    "\t\treturn X.argmax(axis=-1)\n",
    "\n",
    "    #tensor transformation, less dimensions\n",
    "\tdef flatten_layer(X):\n",
    "\t\tflatX = np.zeros((X.shape[0],np.prod(X.shape[1:])))\n",
    "\t\tfor i in range(X.shape[0]):\n",
    "\t\t\tflatX[i,:] = X[i].flatten(order='C')\n",
    "\t\treturn flatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "## Making XOR\n",
    "![title](files/xor.png)\n",
    "Exclusive or or exclusive disjunction is a logical operation that outputs true only when inputs differ (one is true, the other is false).\n",
    "It is symbolized by the prefix operator J and by the infix operators XOR (/ˌɛks ˈɔːr/), EOR, EXOR, ⊻, ⊕, ↮, and ≢. The negation of XOR is logical biconditional, which outputs true only when both inputs are the same.\n",
    "It gains the name \"exclusive or\" because the meaning of \"or\" is ambiguous when both operands are true; the exclusive or operator excludes that case. This is sometimes thought of as \"one or the other but not both\". This could be written as \"A or B, but not, A and B\".\n",
    "More generally, XOR is true only when an odd number of inputs are true. A chain of XORs—a XOR b XOR c XOR d (and so on)—is true whenever an odd number of the inputs are true and is false whenever an even number of inputs are true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x, deriv=False):  \n",
    "    if(deriv==True):\n",
    "        return (x*(1-x))\n",
    "    \n",
    "    return 1/(1+np.exp(-x))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#input data\n",
    "X = np.array([[0,0,1],  # Note: there is a typo on this line in the video\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the exclusive OR function follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#output data\n",
    "y = np.array([[0],\n",
    "             [1],\n",
    "             [1],\n",
    "             [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seed for the random generator is set so that it will return the same random numbers each time, which is sometimes useful for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we intialize the weights to random values. syn0 are the weights between the input layer and the hidden layer. It is a 3x4 matrix because there are two input weights plus a bias term (=3) and four nodes in the hidden layer (=4). syn1 are the weights between the hidden layer and the output layer. It is a 4x1 matrix because there are 4 nodes in the hidden layer and one output. Note that there is no bias term feeding the output layer in this example. The weights are initially generated randomly because optimization tends not to work well when all the weights start at the same value. Note that neither of the neural networks shown in the video describe the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#synapses\n",
    "syn0 = 2*np.random.random((3,4)) - 1  # 3x4 matrix of weights ((2 inputs + 1 bias) x 4 nodes in the hidden layer)\n",
    "syn1 = 2*np.random.random((4,1)) - 1  # 4x1 matrix of weights. (4 nodes x 1 output) - no bias term in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main training loop. The output shows the evolution of the error between the model and desired. The error steadily decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.500386946116\n",
      "Error: 0.00867360161359\n",
      "Error: 0.00586357615026\n",
      "Error: 0.00469264457551\n",
      "Error: 0.0040148843226\n",
      "Error: 0.00356083564683\n",
      "Output after training\n",
      "[[ 0.00271394]\n",
      " [ 0.99695061]\n",
      " [ 0.99678411]\n",
      " [ 0.00394104]]\n"
     ]
    }
   ],
   "source": [
    "#training step\n",
    "# Python2 Note: In the follow command, you may improve \n",
    "#   performance by replacing 'range' with 'xrange'. \n",
    "for j in range(60000):  \n",
    "    \n",
    "    # Calculate forward through the network.\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0, syn0))\n",
    "    l2 = nonlin(np.dot(l1, syn1))\n",
    "    \n",
    "    # Back propagation of errors using the chain rule. \n",
    "    l2_error = y - l2\n",
    "    if(j % 10000) == 0:   # Only print the error every 10000 steps, to save time and limit the amount of output. \n",
    "        print(\"Error: \" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    l2_delta = l2_error*nonlin(l2, deriv=True)\n",
    "    \n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "    \n",
    "    #update weights (no learning rate term)\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "    \n",
    "print(\"Output after training\")\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
