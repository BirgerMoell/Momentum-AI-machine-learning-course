{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARAAAAECCAYAAAA2FIiFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b63a748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#exercise\n",
    "\n",
    "input = np.array([[0.52, 0.47, 0.6, 0.5, 0.56]])\n",
    "\n",
    "# plt.matshow(input)\n",
    "# plt.show()\n",
    "\n",
    "plt.matshow(np.arange(0,1, 16).reshape([-1,1] ))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Do you like gradient descent. Look at this notebook.\n",
    "Or put some stuff from that notebook in our notebook.\n",
    "\n",
    "## Objective functions (Loss function)\n",
    "\n",
    "### Derivatives\n",
    "\n",
    "### Chain rule\n",
    "\n",
    "4 + w1 = 5\n",
    "\n",
    "loss equals 1\n",
    "\n",
    "4 + w2 = 3\n",
    "\n",
    "loss equals -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Forward prop\n",
    "\n",
    "## Activation function\n",
    "\n",
    "### Sigmoid, Relu, Softmax\n",
    "\n",
    "### Non-linear relationships\n",
    "\n",
    "\n",
    "## Weight multiplication\n",
    "\n",
    "w*x\n",
    "\n",
    "\n",
    "## Initialisation\n",
    "\n",
    "kx + m = y\n",
    "w*x + b = y\n",
    "\n",
    "w1*x1\n",
    "\n",
    "w1*x1 + b\n",
    "\n",
    "## Universal function approximator\n",
    "\n",
    "### LSTM = Turing Machine\n",
    "\n",
    "\n",
    "## Normalization\n",
    "\n",
    "Normalizisation of input values to avoid feature scaling\n",
    "\n",
    "Normalization of output valies to have fun\n",
    "\n",
    "\n",
    "# Have fun\n",
    "\n",
    "## Feynman lectures\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your Deep Neural Network: Step by Step\n",
    "\n",
    "Welcome to your week 4 assignment (part 1 of 2)! You have previously trained a 2-layer Neural Network (with a single hidden layer). This week, you will build a deep neural network, with as many layers as you want!\n",
    "\n",
    "- In this notebook, you will implement all the functions required to build a deep neural network.\n",
    "- In the next assignment, you will use these functions to build a deep neural network for image classification.\n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "- Use non-linear units like ReLU to improve your model\n",
    "- Build a deeper neural network (with more than 1 hidden layer)\n",
    "- Implement an easy-to-use neural network class\n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- dnn_utils provides some necessary functions for this notebook.\n",
    "- testCases provides some test cases to assess the correctness of your functions\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'testCases_v2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cd2cb3cc70a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtestCases_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdnn_utils_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'testCases_v2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v2 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Gradient Descent\n",
    "\n",
    "A simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all $m$ examples on each step, it is also called Batch Gradient Descent. \n",
    "\n",
    "**Warm-up exercise**: Implement the gradient descent update rule. The  gradient descent rule is, for $l = 1, ..., L$: \n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}$$\n",
    "\n",
    "where L is the number of layers and $\\alpha$ is the learning rate. All parameters should be stored in the `parameters` dictionary. Note that the iterator `l` starts at 0 in the `for` loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift `l` to `l+1` when coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters_with_gd\n",
    "\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using one step of gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters to be updated:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients to update each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        parameters[\"W\" + str(l+1)] = None\n",
    "        parameters[\"b\" + str(l+1)] = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural network as a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4caf9434c108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcost1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "cost1 = NN.costFunction(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dJdW1, dJdW2 = NN.costFunctionPrime(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dJdW1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
