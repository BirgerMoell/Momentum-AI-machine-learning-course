# The math of deep learning

# Backprop

## Gradient descent

Do you like gradient descent. Look at this notebook.
Or put some stuff from that notebook in our notebook.

## Objective functions (Loss function)

### Derivatives

### Chain rule

4 + w1 = 5

loss equals 1

4 + w2 = 3

loss equals -1





# Forward prop

## Activation function

### Sigmoid
Inline-style: 
![alt text](https://imgur.com/mfJoW6t "Sigmoid")


, Relu, Softmax

### Non-linear relationships


## Weight multiplication

w*x


## Initialisation

kx + m = y
w*x + b = y

w1*x1

w1*x1 + b

## Universal function approximator

### LSTM = Turing Machine


## Normalization

Normalizisation of input values to avoid feature scaling

Normalization of output valies to have fun


# Have fun

## Feynman lectures



