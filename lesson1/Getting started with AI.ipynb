{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is AI?\n",
    "\n",
    "https://en.wikipedia.org/wiki/Artificial_intelligence\n",
    "\n",
    "AI or Artificial Intelligence is the idea that intelligence can be defined so precisely that we can make an make a machine perform all the required steps to become intelligent.\n",
    "\n",
    "One of the fields long term goals is general intelligence, making machine that can think like humans.\n",
    "\n",
    "## Why is AI suddenly so popular?\n",
    "AI has suddenly become incredibly popular. It is mainly based on two factors, the increased computational powers of modern computers and the availability of huge datasets.\n",
    "\n",
    "AI like people gain information from the world that they use to make predictions on how to act. The more information, the better the prediction. \n",
    "\n",
    "## AI and neuroscience\n",
    "AI and neuroscience are two fields that are becoming more and more intertwined. The most effective forms of AI are based on theories for how the brain works. Artificial neural networks are modeled on the biological neural networks of the brain. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a Jupyter notebook. \n",
    "\n",
    "It's an interactive way to write Python code. Most of machine learning is done using Python because Python is great for data analysis and the biggest machine learning libraries (Tensorflow and Keras) are made in Python.\n",
    "\n",
    "Here you can learn how to get started with Jupyter Notebooks.\n",
    "http://jupyter.org/\n",
    "\n",
    "In Machine Learning there really isn't a big question of what language to use. The answer is obviously Python. So just start learning Python.\n",
    "https://www.python.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is machine learning?\n",
    "![Datasteps](files/datasteps.jpg)\n",
    "\n",
    "Machine learning is a part of AI. It is the subfield of computer science that, according to Arthur Samuel in 1959, gives \"computers the ability to learn without being explicitly programmed.\"\n",
    "https://en.wikipedia.org/wiki/Machine_learning\n",
    "\n",
    "Machine Learning is a way to make computers make decisions based on data. There are two types of machine learning, supervised learning and unsupervised learning.\n",
    "\n",
    "## Supervised Learning\n",
    "Our work will focus on supervised learning because it is the field that is making the most progress right now and is the most mature. So it's the easiest to get up and running in production. \n",
    "\n",
    "For all supervised learning , the first rule of machine learning is this.\n",
    "All your output is determined on your input.\n",
    "\n",
    "## Garbaged in -> Garbaged out\n",
    "\n",
    "So the biggest problem in machine learning isn't always how to train your machine learning models, it's how to get good data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do I need to do in order to get started with machine learning?\n",
    "![Machine Learning Map](files/machinelearningmap.png)\n",
    "\n",
    "## AI is a fast moving discipline\n",
    "Machine learning and AI is a huge discipline that is always growing. However it's easy to get started and it's moving really fast so you can just grab on and stay on the bleeding edge of technology.\n",
    "\n",
    "## Tensorflow\n",
    "Google open sourcing Tensorflow https://www.tensorflow.org/ made it much easier to get started in machine learning. What would have taken months to do ten years ago can now be accomplished in days.\n",
    "\n",
    "## Keras\n",
    "Even better, Keras came along as an abstraction on top of Tensorflow for deep learning, one of the most advanced parts of deep learning. \n",
    "\n",
    "## Types of machine learning\n",
    "\n",
    "There are many different types of machine learning. The most advances in machine learning have been done using artificial neural networks, so if you want to start out doing just one thing, start out with artificial neural networks.\n",
    "\n",
    "Here is a list of different types of machine learning.\n",
    "\n",
    "Decision tree learning\n",
    "Association rule learning\n",
    "Artificial neural networks\n",
    "Deep learning\n",
    "Inductive logic programming\n",
    "Support vector machines\n",
    "Clustering\n",
    "Bayesian networks\n",
    "Reinforcement learning\n",
    "Representation learning\n",
    "Similarity and metric learning\n",
    "Sparse dictionary learning\n",
    "Genetic algorithms\n",
    "Rule-based machine learning\n",
    "Learning classifier systems\n",
    "\n",
    "\n",
    "## What you need to know about data and machine learning\n",
    "Machine learning is a way to make advanced statistical models using math. It’s a way to make a computer guess. For machine learning to work you need good data, the more data the better. So before you go further it might be good to take a moment and think about data. The reason why machine learning models are becoming so good is that we have so much data. However machine learning models can’t perform magic (even thought it might seem like it).\n",
    "\n",
    "## Garbage in = Garbage out\n",
    "\n",
    "Take a moment and think about what problem you are trying to solve? What data do you need for solving your problem?\n",
    "\n",
    "## Classic machine learning datasets\n",
    "\n",
    "## MNIST\n",
    "MNIST is a dataset of handwritten digits. It is the hello world of machine learning. The goal with MNIST is to classify (guess) which handwritten digit is displayed by use of a machine learning model.\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "## IRIS\n",
    "IRIS is another equally famous machine learning dataset. Iris is a dataset for classifying different types of flowers.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/iris\n",
    "\n",
    "\n",
    "## Open Datasets from Kaggle\n",
    "\n",
    "Here are places you can find ready made datasets for machine learning\n",
    "http://kaggle.com/\n",
    "\n",
    "Here is a list of all the datasets available at Kaggle with a link on where to download them. \n",
    "\n",
    "Dataset\n",
    "Link\n",
    "IMDB\n",
    "\n",
    "https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset\n",
    "Soccer scores dataset\n",
    "https://www.kaggle.com/hugomathien/soccer\n",
    "Credit card fraud detection\n",
    "\n",
    "\n",
    "https://www.kaggle.com/dalpozz/creditcardfraud\n",
    "Human resources analytics\n",
    "\n",
    "\n",
    "https://www.kaggle.com/ludobenistant/hr-analytics\n",
    "Food facts\n",
    "\n",
    "\n",
    "https://www.kaggle.com/openfoodfacts/world-food-facts\n",
    "Climate change\n",
    "\n",
    "\n",
    "https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "Daily news for stock market predictions\n",
    "https://www.kaggle.com/aaron7sun/stocknews\n",
    "New York Stock Exchange\n",
    "https://www.kaggle.com/dgawlik/nyse\n",
    "US Stocks Fundamentals\n",
    "\n",
    "\n",
    "https://www.kaggle.com/usfundamentals/us-stocks-fundamentals\n",
    "Bitcoin historical data\n",
    "https://www.kaggle.com/mczielinski/bitcoin-historical-data\n",
    "Adult census income, predict earning based on census\n",
    "https://www.kaggle.com/uciml/adult-census-income\n",
    "House sales in King County\n",
    "https://www.kaggle.com/harlfoxem/housesalesprediction\n",
    "EEG brain waves\n",
    "https://www.kaggle.com/wanghaohan/eeg-brain-wave-for-confusion\n",
    "Synchronized brain wave dataset\n",
    "https://www.kaggle.com/berkeley-biosense/synchronized-brainwave-dataset\n",
    "CT medical image analysis \n",
    "https://www.kaggle.com/kmader/siim-medical-image-analysis-tutorial\n",
    "Structural MRI Dataset\n",
    "https://www.kaggle.com/ilknuricke/neurohackinginrimages\n",
    "Circadian rhythm in the brain\n",
    "https://www.kaggle.com/kmader/circadian-rhythm-in-the-brain\n",
    "Complete genome dataset\n",
    "https://www.kaggle.com/zusmani/mygenome\n",
    "Question answers dataset 100 000 + questions\n",
    "https://www.kaggle.com/stanfordu/stanford-question-answering-dataset\n",
    "What is a note\n",
    "https://www.kaggle.com/juliancienfuegos/what-is-a-note\n",
    "Health nutrition and population across the world\n",
    "https://www.kaggle.com/theworldbank/health-nutrition-and-population-statistics\n",
    "\n",
    "\n",
    "## Scrape data for machine learning\n",
    "Scraping is another way to get data. Scraping means to request data from websites and store it offline for later analysis.\n",
    "\n",
    "Here you can learn more about scraping.\n",
    "\n",
    "https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an artificial neural network?\n",
    "![Machine Learning Map](files/neuralnetwork.png)\n",
    "\n",
    "The part of machine learning that has been making the most progress in the last years are artificial neural networks, a form of machine learning that is loosely modeled on how the neurons in our brain works. Artificual neural network has been proved really effective at making progress in many different domains.\n",
    "\n",
    "For getting started with artificial neural networks, Keras is an excellent library built on top of the Tensorflow API.\n",
    "https://keras.io/\n",
    "\n",
    "\n",
    "## Here is a good video explaining artificial neural networks\n",
    "https://www.youtube.com/watch?v=ILsA4nyG7I0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The math of machine learning\n",
    "\n",
    "![Image of Activation function](files/activation.jpeg)\n",
    "The truth is that math is a big part of machine learning but you don't need to know any math to get started with machine learning. It's an excellent idea to pick up the math along the way and to expand the field of machine learning you definitely need to learn math.\n",
    "\n",
    "If you are curious, here are some resources for looking at the math behind deep learning.\n",
    "\n",
    "## The different math sources\n",
    "AI draws math from three different sources, linear algebra, calculus and statistics. \n",
    "\n",
    "### Linear algebra\n",
    "http://machinelearningmastery.com/linear-algebra-machine-learning/\n",
    "\n",
    "### Calculus\n",
    "https://www.umiacs.umd.edu/~hal/courses/2013S_ML/math4ml.pdf\n",
    "\n",
    "### Statistics\n",
    "http://machinelearningmastery.com/crash-course-statistics-machine-learning/\n",
    "\n",
    "## Neural network from scratch\n",
    "Building a simple neural network from scratch. Walkthrough and experiments. \n",
    "http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lets train our first neural network!\n",
    "To train a neural network we need two things.\n",
    "1. A dataset with a classifier to train on\n",
    "2. A mathematical neural network\n",
    "\n",
    "## Task default dataset from my brain\n",
    "To make it fun, lets use a dataset that takes input data from my brain measured with laser sensors to try to determine what brain region is active, the default mode network or the task positive network.\n",
    "\n",
    "The data takes measurement from two probes placed on my forehead and returns eight values of oxygenated and deoxygenated blood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Oxy1</th>\n",
       "      <th>DeOxy1</th>\n",
       "      <th>Oxy2</th>\n",
       "      <th>DeOxy2</th>\n",
       "      <th>Oxy3</th>\n",
       "      <th>DeOxy3</th>\n",
       "      <th>Oxy4</th>\n",
       "      <th>DeOxy4</th>\n",
       "      <th>Oxy5</th>\n",
       "      <th>DeOxy5</th>\n",
       "      <th>Oxy6</th>\n",
       "      <th>DeOxy6</th>\n",
       "      <th>Oxy7</th>\n",
       "      <th>DeOxy7</th>\n",
       "      <th>Oxy8</th>\n",
       "      <th>DeOxy8</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-2.590000e-305</td>\n",
       "      <td>3.087000e-305</td>\n",
       "      <td>-4.040000e-306</td>\n",
       "      <td>4.816000e-306</td>\n",
       "      <td>-4.747000e-306</td>\n",
       "      <td>5.658000e-306</td>\n",
       "      <td>-3.440000e-306</td>\n",
       "      <td>4.100000e-306</td>\n",
       "      <td>5.880000e-305</td>\n",
       "      <td>-7.009000e-305</td>\n",
       "      <td>4.894000e-306</td>\n",
       "      <td>-5.833000e-306</td>\n",
       "      <td>3.826000e-306</td>\n",
       "      <td>-4.561000e-306</td>\n",
       "      <td>1.449000e-306</td>\n",
       "      <td>-1.727000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-2.315000e-305</td>\n",
       "      <td>2.759000e-305</td>\n",
       "      <td>-3.601000e-306</td>\n",
       "      <td>4.292000e-306</td>\n",
       "      <td>-4.902000e-306</td>\n",
       "      <td>5.843000e-306</td>\n",
       "      <td>-3.444000e-306</td>\n",
       "      <td>4.106000e-306</td>\n",
       "      <td>6.224000e-305</td>\n",
       "      <td>-7.418000e-305</td>\n",
       "      <td>4.948000e-306</td>\n",
       "      <td>-5.898000e-306</td>\n",
       "      <td>3.715000e-306</td>\n",
       "      <td>-4.428000e-306</td>\n",
       "      <td>1.947000e-306</td>\n",
       "      <td>-2.321000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-2.571000e-305</td>\n",
       "      <td>3.065000e-305</td>\n",
       "      <td>-3.766000e-306</td>\n",
       "      <td>4.489000e-306</td>\n",
       "      <td>-4.511000e-306</td>\n",
       "      <td>5.376000e-306</td>\n",
       "      <td>-3.847000e-306</td>\n",
       "      <td>4.586000e-306</td>\n",
       "      <td>5.882000e-305</td>\n",
       "      <td>-7.011000e-305</td>\n",
       "      <td>4.274000e-306</td>\n",
       "      <td>-5.095000e-306</td>\n",
       "      <td>4.150000e-306</td>\n",
       "      <td>-4.947000e-306</td>\n",
       "      <td>1.831000e-306</td>\n",
       "      <td>-2.182000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-3.335000e-305</td>\n",
       "      <td>3.975000e-305</td>\n",
       "      <td>-4.188000e-306</td>\n",
       "      <td>4.992000e-306</td>\n",
       "      <td>-4.373000e-306</td>\n",
       "      <td>5.213000e-306</td>\n",
       "      <td>-3.726000e-306</td>\n",
       "      <td>4.442000e-306</td>\n",
       "      <td>5.547000e-305</td>\n",
       "      <td>-6.612000e-305</td>\n",
       "      <td>4.184000e-306</td>\n",
       "      <td>-4.987000e-306</td>\n",
       "      <td>3.722000e-306</td>\n",
       "      <td>-4.436000e-306</td>\n",
       "      <td>1.948000e-306</td>\n",
       "      <td>-2.322000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-2.897000e-305</td>\n",
       "      <td>3.453000e-305</td>\n",
       "      <td>-4.086000e-306</td>\n",
       "      <td>4.870000e-306</td>\n",
       "      <td>-3.768000e-306</td>\n",
       "      <td>4.491000e-306</td>\n",
       "      <td>-3.724000e-306</td>\n",
       "      <td>4.439000e-306</td>\n",
       "      <td>5.415000e-305</td>\n",
       "      <td>-6.454000e-305</td>\n",
       "      <td>3.878000e-306</td>\n",
       "      <td>-4.622000e-306</td>\n",
       "      <td>3.749000e-306</td>\n",
       "      <td>-4.469000e-306</td>\n",
       "      <td>1.614000e-306</td>\n",
       "      <td>-1.923000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>-3.190000e-305</td>\n",
       "      <td>3.802000e-305</td>\n",
       "      <td>-4.075000e-306</td>\n",
       "      <td>4.857000e-306</td>\n",
       "      <td>-4.281000e-306</td>\n",
       "      <td>5.103000e-306</td>\n",
       "      <td>-3.555000e-306</td>\n",
       "      <td>4.237000e-306</td>\n",
       "      <td>5.650000e-305</td>\n",
       "      <td>-6.734000e-305</td>\n",
       "      <td>4.038000e-306</td>\n",
       "      <td>-4.813000e-306</td>\n",
       "      <td>3.536000e-306</td>\n",
       "      <td>-4.214000e-306</td>\n",
       "      <td>1.665000e-306</td>\n",
       "      <td>-1.985000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>-2.973000e-305</td>\n",
       "      <td>3.544000e-305</td>\n",
       "      <td>-3.587000e-306</td>\n",
       "      <td>4.275000e-306</td>\n",
       "      <td>-4.187000e-306</td>\n",
       "      <td>4.991000e-306</td>\n",
       "      <td>-3.385000e-306</td>\n",
       "      <td>4.035000e-306</td>\n",
       "      <td>5.781000e-305</td>\n",
       "      <td>-6.891000e-305</td>\n",
       "      <td>4.578000e-306</td>\n",
       "      <td>-5.457000e-306</td>\n",
       "      <td>3.904000e-306</td>\n",
       "      <td>-4.653000e-306</td>\n",
       "      <td>1.846000e-306</td>\n",
       "      <td>-2.201000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-2.889000e-305</td>\n",
       "      <td>3.443000e-305</td>\n",
       "      <td>-4.629000e-306</td>\n",
       "      <td>5.518000e-306</td>\n",
       "      <td>-4.397000e-306</td>\n",
       "      <td>5.241000e-306</td>\n",
       "      <td>-3.511000e-306</td>\n",
       "      <td>4.185000e-306</td>\n",
       "      <td>5.751000e-305</td>\n",
       "      <td>-6.855000e-305</td>\n",
       "      <td>4.372000e-306</td>\n",
       "      <td>-5.211000e-306</td>\n",
       "      <td>3.611000e-306</td>\n",
       "      <td>-4.305000e-306</td>\n",
       "      <td>1.959000e-306</td>\n",
       "      <td>-2.336000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>-2.816000e-305</td>\n",
       "      <td>3.357000e-305</td>\n",
       "      <td>-3.981000e-306</td>\n",
       "      <td>4.745000e-306</td>\n",
       "      <td>-4.038000e-306</td>\n",
       "      <td>4.814000e-306</td>\n",
       "      <td>-3.389000e-306</td>\n",
       "      <td>4.039000e-306</td>\n",
       "      <td>6.264000e-305</td>\n",
       "      <td>-7.466000e-305</td>\n",
       "      <td>4.382000e-306</td>\n",
       "      <td>-5.224000e-306</td>\n",
       "      <td>3.372000e-306</td>\n",
       "      <td>-4.019000e-306</td>\n",
       "      <td>1.476000e-306</td>\n",
       "      <td>-1.760000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-3.034000e-305</td>\n",
       "      <td>3.616000e-305</td>\n",
       "      <td>-3.769000e-306</td>\n",
       "      <td>4.493000e-306</td>\n",
       "      <td>-4.417000e-306</td>\n",
       "      <td>5.264000e-306</td>\n",
       "      <td>-3.315000e-306</td>\n",
       "      <td>3.952000e-306</td>\n",
       "      <td>6.293000e-305</td>\n",
       "      <td>-7.501000e-305</td>\n",
       "      <td>4.328000e-306</td>\n",
       "      <td>-5.159000e-306</td>\n",
       "      <td>3.911000e-306</td>\n",
       "      <td>-4.662000e-306</td>\n",
       "      <td>1.501000e-306</td>\n",
       "      <td>-1.790000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>-2.485000e-305</td>\n",
       "      <td>2.962000e-305</td>\n",
       "      <td>-4.116000e-306</td>\n",
       "      <td>4.906000e-306</td>\n",
       "      <td>-4.311000e-306</td>\n",
       "      <td>5.138000e-306</td>\n",
       "      <td>-3.056000e-306</td>\n",
       "      <td>3.643000e-306</td>\n",
       "      <td>5.568000e-305</td>\n",
       "      <td>-6.636000e-305</td>\n",
       "      <td>4.165000e-306</td>\n",
       "      <td>-4.965000e-306</td>\n",
       "      <td>3.611000e-306</td>\n",
       "      <td>-4.304000e-306</td>\n",
       "      <td>1.627000e-306</td>\n",
       "      <td>-1.940000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>-2.536000e-305</td>\n",
       "      <td>3.023000e-305</td>\n",
       "      <td>-4.095000e-306</td>\n",
       "      <td>4.881000e-306</td>\n",
       "      <td>-4.130000e-306</td>\n",
       "      <td>4.923000e-306</td>\n",
       "      <td>-3.339000e-306</td>\n",
       "      <td>3.980000e-306</td>\n",
       "      <td>5.815000e-305</td>\n",
       "      <td>-6.931000e-305</td>\n",
       "      <td>5.019000e-306</td>\n",
       "      <td>-5.982000e-306</td>\n",
       "      <td>3.215000e-306</td>\n",
       "      <td>-3.832000e-306</td>\n",
       "      <td>1.698000e-306</td>\n",
       "      <td>-2.023000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>-2.029000e-305</td>\n",
       "      <td>2.419000e-305</td>\n",
       "      <td>-3.427000e-306</td>\n",
       "      <td>4.085000e-306</td>\n",
       "      <td>-3.911000e-306</td>\n",
       "      <td>4.662000e-306</td>\n",
       "      <td>-3.039000e-306</td>\n",
       "      <td>3.622000e-306</td>\n",
       "      <td>5.078000e-305</td>\n",
       "      <td>-6.053000e-305</td>\n",
       "      <td>4.064000e-306</td>\n",
       "      <td>-4.844000e-306</td>\n",
       "      <td>3.347000e-306</td>\n",
       "      <td>-3.990000e-306</td>\n",
       "      <td>1.565000e-306</td>\n",
       "      <td>-1.866000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>-2.618000e-305</td>\n",
       "      <td>3.121000e-305</td>\n",
       "      <td>-3.072000e-306</td>\n",
       "      <td>3.662000e-306</td>\n",
       "      <td>-3.469000e-306</td>\n",
       "      <td>4.136000e-306</td>\n",
       "      <td>-3.090000e-306</td>\n",
       "      <td>3.683000e-306</td>\n",
       "      <td>4.812000e-305</td>\n",
       "      <td>-5.735000e-305</td>\n",
       "      <td>3.648000e-306</td>\n",
       "      <td>-4.349000e-306</td>\n",
       "      <td>3.161000e-306</td>\n",
       "      <td>-3.768000e-306</td>\n",
       "      <td>1.328000e-306</td>\n",
       "      <td>-1.583000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>-2.634000e-305</td>\n",
       "      <td>3.140000e-305</td>\n",
       "      <td>-3.259000e-306</td>\n",
       "      <td>3.885000e-306</td>\n",
       "      <td>-3.590000e-306</td>\n",
       "      <td>4.279000e-306</td>\n",
       "      <td>-2.856000e-306</td>\n",
       "      <td>3.404000e-306</td>\n",
       "      <td>4.955000e-305</td>\n",
       "      <td>-5.906000e-305</td>\n",
       "      <td>3.360000e-306</td>\n",
       "      <td>-4.005000e-306</td>\n",
       "      <td>2.755000e-306</td>\n",
       "      <td>-3.283000e-306</td>\n",
       "      <td>1.411000e-306</td>\n",
       "      <td>-1.682000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>-2.816000e-305</td>\n",
       "      <td>3.357000e-305</td>\n",
       "      <td>-2.989000e-306</td>\n",
       "      <td>3.563000e-306</td>\n",
       "      <td>-3.591000e-306</td>\n",
       "      <td>4.280000e-306</td>\n",
       "      <td>-2.647000e-306</td>\n",
       "      <td>3.155000e-306</td>\n",
       "      <td>4.466000e-305</td>\n",
       "      <td>-5.323000e-305</td>\n",
       "      <td>3.574000e-306</td>\n",
       "      <td>-4.261000e-306</td>\n",
       "      <td>3.172000e-306</td>\n",
       "      <td>-3.781000e-306</td>\n",
       "      <td>1.334000e-306</td>\n",
       "      <td>-1.590000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>-2.703000e-305</td>\n",
       "      <td>3.222000e-305</td>\n",
       "      <td>-3.461000e-306</td>\n",
       "      <td>4.126000e-306</td>\n",
       "      <td>-3.694000e-306</td>\n",
       "      <td>4.403000e-306</td>\n",
       "      <td>-2.949000e-306</td>\n",
       "      <td>3.515000e-306</td>\n",
       "      <td>5.087000e-305</td>\n",
       "      <td>-6.064000e-305</td>\n",
       "      <td>3.492000e-306</td>\n",
       "      <td>-4.162000e-306</td>\n",
       "      <td>2.795000e-306</td>\n",
       "      <td>-3.331000e-306</td>\n",
       "      <td>1.578000e-306</td>\n",
       "      <td>-1.881000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>-2.982000e-305</td>\n",
       "      <td>3.554000e-305</td>\n",
       "      <td>-3.309000e-306</td>\n",
       "      <td>3.944000e-306</td>\n",
       "      <td>-3.950000e-306</td>\n",
       "      <td>4.709000e-306</td>\n",
       "      <td>-3.024000e-306</td>\n",
       "      <td>3.605000e-306</td>\n",
       "      <td>5.476000e-305</td>\n",
       "      <td>-6.527000e-305</td>\n",
       "      <td>3.473000e-306</td>\n",
       "      <td>-4.140000e-306</td>\n",
       "      <td>3.110000e-306</td>\n",
       "      <td>-3.707000e-306</td>\n",
       "      <td>1.446000e-306</td>\n",
       "      <td>-1.724000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>-2.726000e-305</td>\n",
       "      <td>3.250000e-305</td>\n",
       "      <td>-3.910000e-306</td>\n",
       "      <td>4.661000e-306</td>\n",
       "      <td>-3.993000e-306</td>\n",
       "      <td>4.759000e-306</td>\n",
       "      <td>-3.171000e-306</td>\n",
       "      <td>3.780000e-306</td>\n",
       "      <td>5.569000e-305</td>\n",
       "      <td>-6.638000e-305</td>\n",
       "      <td>3.989000e-306</td>\n",
       "      <td>-4.755000e-306</td>\n",
       "      <td>3.163000e-306</td>\n",
       "      <td>-3.770000e-306</td>\n",
       "      <td>1.583000e-306</td>\n",
       "      <td>-1.887000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>-3.112000e-305</td>\n",
       "      <td>3.709000e-305</td>\n",
       "      <td>-4.601000e-306</td>\n",
       "      <td>5.485000e-306</td>\n",
       "      <td>-4.158000e-306</td>\n",
       "      <td>4.956000e-306</td>\n",
       "      <td>-3.275000e-306</td>\n",
       "      <td>3.904000e-306</td>\n",
       "      <td>5.714000e-305</td>\n",
       "      <td>-6.811000e-305</td>\n",
       "      <td>4.346000e-306</td>\n",
       "      <td>-5.180000e-306</td>\n",
       "      <td>3.027000e-306</td>\n",
       "      <td>-3.609000e-306</td>\n",
       "      <td>1.652000e-306</td>\n",
       "      <td>-1.969000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>-2.860000e-305</td>\n",
       "      <td>3.409000e-305</td>\n",
       "      <td>-4.241000e-306</td>\n",
       "      <td>5.056000e-306</td>\n",
       "      <td>-4.624000e-306</td>\n",
       "      <td>5.511000e-306</td>\n",
       "      <td>-3.714000e-306</td>\n",
       "      <td>4.427000e-306</td>\n",
       "      <td>5.988000e-305</td>\n",
       "      <td>-7.138000e-305</td>\n",
       "      <td>4.666000e-306</td>\n",
       "      <td>-5.561000e-306</td>\n",
       "      <td>3.811000e-306</td>\n",
       "      <td>-4.542000e-306</td>\n",
       "      <td>1.465000e-306</td>\n",
       "      <td>-1.747000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>-2.848000e-305</td>\n",
       "      <td>3.395000e-305</td>\n",
       "      <td>-4.471000e-306</td>\n",
       "      <td>5.329000e-306</td>\n",
       "      <td>-4.251000e-306</td>\n",
       "      <td>5.067000e-306</td>\n",
       "      <td>-3.472000e-306</td>\n",
       "      <td>4.139000e-306</td>\n",
       "      <td>6.074000e-305</td>\n",
       "      <td>-7.240000e-305</td>\n",
       "      <td>4.696000e-306</td>\n",
       "      <td>-5.598000e-306</td>\n",
       "      <td>3.842000e-306</td>\n",
       "      <td>-4.579000e-306</td>\n",
       "      <td>1.466000e-306</td>\n",
       "      <td>-1.748000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>-2.363000e-305</td>\n",
       "      <td>2.816000e-305</td>\n",
       "      <td>-4.586000e-306</td>\n",
       "      <td>5.467000e-306</td>\n",
       "      <td>-4.740000e-306</td>\n",
       "      <td>5.649000e-306</td>\n",
       "      <td>-4.016000e-306</td>\n",
       "      <td>4.787000e-306</td>\n",
       "      <td>6.225000e-305</td>\n",
       "      <td>-7.420000e-305</td>\n",
       "      <td>4.427000e-306</td>\n",
       "      <td>-5.277000e-306</td>\n",
       "      <td>4.125000e-306</td>\n",
       "      <td>-4.917000e-306</td>\n",
       "      <td>1.620000e-306</td>\n",
       "      <td>-1.930000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>-2.683000e-305</td>\n",
       "      <td>3.198000e-305</td>\n",
       "      <td>-4.445000e-306</td>\n",
       "      <td>5.299000e-306</td>\n",
       "      <td>-4.759000e-306</td>\n",
       "      <td>5.672000e-306</td>\n",
       "      <td>-3.374000e-306</td>\n",
       "      <td>4.022000e-306</td>\n",
       "      <td>6.005000e-305</td>\n",
       "      <td>-7.157000e-305</td>\n",
       "      <td>4.751000e-306</td>\n",
       "      <td>-5.663000e-306</td>\n",
       "      <td>3.982000e-306</td>\n",
       "      <td>-4.747000e-306</td>\n",
       "      <td>1.701000e-306</td>\n",
       "      <td>-2.028000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>-3.186000e-305</td>\n",
       "      <td>3.797000e-305</td>\n",
       "      <td>-4.319000e-306</td>\n",
       "      <td>5.148000e-306</td>\n",
       "      <td>-4.940000e-306</td>\n",
       "      <td>5.888000e-306</td>\n",
       "      <td>-3.776000e-306</td>\n",
       "      <td>4.500000e-306</td>\n",
       "      <td>6.025000e-305</td>\n",
       "      <td>-7.182000e-305</td>\n",
       "      <td>4.495000e-306</td>\n",
       "      <td>-5.358000e-306</td>\n",
       "      <td>3.947000e-306</td>\n",
       "      <td>-4.705000e-306</td>\n",
       "      <td>1.912000e-306</td>\n",
       "      <td>-2.280000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>-3.066000e-305</td>\n",
       "      <td>3.654000e-305</td>\n",
       "      <td>-4.567000e-306</td>\n",
       "      <td>5.443000e-306</td>\n",
       "      <td>-4.923000e-306</td>\n",
       "      <td>5.868000e-306</td>\n",
       "      <td>-3.803000e-306</td>\n",
       "      <td>4.533000e-306</td>\n",
       "      <td>5.997000e-305</td>\n",
       "      <td>-7.148000e-305</td>\n",
       "      <td>4.569000e-306</td>\n",
       "      <td>-5.446000e-306</td>\n",
       "      <td>4.022000e-306</td>\n",
       "      <td>-4.794000e-306</td>\n",
       "      <td>1.917000e-306</td>\n",
       "      <td>-2.285000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>-2.799000e-305</td>\n",
       "      <td>3.337000e-305</td>\n",
       "      <td>-4.486000e-306</td>\n",
       "      <td>5.347000e-306</td>\n",
       "      <td>-4.563000e-306</td>\n",
       "      <td>5.439000e-306</td>\n",
       "      <td>-3.906000e-306</td>\n",
       "      <td>4.655000e-306</td>\n",
       "      <td>5.621000e-305</td>\n",
       "      <td>-6.700000e-305</td>\n",
       "      <td>4.132000e-306</td>\n",
       "      <td>-4.925000e-306</td>\n",
       "      <td>4.069000e-306</td>\n",
       "      <td>-4.851000e-306</td>\n",
       "      <td>2.159000e-306</td>\n",
       "      <td>-2.573000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>-2.965000e-305</td>\n",
       "      <td>3.535000e-305</td>\n",
       "      <td>-4.150000e-306</td>\n",
       "      <td>4.946000e-306</td>\n",
       "      <td>-4.621000e-306</td>\n",
       "      <td>5.508000e-306</td>\n",
       "      <td>-3.704000e-306</td>\n",
       "      <td>4.415000e-306</td>\n",
       "      <td>5.584000e-305</td>\n",
       "      <td>-6.656000e-305</td>\n",
       "      <td>4.415000e-306</td>\n",
       "      <td>-5.262000e-306</td>\n",
       "      <td>3.971000e-306</td>\n",
       "      <td>-4.734000e-306</td>\n",
       "      <td>2.002000e-306</td>\n",
       "      <td>-2.387000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>-3.161000e-305</td>\n",
       "      <td>3.768000e-305</td>\n",
       "      <td>-4.631000e-306</td>\n",
       "      <td>5.520000e-306</td>\n",
       "      <td>-3.914000e-306</td>\n",
       "      <td>4.666000e-306</td>\n",
       "      <td>-3.508000e-306</td>\n",
       "      <td>4.181000e-306</td>\n",
       "      <td>5.776000e-305</td>\n",
       "      <td>-6.885000e-305</td>\n",
       "      <td>4.713000e-306</td>\n",
       "      <td>-5.617000e-306</td>\n",
       "      <td>3.479000e-306</td>\n",
       "      <td>-4.147000e-306</td>\n",
       "      <td>1.687000e-306</td>\n",
       "      <td>-2.011000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>-3.083000e-305</td>\n",
       "      <td>3.675000e-305</td>\n",
       "      <td>-4.347000e-306</td>\n",
       "      <td>5.181000e-306</td>\n",
       "      <td>-4.322000e-306</td>\n",
       "      <td>5.151000e-306</td>\n",
       "      <td>-3.225000e-306</td>\n",
       "      <td>3.844000e-306</td>\n",
       "      <td>6.146000e-305</td>\n",
       "      <td>-7.325000e-305</td>\n",
       "      <td>4.402000e-306</td>\n",
       "      <td>-5.247000e-306</td>\n",
       "      <td>3.757000e-306</td>\n",
       "      <td>-4.479000e-306</td>\n",
       "      <td>1.783000e-306</td>\n",
       "      <td>-2.126000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>-3.171000e-305</td>\n",
       "      <td>3.780000e-305</td>\n",
       "      <td>-4.629000e-306</td>\n",
       "      <td>5.517000e-306</td>\n",
       "      <td>-4.569000e-306</td>\n",
       "      <td>5.446000e-306</td>\n",
       "      <td>-3.133000e-306</td>\n",
       "      <td>3.735000e-306</td>\n",
       "      <td>6.133000e-305</td>\n",
       "      <td>-7.310000e-305</td>\n",
       "      <td>4.457000e-306</td>\n",
       "      <td>-5.313000e-306</td>\n",
       "      <td>3.664000e-306</td>\n",
       "      <td>-4.367000e-306</td>\n",
       "      <td>1.752000e-306</td>\n",
       "      <td>-2.088000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>-3.095000e-305</td>\n",
       "      <td>3.689000e-305</td>\n",
       "      <td>-4.811000e-306</td>\n",
       "      <td>5.734000e-306</td>\n",
       "      <td>-4.901000e-306</td>\n",
       "      <td>5.841000e-306</td>\n",
       "      <td>-3.494000e-306</td>\n",
       "      <td>4.164000e-306</td>\n",
       "      <td>6.355000e-305</td>\n",
       "      <td>-7.575000e-305</td>\n",
       "      <td>4.698000e-306</td>\n",
       "      <td>-5.600000e-306</td>\n",
       "      <td>3.558000e-306</td>\n",
       "      <td>-4.241000e-306</td>\n",
       "      <td>1.763000e-306</td>\n",
       "      <td>-2.102000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>-2.961000e-305</td>\n",
       "      <td>3.529000e-305</td>\n",
       "      <td>-4.543000e-306</td>\n",
       "      <td>5.415000e-306</td>\n",
       "      <td>-4.773000e-306</td>\n",
       "      <td>5.689000e-306</td>\n",
       "      <td>-3.524000e-306</td>\n",
       "      <td>4.200000e-306</td>\n",
       "      <td>6.070000e-305</td>\n",
       "      <td>-7.236000e-305</td>\n",
       "      <td>4.727000e-306</td>\n",
       "      <td>-5.635000e-306</td>\n",
       "      <td>3.760000e-306</td>\n",
       "      <td>-4.482000e-306</td>\n",
       "      <td>1.755000e-306</td>\n",
       "      <td>-2.092000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>-3.063000e-305</td>\n",
       "      <td>3.651000e-305</td>\n",
       "      <td>-5.038000e-306</td>\n",
       "      <td>6.005000e-306</td>\n",
       "      <td>-4.805000e-306</td>\n",
       "      <td>5.728000e-306</td>\n",
       "      <td>-3.722000e-306</td>\n",
       "      <td>4.436000e-306</td>\n",
       "      <td>6.279000e-305</td>\n",
       "      <td>-7.485000e-305</td>\n",
       "      <td>4.975000e-306</td>\n",
       "      <td>-5.930000e-306</td>\n",
       "      <td>4.260000e-306</td>\n",
       "      <td>-5.077000e-306</td>\n",
       "      <td>1.704000e-306</td>\n",
       "      <td>-2.031000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>-2.776000e-305</td>\n",
       "      <td>3.309000e-305</td>\n",
       "      <td>-4.270000e-306</td>\n",
       "      <td>5.090000e-306</td>\n",
       "      <td>-5.076000e-306</td>\n",
       "      <td>6.051000e-306</td>\n",
       "      <td>-3.976000e-306</td>\n",
       "      <td>4.739000e-306</td>\n",
       "      <td>6.068000e-305</td>\n",
       "      <td>-7.233000e-305</td>\n",
       "      <td>4.781000e-306</td>\n",
       "      <td>-5.699000e-306</td>\n",
       "      <td>4.027000e-306</td>\n",
       "      <td>-4.800000e-306</td>\n",
       "      <td>1.643000e-306</td>\n",
       "      <td>-1.959000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>-2.650000e-305</td>\n",
       "      <td>3.158000e-305</td>\n",
       "      <td>-4.355000e-306</td>\n",
       "      <td>5.191000e-306</td>\n",
       "      <td>-4.887000e-306</td>\n",
       "      <td>5.825000e-306</td>\n",
       "      <td>-3.999000e-306</td>\n",
       "      <td>4.767000e-306</td>\n",
       "      <td>6.175000e-305</td>\n",
       "      <td>-7.361000e-305</td>\n",
       "      <td>4.969000e-306</td>\n",
       "      <td>-5.923000e-306</td>\n",
       "      <td>4.108000e-306</td>\n",
       "      <td>-4.897000e-306</td>\n",
       "      <td>1.913000e-306</td>\n",
       "      <td>-2.280000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>-3.403000e-305</td>\n",
       "      <td>4.057000e-305</td>\n",
       "      <td>-4.584000e-306</td>\n",
       "      <td>5.463000e-306</td>\n",
       "      <td>-4.400000e-306</td>\n",
       "      <td>5.245000e-306</td>\n",
       "      <td>-4.059000e-306</td>\n",
       "      <td>4.839000e-306</td>\n",
       "      <td>5.777000e-305</td>\n",
       "      <td>-6.886000e-305</td>\n",
       "      <td>4.764000e-306</td>\n",
       "      <td>-5.679000e-306</td>\n",
       "      <td>3.889000e-306</td>\n",
       "      <td>-4.636000e-306</td>\n",
       "      <td>1.905000e-306</td>\n",
       "      <td>-2.271000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>-2.971000e-305</td>\n",
       "      <td>3.541000e-305</td>\n",
       "      <td>-4.676000e-306</td>\n",
       "      <td>5.574000e-306</td>\n",
       "      <td>-5.092000e-306</td>\n",
       "      <td>6.069000e-306</td>\n",
       "      <td>-3.769000e-306</td>\n",
       "      <td>4.493000e-306</td>\n",
       "      <td>5.772000e-305</td>\n",
       "      <td>-6.880000e-305</td>\n",
       "      <td>4.684000e-306</td>\n",
       "      <td>-5.583000e-306</td>\n",
       "      <td>4.214000e-306</td>\n",
       "      <td>-5.024000e-306</td>\n",
       "      <td>1.664000e-306</td>\n",
       "      <td>-1.983000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>-2.854000e-305</td>\n",
       "      <td>3.402000e-305</td>\n",
       "      <td>-3.925000e-306</td>\n",
       "      <td>4.678000e-306</td>\n",
       "      <td>-4.863000e-306</td>\n",
       "      <td>5.797000e-306</td>\n",
       "      <td>-3.810000e-306</td>\n",
       "      <td>4.541000e-306</td>\n",
       "      <td>5.842000e-305</td>\n",
       "      <td>-6.964000e-305</td>\n",
       "      <td>4.682000e-306</td>\n",
       "      <td>-5.581000e-306</td>\n",
       "      <td>4.249000e-306</td>\n",
       "      <td>-5.064000e-306</td>\n",
       "      <td>2.110000e-306</td>\n",
       "      <td>-2.515000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>-3.204000e-305</td>\n",
       "      <td>3.819000e-305</td>\n",
       "      <td>-4.274000e-306</td>\n",
       "      <td>5.095000e-306</td>\n",
       "      <td>-4.603000e-306</td>\n",
       "      <td>5.486000e-306</td>\n",
       "      <td>-4.054000e-306</td>\n",
       "      <td>4.832000e-306</td>\n",
       "      <td>5.936000e-305</td>\n",
       "      <td>-7.076000e-305</td>\n",
       "      <td>4.407000e-306</td>\n",
       "      <td>-5.253000e-306</td>\n",
       "      <td>4.306000e-306</td>\n",
       "      <td>-5.133000e-306</td>\n",
       "      <td>1.855000e-306</td>\n",
       "      <td>-2.212000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>-3.013000e-305</td>\n",
       "      <td>3.591000e-305</td>\n",
       "      <td>-4.289000e-306</td>\n",
       "      <td>5.112000e-306</td>\n",
       "      <td>-4.674000e-306</td>\n",
       "      <td>5.571000e-306</td>\n",
       "      <td>-3.823000e-306</td>\n",
       "      <td>4.557000e-306</td>\n",
       "      <td>5.945000e-305</td>\n",
       "      <td>-7.087000e-305</td>\n",
       "      <td>4.532000e-306</td>\n",
       "      <td>-5.402000e-306</td>\n",
       "      <td>4.317000e-306</td>\n",
       "      <td>-5.146000e-306</td>\n",
       "      <td>2.134000e-306</td>\n",
       "      <td>-2.544000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>-3.249000e-305</td>\n",
       "      <td>3.873000e-305</td>\n",
       "      <td>-4.464000e-306</td>\n",
       "      <td>5.321000e-306</td>\n",
       "      <td>-4.338000e-306</td>\n",
       "      <td>5.171000e-306</td>\n",
       "      <td>-3.742000e-306</td>\n",
       "      <td>4.461000e-306</td>\n",
       "      <td>5.874000e-305</td>\n",
       "      <td>-7.002000e-305</td>\n",
       "      <td>4.850000e-306</td>\n",
       "      <td>-5.781000e-306</td>\n",
       "      <td>3.653000e-306</td>\n",
       "      <td>-4.354000e-306</td>\n",
       "      <td>2.031000e-306</td>\n",
       "      <td>-2.421000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>-2.804000e-305</td>\n",
       "      <td>3.342000e-305</td>\n",
       "      <td>-4.474000e-306</td>\n",
       "      <td>5.333000e-306</td>\n",
       "      <td>-4.854000e-306</td>\n",
       "      <td>5.786000e-306</td>\n",
       "      <td>-3.321000e-306</td>\n",
       "      <td>3.959000e-306</td>\n",
       "      <td>5.945000e-305</td>\n",
       "      <td>-7.086000e-305</td>\n",
       "      <td>4.585000e-306</td>\n",
       "      <td>-5.465000e-306</td>\n",
       "      <td>3.665000e-306</td>\n",
       "      <td>-4.369000e-306</td>\n",
       "      <td>2.034000e-306</td>\n",
       "      <td>-2.425000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>-3.084000e-305</td>\n",
       "      <td>3.676000e-305</td>\n",
       "      <td>-4.154000e-306</td>\n",
       "      <td>4.951000e-306</td>\n",
       "      <td>-4.320000e-306</td>\n",
       "      <td>5.150000e-306</td>\n",
       "      <td>-3.665000e-306</td>\n",
       "      <td>4.369000e-306</td>\n",
       "      <td>6.276000e-305</td>\n",
       "      <td>-7.481000e-305</td>\n",
       "      <td>4.600000e-306</td>\n",
       "      <td>-5.484000e-306</td>\n",
       "      <td>3.230000e-306</td>\n",
       "      <td>-3.850000e-306</td>\n",
       "      <td>1.751000e-306</td>\n",
       "      <td>-2.087000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>-2.871000e-305</td>\n",
       "      <td>3.422000e-305</td>\n",
       "      <td>-5.076000e-306</td>\n",
       "      <td>6.051000e-306</td>\n",
       "      <td>-4.500000e-306</td>\n",
       "      <td>5.364000e-306</td>\n",
       "      <td>-3.675000e-306</td>\n",
       "      <td>4.380000e-306</td>\n",
       "      <td>6.199000e-305</td>\n",
       "      <td>-7.389000e-305</td>\n",
       "      <td>4.596000e-306</td>\n",
       "      <td>-5.478000e-306</td>\n",
       "      <td>3.954000e-306</td>\n",
       "      <td>-4.714000e-306</td>\n",
       "      <td>1.734000e-306</td>\n",
       "      <td>-2.067000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>-3.258000e-305</td>\n",
       "      <td>3.883000e-305</td>\n",
       "      <td>-4.731000e-306</td>\n",
       "      <td>5.639000e-306</td>\n",
       "      <td>-5.029000e-306</td>\n",
       "      <td>5.995000e-306</td>\n",
       "      <td>-3.550000e-306</td>\n",
       "      <td>4.231000e-306</td>\n",
       "      <td>5.910000e-305</td>\n",
       "      <td>-7.045000e-305</td>\n",
       "      <td>4.979000e-306</td>\n",
       "      <td>-5.935000e-306</td>\n",
       "      <td>4.045000e-306</td>\n",
       "      <td>-4.821000e-306</td>\n",
       "      <td>1.846000e-306</td>\n",
       "      <td>-2.201000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>-2.945000e-305</td>\n",
       "      <td>3.510000e-305</td>\n",
       "      <td>-4.856000e-306</td>\n",
       "      <td>5.789000e-306</td>\n",
       "      <td>-5.003000e-306</td>\n",
       "      <td>5.963000e-306</td>\n",
       "      <td>-3.719000e-306</td>\n",
       "      <td>4.433000e-306</td>\n",
       "      <td>6.263000e-305</td>\n",
       "      <td>-7.465000e-305</td>\n",
       "      <td>5.479000e-306</td>\n",
       "      <td>-6.530000e-306</td>\n",
       "      <td>4.131000e-306</td>\n",
       "      <td>-4.924000e-306</td>\n",
       "      <td>1.883000e-306</td>\n",
       "      <td>-2.245000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>-2.937000e-305</td>\n",
       "      <td>3.500000e-305</td>\n",
       "      <td>-4.626000e-306</td>\n",
       "      <td>5.514000e-306</td>\n",
       "      <td>-4.848000e-306</td>\n",
       "      <td>5.779000e-306</td>\n",
       "      <td>-4.040000e-306</td>\n",
       "      <td>4.816000e-306</td>\n",
       "      <td>6.043000e-305</td>\n",
       "      <td>-7.203000e-305</td>\n",
       "      <td>5.117000e-306</td>\n",
       "      <td>-6.100000e-306</td>\n",
       "      <td>4.223000e-306</td>\n",
       "      <td>-5.034000e-306</td>\n",
       "      <td>1.893000e-306</td>\n",
       "      <td>-2.257000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>-2.886000e-305</td>\n",
       "      <td>3.440000e-305</td>\n",
       "      <td>-4.518000e-306</td>\n",
       "      <td>5.385000e-306</td>\n",
       "      <td>-5.050000e-306</td>\n",
       "      <td>6.020000e-306</td>\n",
       "      <td>-4.113000e-306</td>\n",
       "      <td>4.903000e-306</td>\n",
       "      <td>6.082000e-305</td>\n",
       "      <td>-7.249000e-305</td>\n",
       "      <td>5.299000e-306</td>\n",
       "      <td>-6.316000e-306</td>\n",
       "      <td>3.941000e-306</td>\n",
       "      <td>-4.698000e-306</td>\n",
       "      <td>1.943000e-306</td>\n",
       "      <td>-2.316000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>-2.917000e-305</td>\n",
       "      <td>3.477000e-305</td>\n",
       "      <td>-3.800000e-306</td>\n",
       "      <td>4.530000e-306</td>\n",
       "      <td>-4.590000e-306</td>\n",
       "      <td>5.471000e-306</td>\n",
       "      <td>-3.858000e-306</td>\n",
       "      <td>4.599000e-306</td>\n",
       "      <td>5.936000e-305</td>\n",
       "      <td>-7.075000e-305</td>\n",
       "      <td>4.810000e-306</td>\n",
       "      <td>-5.734000e-306</td>\n",
       "      <td>4.568000e-306</td>\n",
       "      <td>-5.445000e-306</td>\n",
       "      <td>1.869000e-306</td>\n",
       "      <td>-2.228000e-306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Timestamp           Oxy1         DeOxy1           Oxy2         DeOxy2  \\\n",
       "0           1 -2.590000e-305  3.087000e-305 -4.040000e-306  4.816000e-306   \n",
       "1           2 -2.315000e-305  2.759000e-305 -3.601000e-306  4.292000e-306   \n",
       "2           3 -2.571000e-305  3.065000e-305 -3.766000e-306  4.489000e-306   \n",
       "3           4 -3.335000e-305  3.975000e-305 -4.188000e-306  4.992000e-306   \n",
       "4           5 -2.897000e-305  3.453000e-305 -4.086000e-306  4.870000e-306   \n",
       "5           6 -3.190000e-305  3.802000e-305 -4.075000e-306  4.857000e-306   \n",
       "6           7 -2.973000e-305  3.544000e-305 -3.587000e-306  4.275000e-306   \n",
       "7           8 -2.889000e-305  3.443000e-305 -4.629000e-306  5.518000e-306   \n",
       "8           9 -2.816000e-305  3.357000e-305 -3.981000e-306  4.745000e-306   \n",
       "9          10 -3.034000e-305  3.616000e-305 -3.769000e-306  4.493000e-306   \n",
       "10         11 -2.485000e-305  2.962000e-305 -4.116000e-306  4.906000e-306   \n",
       "11         12 -2.536000e-305  3.023000e-305 -4.095000e-306  4.881000e-306   \n",
       "12         13 -2.029000e-305  2.419000e-305 -3.427000e-306  4.085000e-306   \n",
       "13         14 -2.618000e-305  3.121000e-305 -3.072000e-306  3.662000e-306   \n",
       "14         15 -2.634000e-305  3.140000e-305 -3.259000e-306  3.885000e-306   \n",
       "15         16 -2.816000e-305  3.357000e-305 -2.989000e-306  3.563000e-306   \n",
       "16         17 -2.703000e-305  3.222000e-305 -3.461000e-306  4.126000e-306   \n",
       "17         18 -2.982000e-305  3.554000e-305 -3.309000e-306  3.944000e-306   \n",
       "18         19 -2.726000e-305  3.250000e-305 -3.910000e-306  4.661000e-306   \n",
       "19         20 -3.112000e-305  3.709000e-305 -4.601000e-306  5.485000e-306   \n",
       "20         21 -2.860000e-305  3.409000e-305 -4.241000e-306  5.056000e-306   \n",
       "21         22 -2.848000e-305  3.395000e-305 -4.471000e-306  5.329000e-306   \n",
       "22         23 -2.363000e-305  2.816000e-305 -4.586000e-306  5.467000e-306   \n",
       "23         24 -2.683000e-305  3.198000e-305 -4.445000e-306  5.299000e-306   \n",
       "24         25 -3.186000e-305  3.797000e-305 -4.319000e-306  5.148000e-306   \n",
       "25         26 -3.066000e-305  3.654000e-305 -4.567000e-306  5.443000e-306   \n",
       "26         27 -2.799000e-305  3.337000e-305 -4.486000e-306  5.347000e-306   \n",
       "27         28 -2.965000e-305  3.535000e-305 -4.150000e-306  4.946000e-306   \n",
       "28         29 -3.161000e-305  3.768000e-305 -4.631000e-306  5.520000e-306   \n",
       "29         30 -3.083000e-305  3.675000e-305 -4.347000e-306  5.181000e-306   \n",
       "30         31 -3.171000e-305  3.780000e-305 -4.629000e-306  5.517000e-306   \n",
       "31         32 -3.095000e-305  3.689000e-305 -4.811000e-306  5.734000e-306   \n",
       "32         33 -2.961000e-305  3.529000e-305 -4.543000e-306  5.415000e-306   \n",
       "33         34 -3.063000e-305  3.651000e-305 -5.038000e-306  6.005000e-306   \n",
       "34         35 -2.776000e-305  3.309000e-305 -4.270000e-306  5.090000e-306   \n",
       "35         36 -2.650000e-305  3.158000e-305 -4.355000e-306  5.191000e-306   \n",
       "36         37 -3.403000e-305  4.057000e-305 -4.584000e-306  5.463000e-306   \n",
       "37         38 -2.971000e-305  3.541000e-305 -4.676000e-306  5.574000e-306   \n",
       "38         39 -2.854000e-305  3.402000e-305 -3.925000e-306  4.678000e-306   \n",
       "39         40 -3.204000e-305  3.819000e-305 -4.274000e-306  5.095000e-306   \n",
       "40         41 -3.013000e-305  3.591000e-305 -4.289000e-306  5.112000e-306   \n",
       "41         42 -3.249000e-305  3.873000e-305 -4.464000e-306  5.321000e-306   \n",
       "42         43 -2.804000e-305  3.342000e-305 -4.474000e-306  5.333000e-306   \n",
       "43         44 -3.084000e-305  3.676000e-305 -4.154000e-306  4.951000e-306   \n",
       "44         45 -2.871000e-305  3.422000e-305 -5.076000e-306  6.051000e-306   \n",
       "45         46 -3.258000e-305  3.883000e-305 -4.731000e-306  5.639000e-306   \n",
       "46         47 -2.945000e-305  3.510000e-305 -4.856000e-306  5.789000e-306   \n",
       "47         48 -2.937000e-305  3.500000e-305 -4.626000e-306  5.514000e-306   \n",
       "48         49 -2.886000e-305  3.440000e-305 -4.518000e-306  5.385000e-306   \n",
       "49         50 -2.917000e-305  3.477000e-305 -3.800000e-306  4.530000e-306   \n",
       "\n",
       "             Oxy3         DeOxy3           Oxy4         DeOxy4           Oxy5  \\\n",
       "0  -4.747000e-306  5.658000e-306 -3.440000e-306  4.100000e-306  5.880000e-305   \n",
       "1  -4.902000e-306  5.843000e-306 -3.444000e-306  4.106000e-306  6.224000e-305   \n",
       "2  -4.511000e-306  5.376000e-306 -3.847000e-306  4.586000e-306  5.882000e-305   \n",
       "3  -4.373000e-306  5.213000e-306 -3.726000e-306  4.442000e-306  5.547000e-305   \n",
       "4  -3.768000e-306  4.491000e-306 -3.724000e-306  4.439000e-306  5.415000e-305   \n",
       "5  -4.281000e-306  5.103000e-306 -3.555000e-306  4.237000e-306  5.650000e-305   \n",
       "6  -4.187000e-306  4.991000e-306 -3.385000e-306  4.035000e-306  5.781000e-305   \n",
       "7  -4.397000e-306  5.241000e-306 -3.511000e-306  4.185000e-306  5.751000e-305   \n",
       "8  -4.038000e-306  4.814000e-306 -3.389000e-306  4.039000e-306  6.264000e-305   \n",
       "9  -4.417000e-306  5.264000e-306 -3.315000e-306  3.952000e-306  6.293000e-305   \n",
       "10 -4.311000e-306  5.138000e-306 -3.056000e-306  3.643000e-306  5.568000e-305   \n",
       "11 -4.130000e-306  4.923000e-306 -3.339000e-306  3.980000e-306  5.815000e-305   \n",
       "12 -3.911000e-306  4.662000e-306 -3.039000e-306  3.622000e-306  5.078000e-305   \n",
       "13 -3.469000e-306  4.136000e-306 -3.090000e-306  3.683000e-306  4.812000e-305   \n",
       "14 -3.590000e-306  4.279000e-306 -2.856000e-306  3.404000e-306  4.955000e-305   \n",
       "15 -3.591000e-306  4.280000e-306 -2.647000e-306  3.155000e-306  4.466000e-305   \n",
       "16 -3.694000e-306  4.403000e-306 -2.949000e-306  3.515000e-306  5.087000e-305   \n",
       "17 -3.950000e-306  4.709000e-306 -3.024000e-306  3.605000e-306  5.476000e-305   \n",
       "18 -3.993000e-306  4.759000e-306 -3.171000e-306  3.780000e-306  5.569000e-305   \n",
       "19 -4.158000e-306  4.956000e-306 -3.275000e-306  3.904000e-306  5.714000e-305   \n",
       "20 -4.624000e-306  5.511000e-306 -3.714000e-306  4.427000e-306  5.988000e-305   \n",
       "21 -4.251000e-306  5.067000e-306 -3.472000e-306  4.139000e-306  6.074000e-305   \n",
       "22 -4.740000e-306  5.649000e-306 -4.016000e-306  4.787000e-306  6.225000e-305   \n",
       "23 -4.759000e-306  5.672000e-306 -3.374000e-306  4.022000e-306  6.005000e-305   \n",
       "24 -4.940000e-306  5.888000e-306 -3.776000e-306  4.500000e-306  6.025000e-305   \n",
       "25 -4.923000e-306  5.868000e-306 -3.803000e-306  4.533000e-306  5.997000e-305   \n",
       "26 -4.563000e-306  5.439000e-306 -3.906000e-306  4.655000e-306  5.621000e-305   \n",
       "27 -4.621000e-306  5.508000e-306 -3.704000e-306  4.415000e-306  5.584000e-305   \n",
       "28 -3.914000e-306  4.666000e-306 -3.508000e-306  4.181000e-306  5.776000e-305   \n",
       "29 -4.322000e-306  5.151000e-306 -3.225000e-306  3.844000e-306  6.146000e-305   \n",
       "30 -4.569000e-306  5.446000e-306 -3.133000e-306  3.735000e-306  6.133000e-305   \n",
       "31 -4.901000e-306  5.841000e-306 -3.494000e-306  4.164000e-306  6.355000e-305   \n",
       "32 -4.773000e-306  5.689000e-306 -3.524000e-306  4.200000e-306  6.070000e-305   \n",
       "33 -4.805000e-306  5.728000e-306 -3.722000e-306  4.436000e-306  6.279000e-305   \n",
       "34 -5.076000e-306  6.051000e-306 -3.976000e-306  4.739000e-306  6.068000e-305   \n",
       "35 -4.887000e-306  5.825000e-306 -3.999000e-306  4.767000e-306  6.175000e-305   \n",
       "36 -4.400000e-306  5.245000e-306 -4.059000e-306  4.839000e-306  5.777000e-305   \n",
       "37 -5.092000e-306  6.069000e-306 -3.769000e-306  4.493000e-306  5.772000e-305   \n",
       "38 -4.863000e-306  5.797000e-306 -3.810000e-306  4.541000e-306  5.842000e-305   \n",
       "39 -4.603000e-306  5.486000e-306 -4.054000e-306  4.832000e-306  5.936000e-305   \n",
       "40 -4.674000e-306  5.571000e-306 -3.823000e-306  4.557000e-306  5.945000e-305   \n",
       "41 -4.338000e-306  5.171000e-306 -3.742000e-306  4.461000e-306  5.874000e-305   \n",
       "42 -4.854000e-306  5.786000e-306 -3.321000e-306  3.959000e-306  5.945000e-305   \n",
       "43 -4.320000e-306  5.150000e-306 -3.665000e-306  4.369000e-306  6.276000e-305   \n",
       "44 -4.500000e-306  5.364000e-306 -3.675000e-306  4.380000e-306  6.199000e-305   \n",
       "45 -5.029000e-306  5.995000e-306 -3.550000e-306  4.231000e-306  5.910000e-305   \n",
       "46 -5.003000e-306  5.963000e-306 -3.719000e-306  4.433000e-306  6.263000e-305   \n",
       "47 -4.848000e-306  5.779000e-306 -4.040000e-306  4.816000e-306  6.043000e-305   \n",
       "48 -5.050000e-306  6.020000e-306 -4.113000e-306  4.903000e-306  6.082000e-305   \n",
       "49 -4.590000e-306  5.471000e-306 -3.858000e-306  4.599000e-306  5.936000e-305   \n",
       "\n",
       "           DeOxy5           Oxy6         DeOxy6           Oxy7         DeOxy7  \\\n",
       "0  -7.009000e-305  4.894000e-306 -5.833000e-306  3.826000e-306 -4.561000e-306   \n",
       "1  -7.418000e-305  4.948000e-306 -5.898000e-306  3.715000e-306 -4.428000e-306   \n",
       "2  -7.011000e-305  4.274000e-306 -5.095000e-306  4.150000e-306 -4.947000e-306   \n",
       "3  -6.612000e-305  4.184000e-306 -4.987000e-306  3.722000e-306 -4.436000e-306   \n",
       "4  -6.454000e-305  3.878000e-306 -4.622000e-306  3.749000e-306 -4.469000e-306   \n",
       "5  -6.734000e-305  4.038000e-306 -4.813000e-306  3.536000e-306 -4.214000e-306   \n",
       "6  -6.891000e-305  4.578000e-306 -5.457000e-306  3.904000e-306 -4.653000e-306   \n",
       "7  -6.855000e-305  4.372000e-306 -5.211000e-306  3.611000e-306 -4.305000e-306   \n",
       "8  -7.466000e-305  4.382000e-306 -5.224000e-306  3.372000e-306 -4.019000e-306   \n",
       "9  -7.501000e-305  4.328000e-306 -5.159000e-306  3.911000e-306 -4.662000e-306   \n",
       "10 -6.636000e-305  4.165000e-306 -4.965000e-306  3.611000e-306 -4.304000e-306   \n",
       "11 -6.931000e-305  5.019000e-306 -5.982000e-306  3.215000e-306 -3.832000e-306   \n",
       "12 -6.053000e-305  4.064000e-306 -4.844000e-306  3.347000e-306 -3.990000e-306   \n",
       "13 -5.735000e-305  3.648000e-306 -4.349000e-306  3.161000e-306 -3.768000e-306   \n",
       "14 -5.906000e-305  3.360000e-306 -4.005000e-306  2.755000e-306 -3.283000e-306   \n",
       "15 -5.323000e-305  3.574000e-306 -4.261000e-306  3.172000e-306 -3.781000e-306   \n",
       "16 -6.064000e-305  3.492000e-306 -4.162000e-306  2.795000e-306 -3.331000e-306   \n",
       "17 -6.527000e-305  3.473000e-306 -4.140000e-306  3.110000e-306 -3.707000e-306   \n",
       "18 -6.638000e-305  3.989000e-306 -4.755000e-306  3.163000e-306 -3.770000e-306   \n",
       "19 -6.811000e-305  4.346000e-306 -5.180000e-306  3.027000e-306 -3.609000e-306   \n",
       "20 -7.138000e-305  4.666000e-306 -5.561000e-306  3.811000e-306 -4.542000e-306   \n",
       "21 -7.240000e-305  4.696000e-306 -5.598000e-306  3.842000e-306 -4.579000e-306   \n",
       "22 -7.420000e-305  4.427000e-306 -5.277000e-306  4.125000e-306 -4.917000e-306   \n",
       "23 -7.157000e-305  4.751000e-306 -5.663000e-306  3.982000e-306 -4.747000e-306   \n",
       "24 -7.182000e-305  4.495000e-306 -5.358000e-306  3.947000e-306 -4.705000e-306   \n",
       "25 -7.148000e-305  4.569000e-306 -5.446000e-306  4.022000e-306 -4.794000e-306   \n",
       "26 -6.700000e-305  4.132000e-306 -4.925000e-306  4.069000e-306 -4.851000e-306   \n",
       "27 -6.656000e-305  4.415000e-306 -5.262000e-306  3.971000e-306 -4.734000e-306   \n",
       "28 -6.885000e-305  4.713000e-306 -5.617000e-306  3.479000e-306 -4.147000e-306   \n",
       "29 -7.325000e-305  4.402000e-306 -5.247000e-306  3.757000e-306 -4.479000e-306   \n",
       "30 -7.310000e-305  4.457000e-306 -5.313000e-306  3.664000e-306 -4.367000e-306   \n",
       "31 -7.575000e-305  4.698000e-306 -5.600000e-306  3.558000e-306 -4.241000e-306   \n",
       "32 -7.236000e-305  4.727000e-306 -5.635000e-306  3.760000e-306 -4.482000e-306   \n",
       "33 -7.485000e-305  4.975000e-306 -5.930000e-306  4.260000e-306 -5.077000e-306   \n",
       "34 -7.233000e-305  4.781000e-306 -5.699000e-306  4.027000e-306 -4.800000e-306   \n",
       "35 -7.361000e-305  4.969000e-306 -5.923000e-306  4.108000e-306 -4.897000e-306   \n",
       "36 -6.886000e-305  4.764000e-306 -5.679000e-306  3.889000e-306 -4.636000e-306   \n",
       "37 -6.880000e-305  4.684000e-306 -5.583000e-306  4.214000e-306 -5.024000e-306   \n",
       "38 -6.964000e-305  4.682000e-306 -5.581000e-306  4.249000e-306 -5.064000e-306   \n",
       "39 -7.076000e-305  4.407000e-306 -5.253000e-306  4.306000e-306 -5.133000e-306   \n",
       "40 -7.087000e-305  4.532000e-306 -5.402000e-306  4.317000e-306 -5.146000e-306   \n",
       "41 -7.002000e-305  4.850000e-306 -5.781000e-306  3.653000e-306 -4.354000e-306   \n",
       "42 -7.086000e-305  4.585000e-306 -5.465000e-306  3.665000e-306 -4.369000e-306   \n",
       "43 -7.481000e-305  4.600000e-306 -5.484000e-306  3.230000e-306 -3.850000e-306   \n",
       "44 -7.389000e-305  4.596000e-306 -5.478000e-306  3.954000e-306 -4.714000e-306   \n",
       "45 -7.045000e-305  4.979000e-306 -5.935000e-306  4.045000e-306 -4.821000e-306   \n",
       "46 -7.465000e-305  5.479000e-306 -6.530000e-306  4.131000e-306 -4.924000e-306   \n",
       "47 -7.203000e-305  5.117000e-306 -6.100000e-306  4.223000e-306 -5.034000e-306   \n",
       "48 -7.249000e-305  5.299000e-306 -6.316000e-306  3.941000e-306 -4.698000e-306   \n",
       "49 -7.075000e-305  4.810000e-306 -5.734000e-306  4.568000e-306 -5.445000e-306   \n",
       "\n",
       "             Oxy8         DeOxy8  State  \n",
       "0   1.449000e-306 -1.727000e-306    1.0  \n",
       "1   1.947000e-306 -2.321000e-306    1.0  \n",
       "2   1.831000e-306 -2.182000e-306    1.0  \n",
       "3   1.948000e-306 -2.322000e-306    1.0  \n",
       "4   1.614000e-306 -1.923000e-306    1.0  \n",
       "5   1.665000e-306 -1.985000e-306    1.0  \n",
       "6   1.846000e-306 -2.201000e-306    1.0  \n",
       "7   1.959000e-306 -2.336000e-306    1.0  \n",
       "8   1.476000e-306 -1.760000e-306    1.0  \n",
       "9   1.501000e-306 -1.790000e-306    1.0  \n",
       "10  1.627000e-306 -1.940000e-306    1.0  \n",
       "11  1.698000e-306 -2.023000e-306    1.0  \n",
       "12  1.565000e-306 -1.866000e-306    1.0  \n",
       "13  1.328000e-306 -1.583000e-306    1.0  \n",
       "14  1.411000e-306 -1.682000e-306    1.0  \n",
       "15  1.334000e-306 -1.590000e-306    1.0  \n",
       "16  1.578000e-306 -1.881000e-306    1.0  \n",
       "17  1.446000e-306 -1.724000e-306    1.0  \n",
       "18  1.583000e-306 -1.887000e-306    1.0  \n",
       "19  1.652000e-306 -1.969000e-306    1.0  \n",
       "20  1.465000e-306 -1.747000e-306    1.0  \n",
       "21  1.466000e-306 -1.748000e-306    1.0  \n",
       "22  1.620000e-306 -1.930000e-306    1.0  \n",
       "23  1.701000e-306 -2.028000e-306    1.0  \n",
       "24  1.912000e-306 -2.280000e-306    1.0  \n",
       "25  1.917000e-306 -2.285000e-306    1.0  \n",
       "26  2.159000e-306 -2.573000e-306    1.0  \n",
       "27  2.002000e-306 -2.387000e-306    1.0  \n",
       "28  1.687000e-306 -2.011000e-306    1.0  \n",
       "29  1.783000e-306 -2.126000e-306    1.0  \n",
       "30  1.752000e-306 -2.088000e-306    1.0  \n",
       "31  1.763000e-306 -2.102000e-306    1.0  \n",
       "32  1.755000e-306 -2.092000e-306    1.0  \n",
       "33  1.704000e-306 -2.031000e-306    1.0  \n",
       "34  1.643000e-306 -1.959000e-306    1.0  \n",
       "35  1.913000e-306 -2.280000e-306    1.0  \n",
       "36  1.905000e-306 -2.271000e-306    1.0  \n",
       "37  1.664000e-306 -1.983000e-306    1.0  \n",
       "38  2.110000e-306 -2.515000e-306    1.0  \n",
       "39  1.855000e-306 -2.212000e-306    1.0  \n",
       "40  2.134000e-306 -2.544000e-306    1.0  \n",
       "41  2.031000e-306 -2.421000e-306    1.0  \n",
       "42  2.034000e-306 -2.425000e-306    1.0  \n",
       "43  1.751000e-306 -2.087000e-306    1.0  \n",
       "44  1.734000e-306 -2.067000e-306    1.0  \n",
       "45  1.846000e-306 -2.201000e-306    1.0  \n",
       "46  1.883000e-306 -2.245000e-306    1.0  \n",
       "47  1.893000e-306 -2.257000e-306    1.0  \n",
       "48  1.943000e-306 -2.316000e-306    1.0  \n",
       "49  1.869000e-306 -2.228000e-306    1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas is a library for storing csv files in a dataframe, a data format that makes it easy to work with csv files\n",
    "import pandas as pd\n",
    "\n",
    "# read in the csv file as the variable dataset\n",
    "dataset = pd.read_csv(\"../datasets/taskdefault.csv\")\n",
    "# add columns to the dataset\n",
    "dataset.columns = ['Timestamp','Oxy1','DeOxy1','Oxy2','DeOxy2','Oxy3','DeOxy3', 'Oxy4','DeOxy4','Oxy5','DeOxy5','Oxy6','DeOxy6','Oxy7','DeOxy7','Oxy8','DeOxy8', 'State']\n",
    "\n",
    "# shows the first 50 rows in the dataset\n",
    "dataset.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'Oxy1', 'DeOxy1', 'Oxy2', 'DeOxy2', 'Oxy3', 'DeOxy3',\n",
       "       'Oxy4', 'DeOxy4', 'Oxy5', 'DeOxy5', 'Oxy6', 'DeOxy6', 'Oxy7', 'DeOxy7',\n",
       "       'Oxy8', 'DeOxy8', 'State'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the columns in the dataset\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b9a8b1348e1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# batchnormalization, makes the value fit between 0-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Input layer with 16 inputs, because we X has 16 values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;31m# Dropout - the number of neurons removed at each layers, who are readded when testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/layers/normalization.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    101\u001b[0m                                          \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                                          \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                                          constraint=self.gamma_constraint)\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    397\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m                             constraint=constraint)\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name, constraint)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# TODO: move to `tf.get_variable` when supported in public release.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "# Making our neural network\n",
    "\n",
    "# Adding dependencies\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import keras\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# Remove missing values\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Shuffle the data\n",
    "dataset = dataset.sample(frac=1)\n",
    "\n",
    "# for use with two channels\n",
    "X = np.column_stack((dataset[\"Oxy1\"], dataset[\"Oxy2\"], dataset[\"Oxy3\"], dataset[\"Oxy4\"], dataset[\"Oxy5\"], dataset[\"Oxy6\"], dataset[\"Oxy7\"], dataset[\"Oxy8\"], dataset[\"DeOxy1\"], dataset[\"DeOxy2\"], dataset[\"DeOxy3\"], dataset[\"DeOxy4\"], dataset[\"DeOxy5\"], dataset[\"DeOxy6\"], dataset[\"DeOxy7\"], dataset[\"DeOxy8\"] ))\n",
    "Y = np.array([[1,0] if i == 0 else [0,1] for i in dataset.State])\n",
    "\n",
    "# Batch size - the number of data points added at each time, affects training time\n",
    "# Epochs - the number of training/test sessions\n",
    "# create model\n",
    "model = Sequential()\n",
    "# batchnormalization, makes the value fit between 0-1\n",
    "# Input layer with 16 inputs, because we X has 16 values\n",
    "model.add(BatchNormalization(input_shape=(16, )))\n",
    "# Dropout - the number of neurons removed at each layers, who are readded when testing\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "\n",
    "# output layer guesses default or task positive network\n",
    "model.add(Dense(2, init=\"normal\", activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "history = model.fit(X, Y, validation_split=0.5, nb_epoch=5, batch_size=5, verbose=1)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what just happened?\n",
    "We trained a neural network that took half of the input 1228 samples to train and half of the sample 1228 samples to guess what state my mind was in.\n",
    "\n",
    "It doesn't seem to be working so well. This an excellent chance for us to figure out why that is!\n",
    "\n",
    "## The unsexy part of machine learning\n",
    "The sexy part of machine learning might be building machine learning models, sadly that is a tiny part of your job as a data scientist. The big job is figuring out, why doesn't your model that should be working work?\n",
    "\n",
    "## What could be wrong?\n",
    "When we are working with AI it is important to learn how to troubleshoot. Everything that can go wrong will go wrong so we just have to learn to deal with that.\n",
    "\n",
    "The most common problems in machine learning.\n",
    "## There is something wrong with your data\n",
    "One of the most common problems in machine learning is that your data isn't any good.\n",
    "\n",
    "Remember? \n",
    "Garbage in = Garbage out\n",
    "\n",
    "Depending on your dataset there could be many things that could be wrong. So lets take a moment to think what could be wrong with our data?\n",
    "\n",
    "## Can you think of all the things that might be wrong with our data?\n",
    "\n",
    "There was too much light in the room that made the sensor have an overflow error\n",
    "The placement of the brain sensor was wrong so it couldn't detect activity in the brain properly\n",
    "There was something wrong with the sensor that detected the brain signal, so it couldn't detect it properly.\n",
    "There was something wrong with the program that gave us the data from the brain sensor that corrupted the data.\n",
    "A mistake was made while converting the data into a csv file.\n",
    "We wrote something wrong when we prepropessed the data (adding labels, removing missing values, shuffling the data)\n",
    "\n",
    "## Can you think of all the things that be wrong with our machine learning model?\n",
    "\n",
    "We didn't assing our X and Y variables properly.\n",
    "We are using a machine learning model that isn't suitable for handling our data format.\n",
    "Our activation function (currently RELU) might be inefficient.\n",
    "Our loss function (currently mean_squared_error) might be inefficient. \n",
    "We need to tweak the number of layers in our neural network (currently a 16 neuron input layer, 4 deep layers of 100 neurons each, with  and a 2 neuron output layer).\n",
    "We need to add our remove dropout to deal with overfitting (currently done twice with 20% of the network)\n",
    "We need to tweak our output parameters (validation_split=0.5, nb_epoch=5, batch_size=5), validation_split is how much of our data is training data and how much is test data, nb_epochs is how long the network will train and batch_size is how much data will be processed before updating weights. \n",
    "\n",
    "## Figure out if you have a problem with your data or your model\n",
    "\n",
    "The reason why it's so important to work with data preprocessing is because it's really good to know that it's actually your neural network that is the problem before you spend all your time trying to fix it.\n",
    "\n",
    "I'll save you the suspense for this dataset. Default mode network activation has never been able to be detected using FNIRS (the technology we are using for the dataset), so even with the worlds greatest neural network, some problems might still be impossible to solve. The reason as always has to do with the input data.\n",
    "\n",
    "Artificial neural network are (like our brain!) universal function approximators. This means that they can theoretically solve any mathematical problem. \n",
    "\n",
    "When you are working in deep learning, your neural network are usually fine once you get the working, but getting the right data will always remain a big problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need more data\n",
    "Okay, so that didn't work so well. Lets try instead with a larger dataset. \n",
    "This dataset uses data taken from the 52 broadmann areas of the brain for 51 users.\n",
    "![alt text](files/brodmannareas2.gif \"Logo Title Text 1\")\n",
    "\n",
    "The goal is to try to detect activation in the default mode network or task positive networks on the brain with the help of FNIRS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The default mode network\n",
    "The default mode network is a brain area that is active during rest.\n",
    "\n",
    "It is the neurological basis for the self\n",
    "\n",
    "Autobiographical information: Memories of collection of events and facts about one's self\n",
    "Self-reference: Referring to traits and descriptions of one's self\n",
    "Emotion of one's self: Reflecting about one's own emotional state\n",
    "Thinking about others:\n",
    "\n",
    "Theory of Mind: Thinking about the thoughts of others and what they might or might not know\n",
    "Emotions of other: Understanding the emotions of other people and empathizing with their feelings\n",
    "Moral reasoning: Determining just and unjust result of an action\n",
    "Social evaluations: Good-bad attitude judgments about social concepts\n",
    "Social categories: Reflecting on important social characteristics and status of a group\n",
    "Remembering the past and thinking about the future:\n",
    "\n",
    "Remembering the past: Recalling events that happened in the past\n",
    "Imagining the future: Envisioning events that might happen in the future\n",
    "Episodic memory: Detailed memory related to specific events in time\n",
    "Story comprehension: Understanding and remembering a narrative\n",
    "The default mode network is active during passive rest and mind-wandering. Mind-wandering usually involves thinking about others, thinking about one's self, remembering the past, and envisioning the future. Electrocorticography studies (which involve placing electrodes on the surface of epileptic patient's brains) have shown the default mode network becomes activated within an order of a fraction of a second after participants finish a task.\n",
    "![alt text](files/defaultmodenetwork2.jpg \"Logo Title Text 1\")\n",
    "## The task positive network\n",
    "The task positive network is active when you are doing a task and focusing your attention.\n",
    "The task-positive network (TPN) is a network of areas in the human brain that typically responds with activation increases to attention-demanding tasks in functional imaging studies. The task-positive network encompasses regions of the dorsal attention system, but in addition includes dorsolateral and ventrolateral prefrontal regions, the insular cortex, and the SMA/pre-SMA. Notably, the nodes of this network are also correlated during rest (i.e., in the absence of any task). The task-positive network is anti-correlated with the default mode network. \n",
    "\n",
    "During rest the TPN has been claimed to subserve intermittent \"external awareness\", defined as the conscious perception through different sensory modalities of one's surrounding environment[further explanation needed].\n",
    "![alt text](files/taskpositivenetwork.png \"Logo Title Text 1\")\n",
    "## Anticorrelations\n",
    "The task positive network and the default mode network are anti-correlated brain regions and you spend your time in either brain region.\n",
    "\n",
    "## Classifying brain region activation\n",
    "The goal of this assignment is to see if we can make a classifier that can understand what brain region is active. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward neural network\n",
    "<img src=\"http://ufldl.stanford.edu/tutorial/images/Network331.png\">\n",
    "## Calculated on an individual users file\n",
    "### Changing the value in pd.read_csv changes the user that the model trains on\n",
    "### This model uses a feedforward neural network on the data from the user\n",
    "### The data is randomized and 50% of the dataset is user for training and 50% for testing\n",
    "### X = input from the 52 broadmann areas at each timestep\n",
    "### Y = REST or ADDITION from the Mark column of the dataset\n",
    "### The model tries learns and tries to predict if each timeserie of data from the 52 broadmann areas is REST (Default mode network) or Addition (Task positive network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 656 samples, validate on 656 samples\n",
      "Epoch 1/50\n",
      "656/656 [==============================] - 0s - loss: 0.2415 - acc: 0.5152 - val_loss: 0.2430 - val_acc: 0.4787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/50\n",
      "656/656 [==============================] - 0s - loss: 0.2338 - acc: 0.5229 - val_loss: 0.2376 - val_acc: 0.4939\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/50\n",
      "656/656 [==============================] - 0s - loss: 0.2263 - acc: 0.6128 - val_loss: 0.2335 - val_acc: 0.6387\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/50\n",
      "656/656 [==============================] - 0s - loss: 0.2179 - acc: 0.6753 - val_loss: 0.2247 - val_acc: 0.6540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/50\n",
      "656/656 [==============================] - 0s - loss: 0.2050 - acc: 0.6997 - val_loss: 0.2103 - val_acc: 0.7363\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/50\n",
      "656/656 [==============================] - 0s - loss: 0.1900 - acc: 0.7210 - val_loss: 0.1967 - val_acc: 0.7332\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/50\n",
      "656/656 [==============================] - 0s - loss: 0.1820 - acc: 0.7302 - val_loss: 0.1877 - val_acc: 0.7409\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/50\n",
      "656/656 [==============================] - 0s - loss: 0.1706 - acc: 0.7470 - val_loss: 0.1731 - val_acc: 0.7652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/50\n",
      "656/656 [==============================] - 0s - loss: 0.1623 - acc: 0.7744 - val_loss: 0.1530 - val_acc: 0.7912\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/50\n",
      "656/656 [==============================] - 0s - loss: 0.1710 - acc: 0.7515 - val_loss: 0.1406 - val_acc: 0.8155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 11/50\n",
      "656/656 [==============================] - 0s - loss: 0.1539 - acc: 0.7790 - val_loss: 0.1359 - val_acc: 0.8399\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 12/50\n",
      "656/656 [==============================] - 0s - loss: 0.1601 - acc: 0.7683 - val_loss: 0.1242 - val_acc: 0.9131\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 13/50\n",
      "656/656 [==============================] - 0s - loss: 0.1482 - acc: 0.7805 - val_loss: 0.1220 - val_acc: 0.8537\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 14/50\n",
      "656/656 [==============================] - 0s - loss: 0.1435 - acc: 0.7957 - val_loss: 0.1092 - val_acc: 0.9360\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 15/50\n",
      "656/656 [==============================] - 0s - loss: 0.1334 - acc: 0.8277 - val_loss: 0.1009 - val_acc: 0.9177\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 16/50\n",
      "656/656 [==============================] - 0s - loss: 0.1514 - acc: 0.7820 - val_loss: 0.0922 - val_acc: 0.9405\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 17/50\n",
      "656/656 [==============================] - 0s - loss: 0.1192 - acc: 0.8354 - val_loss: 0.0908 - val_acc: 0.9162\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 18/50\n",
      "656/656 [==============================] - 0s - loss: 0.1345 - acc: 0.8003 - val_loss: 0.0898 - val_acc: 0.9177\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 19/50\n",
      "656/656 [==============================] - 0s - loss: 0.1349 - acc: 0.8140 - val_loss: 0.0803 - val_acc: 0.9223\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 20/50\n",
      "656/656 [==============================] - 0s - loss: 0.1228 - acc: 0.8369 - val_loss: 0.0707 - val_acc: 0.9497\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 21/50\n",
      "656/656 [==============================] - 0s - loss: 0.1254 - acc: 0.8201 - val_loss: 0.0684 - val_acc: 0.9451\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 22/50\n",
      "656/656 [==============================] - 0s - loss: 0.1121 - acc: 0.8567 - val_loss: 0.0585 - val_acc: 0.9604\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 23/50\n",
      "656/656 [==============================] - 0s - loss: 0.1354 - acc: 0.8079 - val_loss: 0.0586 - val_acc: 0.9634\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 24/50\n",
      "656/656 [==============================] - 0s - loss: 0.1359 - acc: 0.8064 - val_loss: 0.0600 - val_acc: 0.9741\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 25/50\n",
      "656/656 [==============================] - 0s - loss: 0.1270 - acc: 0.8201 - val_loss: 0.0687 - val_acc: 0.9573\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 26/50\n",
      "656/656 [==============================] - 0s - loss: 0.1192 - acc: 0.8369 - val_loss: 0.0662 - val_acc: 0.9223\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 27/50\n",
      "656/656 [==============================] - 0s - loss: 0.1108 - acc: 0.8445 - val_loss: 0.0554 - val_acc: 0.9588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 28/50\n",
      "656/656 [==============================] - 0s - loss: 0.1221 - acc: 0.8293 - val_loss: 0.0485 - val_acc: 0.9665\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 29/50\n",
      "656/656 [==============================] - 0s - loss: 0.1082 - acc: 0.8552 - val_loss: 0.0464 - val_acc: 0.9573\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 30/50\n",
      "656/656 [==============================] - 0s - loss: 0.1220 - acc: 0.8338 - val_loss: 0.0550 - val_acc: 0.9497\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 31/50\n",
      "656/656 [==============================] - 0s - loss: 0.1027 - acc: 0.8628 - val_loss: 0.0434 - val_acc: 0.9604\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 32/50\n",
      "656/656 [==============================] - 0s - loss: 0.1107 - acc: 0.8460 - val_loss: 0.0421 - val_acc: 0.9756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 33/50\n",
      "656/656 [==============================] - 0s - loss: 0.1218 - acc: 0.8369 - val_loss: 0.0443 - val_acc: 0.9741\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 34/50\n",
      "656/656 [==============================] - 0s - loss: 0.0989 - acc: 0.8613 - val_loss: 0.0422 - val_acc: 0.9680\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 35/50\n",
      "656/656 [==============================] - 0s - loss: 0.1086 - acc: 0.8460 - val_loss: 0.0385 - val_acc: 0.9710\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 36/50\n",
      "656/656 [==============================] - 0s - loss: 0.0919 - acc: 0.8857 - val_loss: 0.0364 - val_acc: 0.9726\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 37/50\n",
      "656/656 [==============================] - 0s - loss: 0.1126 - acc: 0.8460 - val_loss: 0.0314 - val_acc: 0.9756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 38/50\n",
      "656/656 [==============================] - 0s - loss: 0.0912 - acc: 0.8780 - val_loss: 0.0409 - val_acc: 0.9527\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 39/50\n",
      "656/656 [==============================] - 0s - loss: 0.0984 - acc: 0.8643 - val_loss: 0.0302 - val_acc: 0.9695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 40/50\n",
      "656/656 [==============================] - 0s - loss: 0.0881 - acc: 0.8918 - val_loss: 0.0359 - val_acc: 0.9680\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 41/50\n",
      "656/656 [==============================] - 0s - loss: 0.1064 - acc: 0.8460 - val_loss: 0.0301 - val_acc: 0.9726\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 42/50\n",
      "656/656 [==============================] - 0s - loss: 0.0939 - acc: 0.8765 - val_loss: 0.0253 - val_acc: 0.9756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 43/50\n",
      "656/656 [==============================] - 0s - loss: 0.0911 - acc: 0.8720 - val_loss: 0.0406 - val_acc: 0.9558\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 44/50\n",
      "656/656 [==============================] - 0s - loss: 0.0871 - acc: 0.8963 - val_loss: 0.0407 - val_acc: 0.9573\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 45/50\n",
      "656/656 [==============================] - 0s - loss: 0.0917 - acc: 0.8765 - val_loss: 0.0266 - val_acc: 0.9710\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 46/50\n",
      "656/656 [==============================] - 0s - loss: 0.0875 - acc: 0.8841 - val_loss: 0.0250 - val_acc: 0.9787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 47/50\n",
      "656/656 [==============================] - 0s - loss: 0.0873 - acc: 0.8918 - val_loss: 0.0244 - val_acc: 0.9756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 48/50\n",
      "656/656 [==============================] - 0s - loss: 0.0943 - acc: 0.8598 - val_loss: 0.0222 - val_acc: 0.9817\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 49/50\n",
      "656/656 [==============================] - 0s - loss: 0.0983 - acc: 0.8628 - val_loss: 0.0250 - val_acc: 0.9771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 50/50\n",
      "656/656 [==============================] - 0s - loss: 0.0983 - acc: 0.8582 - val_loss: 0.0248 - val_acc: 0.9756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWppCRETU0hQiIqKWphAREbX/AWvH+kfH7tYvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f723fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFIyIiKglBSMiImpJwYiIiFr+AFsQb49vRITKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11faad898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training history\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import keras\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "dataset = pd.read_csv(\"https://www.floydhub.com/viewer/data/res7UHjG5WSPgPStBT84xW/ID-OXY-20.csv\")\n",
    "# split into input (X) and output 󰀀 variables\n",
    "\n",
    "# Remove missing values\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# make a label dataset\n",
    "dataset[\"Label\"] = dataset[\"Mark\"]\n",
    "\n",
    "# change rest values to \n",
    "# default mode\n",
    "dataset[\"Mark\"][dataset[\"Mark\"] == \"REST\"] = 0\n",
    "# task positive network\n",
    "dataset[\"Mark\"][dataset[\"Mark\"] == \"ADDITION\"] = 1\n",
    "dataset[\"Mark\"][dataset[\"Mark\"] == \"PASSTHOUGHT\"] = 2\n",
    "dataset[\"Mark\"][dataset[\"Mark\"] == \"JUNK\"] = 3\n",
    "\n",
    "# remove the JUNK data\n",
    "dataset = dataset[dataset.Mark != 2]\n",
    "dataset = dataset[dataset.Mark != 3]\n",
    "\n",
    "# shuffle the data\n",
    "dataset = dataset.sample(frac=1)\n",
    "\n",
    "# 52 broadmann areas data\n",
    "X = np.array(dataset.ix[:, :'CH52'])\n",
    "# default mode network or task positive network\n",
    "Y = np.array([[1,0] if i == 0 else [0,1] for i in dataset.Mark])\n",
    "\n",
    "# Dropout - the number of neurons removed at each layers, who are readded when testing\n",
    "# Batch size - the number of data points added at each time, affects training time\n",
    "# Epochs - the number of training/test sessions\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "# makes the values between 0 and 1\n",
    "model.add(BatchNormalization(input_shape=(52,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, init=\"normal\", activation='relu'))\n",
    "\n",
    "model.add(Dense(2, init=\"normal\", activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "history = model.fit(X, Y, validation_split=0.5, nb_epoch=50, batch_size=50, verbose=1)\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is how you can save a trained model.\n",
    "\n",
    "# Save a trained model\n",
    "\n",
    "# save the trained model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"taskdefault.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"taskdefault.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
